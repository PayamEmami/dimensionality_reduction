[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dimensionality reduction methods",
    "section": "",
    "text": "Introduction\nIn this chapter we are going to learn about a few commonly used dimensionality reduction methods in life science research. We are first going to define what dimensionality reduction is, why and when we should use it. We will then learn how to use these methods and how they work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-reducing-dimentions",
    "href": "index.html#why-reducing-dimentions",
    "title": "Dimensionality reduction methods",
    "section": "Why reducing dimentions",
    "text": "Why reducing dimentions\nThere are many reasons why we might want to reduce data dimensions.\n\nData Visualization: This is probably one of the most common reasons we do dimensionality reduction. High-dimensional data is difficult to visualize in its raw form. Dimensionality reduction techniques like PCA and t-SNE allow us to reduce this data into two or three dimensions. We can then plot the data and see trends, clusters, or outliers. Obviously, having our data summarized to a couple of variables is easier for the human eye to comprehend.\nRemoving Noise and Redundancy: Despite that we have measured many variables, not all of them contribute equally to the information contained in the data. Some variables may be noisy, irrelevant, or redundant. Dimensionality reduction methods can help eliminate these less useful dimensions, giving us a cleaner, more informative dataset. Similary, one can use these method to adjust for unwanted trends in the data such as batch effects etc.\nUncovering Patterns and Trends: Having a lot of variables is not always good, often, the true underlying structure of the data is hidden within many dimensions. Dimensionality reduction helps to reveal the most important patterns and trends by summarizing the data by some form of combinition of the raw variables making it easier to detect relationships between samples and uncover valuable insights.\nImproving Model Performance: We know that in machine learning, too many variables can lead to overfitting, where a model performs well on training data but poorly on unseen data. Dimensionality reduction can help prevent this by focusing on the most important features. So we can improve the model’s generalizability and predictive performance.\n\nThere might also be other reasons to reduce the dimention of the data. For example working with large, high-dimensional datasets can be computationally expensive. Dimensionality reduction lowers the number of variables, which reduces the memory and processing power needed to analyze the data.\nIn general, specially in OMICS data analysis, dimensionality reduction is often performed at some point in the analysis workflow. It might be the case that the results of it might not be the main interest but still might affect the overall decision making process. The example can be quality check, outlier detection etc.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-dimentionality-reduction-works",
    "href": "index.html#how-dimentionality-reduction-works",
    "title": "Dimensionality reduction methods",
    "section": "How dimentionality reduction works",
    "text": "How dimentionality reduction works\nAdmittedly, the answer to this question is not so simple. There are many different approaches to dimensionality reduction, each with its own principles and techniques. Some methods, like Principal Component Analysis (PCA), focus on finding directions in the data that capture the most variance. Others, such as t-SNE and UMAP, are more concerned with preserving the local structure and distances between data points. Some like autoencoders learn compact representations of the data by compressing it into a lower-dimensional form and reconstructing the original inputs. There are even methods like Linear Discriminant Analysis (LDA) and Non-negative Matrix Factorization (NMF) also offer unique ways to reduce dimensions by focusing on class separation or non-negative decomposition.\nMost of these methods, however, work in one way or another with the concept of distances or similarities between data points. For example, PCA seeks to maximize the variance (which is linked to the spread, or “distance,” between data points in the dataset), while t-SNE and UMAP preserve relative distances so that points close together in high-dimensional space remain close after dimensionality reduction. Even methods like autoencoders rely on optimization processes that capture patterns of similarity in the data.\nIt is however very important to pay attend to what the selected method is seeking to show in lower dimentions. This will directly affect the interpretation and usage of the lower dimentional space. So while the goal of these methods remains the same (preserving the structure in the original dataset), the definition of “structure” varies between this methods. For example, PCA is more focused on capturing global structure, meaning it seeks to maximize the overall variance across the entire dataset. It tries to find the directions in which the data varies the most, but it might overlook subtle local relationships between data points. Methods like t-SNE and UMAP focus on local structure, meaning they try to keep the relative distances between nearby points, which is great for understanding clusters but may not accurately show large-scale patterns in the data.\nThe point that i wanted to make is that the choice of dimensionality reduction method directly influences what information from the original data are retained and what are lost. Understanding each method’s “structure” definition is very important for making decisions about how to interpret and use the reduced data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-we-are-going-to-cover",
    "href": "index.html#what-we-are-going-to-cover",
    "title": "Dimensionality reduction methods",
    "section": "What we are going to cover?",
    "text": "What we are going to cover?\nPCA is probably the most well-known dimensionality reduction method. You can learn more about PCA (https://payamemami.com/pca_basics/). In the rest of this book, we are going to learn some other useful dimensionality reduction methods such as t-SNE and UMAP. We might bring up PCA just to compare the results and mathematical formulation. So if you are still learning about dimensionality reduction, please have a look athe PCA chapter first.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "tsne.html",
    "href": "tsne.html",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "",
    "text": "2 t-sne in R\nWithout going further into the details, we are going to start using t-SNE in R. The main function used for this is Rtsne() from the Rtsne package. Before that i want to simulate some data so we can check later how t-SNE is doing.\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of points per subcluster\nn &lt;- 25\n\n# Manually define main cluster centers (global structure)\nmain_cluster_centers &lt;- data.frame(\n  x = c(0, 5, 10, 15),\n  y = c(0, 5, 10, 15),\n  z = c(0, 5, 10, 15),\n  cluster = factor(1:4)\n)\n\n# Manually define subcluster offsets relative to each main cluster\n# These small offsets will determine subcluster locations within each main cluster\nsubcluster_offsets &lt;- data.frame(\n  x_offset = c(-0.25, 0.25, -0.25, 0.25),\n  y_offset = c(-0.25, -0.25, 0.25, 0.25),\n  z_offset = c(0.25, -0.25, -0.25, 0.25),\n  subcluster = factor(1:4)\n)\n\n# Initialize an empty data frame to hold all data\ndata &lt;- data.frame()\n\n# Generate data for each main cluster with subclusters\nfor (i in 1:nrow(main_cluster_centers)) {\n  for (j in 1:nrow(subcluster_offsets)) {\n    # Calculate subcluster center by adding the offset to the main cluster center\n    subcluster_center &lt;- main_cluster_centers[i, 1:3] + subcluster_offsets[j, 1:3]\n    \n    # Generate points for each subcluster with a small spread (to form local clusters)\n    subcluster_data &lt;- data.frame(\n      gene1 = rnorm(n, mean = subcluster_center$x, sd = 0.25),  # Small spread within subclusters\n      gene2 = rnorm(n, mean = subcluster_center$y, sd = 0.25),\n      gene3 = rnorm(n, mean = subcluster_center$z, sd = 0.25),\n      cluster = main_cluster_centers$cluster[i],\n      subcluster = subcluster_offsets$subcluster[j]\n    )\n    \n    # Add generated subcluster data to the main data frame\n    data &lt;- rbind(data, subcluster_data)\n  }\n}\nThis data has just three dimentions for 400 samples (4 major clusters). We can plot the data here:\nCode\n# Visualize the original data in 3D, distinguishing clusters and subclusters\nlibrary(dplyr)\nlibrary(plotly)  # For interactive 3D plots\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\nFor this data, the features are in the column and samples in the row.\nWe are now going to do a t-SNE on this data and see the results. There are some parameters to set but we just go for the default ones. We also do PCA and show the results side by side\nCode\nlibrary(Rtsne)\nlibrary(ggplot2)\nlibrary(cowplot)\n\nset.seed(123)\ntsne_results_30 &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")])\n)\n\n\ndata$tsne_x_30 &lt;- tsne_results_30$Y[, 1]\ndata$tsne_y_30 &lt;- tsne_results_30$Y[, 2]\n\n\ntsne_plot &lt;- plot_ly(\n  data, x = ~tsne_x_30, y = ~tsne_y_30, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n\npca_results &lt;- prcomp(data[, c(\"gene1\", \"gene2\", \"gene3\")], scale. = FALSE)\ndata$pca_x &lt;- pca_results$x[, 1]\ndata$pca_y &lt;- pca_results$x[, 2]\n\n\npca_plot &lt;- plot_ly(\n  data, x = ~pca_x, y = ~pca_y, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n  \nsubplot(pca_plot%&gt;%layout(showlegend = FALSE), tsne_plot%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"PCA\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"t-SNE\", showarrow = F, xref='paper', yref='paper'))\n)\nThe plot shows a side-by-side comparison of results obtained using PCA (left) and t-SNE (right) for dimensionality reduction. In the PCA plot, the data points are clearly separated by their clusters along the horizontal axis, but the clusters themselves are quite stretched and not very compact, indicating that PCA is apturing global variance sources but doesn’t reveal tight local groupings.\nOn the other hand, the t-SNE plot reveals well-separated, compact clusters, showing how good t-SNE is at preserving local structures within the data. The clusters are clearly distinct from one another, and their circular, tightly packed shapes indicate that t-SNE effectively keeps points that are similar (close in high-dimensional space) together in the low-dimensional projection. However, the distances between clusters may not reflect global relationships as well as PCA does (we will get back to this later).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#input-parameters",
    "href": "tsne.html#input-parameters",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "2.1 Input parameters",
    "text": "2.1 Input parameters\nLike any other method, t-SNE also requires some input parameters. Let’s start with the most essential ones and later go through the rest.\nThe data matrix (X) is the core input for t-SNE, where each row represents an observation, and each column is a variable or feature. This matrix is typically high-dimensional, and t-SNE’s job is to map it into a lower-dimensional space (usually 2D or 3D) to make patterns more interpretable.\nNext, dims defines the dimensionality of the output. Typically, we choose 2D for visualization purposes, but 3D is also possible if more complexity is required.\nThe perplexity parameter is probably the most important one. It controls how t-SNE balances local and global structures of the data. You can think of it as determining how many “neighbors” each data point should consider when projecting into the low-dimensional space. Choosing the right perplexity is very important because it affects how t-SNE interprets the relationships between data points.\nAnother key parameter in this specific implementation is theta, which adjusts the balance between accuracy and computational speed. For large datasets, using a larger theta can make t-SNE run faster but at the cost of some accuracy. If you prioritize precision, especially for smaller datasets, you can set this parameter to 0 for exact t-SNE.\nWe also have the max_iter parameter, which controls the number of iterations the algorithm goes through during optimization. More iterations give t-SNE time to better fine-tune the output, though in practice, the default is often sufficient unless you notice the algorithm hasn’t converged.\nAfter these essential parameters, additional settings like PCA preprocessing, momentum terms, and learning rate can further fine-tune the performance of t-SNE for different types of data. We will talk about these later.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#perplexity",
    "href": "tsne.html#perplexity",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "2.2 Perplexity",
    "text": "2.2 Perplexity\nI had to devote a small section to this parameter because it is probably the most important one (if you already have your data 😉). Perplexity essentially defines how t-SNE balances attention between local and global structures in your data. You can think of it as controlling the size of the neighborhood each data point considers when positioning itself in the lower-dimensional space.\nA small perplexity value (e.g., 5–30) means that t-SNE will focus more on preserving the local structure, i.e., ensuring that close neighbors in the high-dimensional space stay close in the low-dimensional projection. This is great for datasets with small clusters or when you’re particularly interested in capturing details.\nA larger perplexity (e.g., 30–50 or even higher) encourages t-SNE to preserve global structure, considering more far away points as part of the neighborhood. This can be useful for larger datasets or when you want to capture broader relationships between clusters.\nFinding the right perplexity often involves some experimentation. If it’s too small, t-SNE might overfit to the local structure and fail to reveal larger patterns in the data. If it’s too large, you might lose relationships between nearby data points. t-SNE is retively robust to different perplexity values, so changing this parameter slightly usually won’t result in big changes, but it can make the difference between a good visualization and a great one.\nA rule of thumb is to set the perplexity such that \\(3 \\times \\text{perplexity} &lt; n-1\\), where \\(n\\) is the number of data points. Testing several values across this range will help you find the best fit for your data.\n\n2.2.1 See the impact of perplexity\nI did not tell you before (you might have realized it from the code though) but there are substructures in the original data. We are now going to plot them again.\n\n\nCode\nlibrary(dplyr)\nlibrary(plotly)  # For interactive 3D plots\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~subcluster,#,symbol=~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\n\n\n\n\n\n\nYou should now see that within each cluster we have subclusters. Let’s see if our original t-SNE was successful in separating them.\n\n\nCode\ntsne_plot2&lt;-plot_ly(\n  data, x = ~tsne_x_30, y = ~tsne_y_30, color = ~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n\npca_plot2 &lt;- plot_ly(\n  data, x = ~pca_x, y = ~pca_y, color = ~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n  \nsubplot(pca_plot2%&gt;%layout(showlegend = FALSE), tsne_plot2%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"PCA\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"t-SNE\", showarrow = F, xref='paper', yref='paper')))\n\n\n\n\n\n\nCompared to PCA we have actually done a good job. Most clusters seems to be well separated. But what we want to do is to change perplexity to see if we can make this better.\n\n\nCode\nset.seed(123)\ntsne_results_20 &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),perplexity = 20\n)\ndata$tsne_x_20 &lt;- tsne_results_20$Y[, 1]\ndata$tsne_y_20 &lt;- tsne_results_20$Y[, 2]\n\ntsne_plot2&lt;-plot_ly(\n  data, x = ~tsne_x_30, y = ~tsne_y_30, color = ~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n\ntsne_plot20&lt;-plot_ly(\n  data, x = ~tsne_x_20, y = ~tsne_y_20, color = ~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n  \nsubplot(tsne_plot2%&gt;%layout(showlegend = FALSE), tsne_plot20%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"t-SNE (30)\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"t-SNE (20)\", showarrow = F, xref='paper', yref='paper')))\n\n\n\n\n\n\nI have now decreased perplexity to 20. What we can see is that at 30 perplexity, t-SNE is accounting for a larger neighborhood of points when embedding the data. This results in clearer separation between clusters, with well-defined compact clusters. At 20 however, each cluster also appears distinctly separated in space, maintaining a reasonable balance between local and global structure. The substructures within the clusters are more prominent, with some separation and internal grouping within each main cluster, suggesting that t-SNE is better at capturing smaller-scale local variations with a lower perplexity.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#output-of-rtsne",
    "href": "tsne.html#output-of-rtsne",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "2.3 Output of Rtsne",
    "text": "2.3 Output of Rtsne\nThis is relatively streightforward. The most important output of this function is \\(Y\\). You can extract using tsne_results_20$Y. This is a matrix that has the exact same number of rows as your orignal data and the number of columns is the same as dims parameter. So basically it is your data transformed into the lower dimention of size dims. In our case, it was 2.\nThe rest of the output is basically either information about the optimization process or the summary of the input paramters. We are going to ignore them for now and will get back to it later if needed.\n\n\n\n\n\n\nDo it yourself\n\n\n\nPlay around with these parameters (perplexity, theta, and max_iter) to see if you can get better results",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#between-cluster-distances-and-densities-might-not-be-accurate",
    "href": "tsne.html#between-cluster-distances-and-densities-might-not-be-accurate",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "3.1 between cluster distances and densities might not be accurate!",
    "text": "3.1 between cluster distances and densities might not be accurate!\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of points per subcluster\nn &lt;- 25\n\n# Manually define main cluster centers (global structure)\nmain_cluster_centers &lt;- data.frame(\n  x = c(0, 10, 15, 35),\n  y = c(0, 10, 15, 35),\n  z = c(0, 10, 15, 35),\n  cluster = factor(1:4)\n)\n\n# Manually define subcluster offsets relative to each main cluster\n# These small offsets will determine subcluster locations within each main cluster\nsubcluster_offsets &lt;- data.frame(\n  x_offset = c(-0.25, 0.25, -0.25, 0.25),\n  y_offset = c(-0.25, -0.25, 0.25, 0.25),\n  z_offset = c(0.25, -0.25, -0.25, 0.25),\n  subcluster = factor(1:4)\n)\n\n# Initialize an empty data frame to hold all data\ndata &lt;- data.frame()\n\n# Generate data for each main cluster with subclusters\n\nfor (i in 1:nrow(main_cluster_centers)) {\n  for (j in 1:nrow(subcluster_offsets)) {\n    # Calculate subcluster center by adding the offset to the main cluster center\n    subcluster_center &lt;- main_cluster_centers[i, 1:3] + subcluster_offsets[j, 1:3]\n    \n    # Generate points for each subcluster with a small spread (to form local clusters)\n    subcluster_data &lt;- data.frame(\n      gene1 = rnorm(n, mean = subcluster_center$x, sd = 0.25*i*i),  # Small spread within subclusters\n      gene2 = rnorm(n, mean = subcluster_center$y, sd = 0.25*i*i),\n      gene3 = rnorm(n, mean = subcluster_center$z, sd = 0.25*i*i),\n      cluster = main_cluster_centers$cluster[i],\n      subcluster = subcluster_offsets$subcluster[j]\n    )\n    \n    # Add generated subcluster data to the main data frame\n    data &lt;- rbind(data, subcluster_data)\n  }\n}\n\n\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\n\n\n\n\n\n\nWhat i have done now is to give different density to each larger cluster and also different distances between the clusters. We can do t-SNE and compare the results to PCA.\n\n\nCode\nset.seed(123)\ntsne_results_30 &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")])\n)\n\n\ndata$tsne_x_30 &lt;- tsne_results_30$Y[, 1]\ndata$tsne_y_30 &lt;- tsne_results_30$Y[, 2]\n\n\ntsne_plot &lt;- plot_ly(\n  data, x = ~tsne_x_30, y = ~tsne_y_30, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n\npca_results &lt;- prcomp(data[, c(\"gene1\", \"gene2\", \"gene3\")], scale. = FALSE)\ndata$pca_x &lt;- pca_results$x[, 1]\ndata$pca_y &lt;- pca_results$x[, 2]\n\n\npca_plot &lt;- plot_ly(\n  data, x = ~pca_x, y = ~pca_y, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n  \nsubplot(pca_plot%&gt;%layout(showlegend = FALSE), tsne_plot%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"PCA\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"t-SNE\", showarrow = F, xref='paper', yref='paper'))\n)\n\n\n\n\n\n\nPCA did a great job in preserving the relative distances between clusters, reflecting the original distribution of the data. The density of the clusters is also proportional to the original data, with tighter clusters remaining dense and more spread-out clusters maintaining their looser arrangement. In contrast, t-SNE naturally equalizes the densities of the clusters, making them appear more uniform. This is not an artifact, but a deliberate feature of t-SNE, where dense clusters are expanded and sparse ones contracted. As a result, the distances between clusters in the t-SNE plot may appear more similar, and the clusters themselves more evenly distributed, which can distort the true global relationships.\n\n\n\n\n\n\nClustering on t-SNE results\n\n\n\nAvoid doing density and distance based clustering on t-SNE space. In vast majority of the cases the distances and density don’t have much meaning!\n\n\nThe last thing i want to mention here is that having loo little perplexity might cause misinterpretation of noise as clusters. In our previous example we know that we have four major clusters in our data but look what happens if we decrease the perplexity too much.\n\n\nCode\ntnse_data_frames&lt;-c()\nfor(it in c(2:10))\n{\n  \n  set.seed(123)\ntsne_results_it &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),perplexity = it,\n)\n  tnse_data_frames&lt;-rbind(tnse_data_frames,data.frame(tsne_results_it$Y,data[,c(4,5)],perplexity=it))\n}\n\n\ntsne_plot2&lt;-plot_ly(\n  tnse_data_frames, x = ~X1, y = ~X2, color = ~subcluster,frame=~perplexity,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\ntsne_plot2\n\n\n\n\n\n\nSmall clusters start to appear, which don’t exist in the original data. Unfortunately, this is largely dependent on the perplexity parameter. Lower perplexities overemphasize local structures, making t-SNE susceptible to identifying random noise as distinct clusters. This can lead to false interpretations, especially when exploring new datasets where the true structure is not well known. Therefore, it’s important to experiment with different perplexity values and validate findings with complementary methods.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#gaussian-or-normal-distribution",
    "href": "tsne.html#gaussian-or-normal-distribution",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.1 Gaussian (or normal) distribution",
    "text": "5.1 Gaussian (or normal) distribution\nOne issue with the Euclidean distance is that it does not inherently prioritize nearby points over distant ones. Every pair of points, whether close or far apart, is treated equally when computing the overall data structure. While this is not necessarily a flaw, our goal here is to capture the local structure rather than the global one. To address this, we can explore modifications to the Euclidean distance to make it slightly more local, such as incorporating a scaling factor like the standard deviation, which emphasizes nearby points without completely discarding global relationships.\nMore specifically we could divide the Euclidean distance between two points by the standard deviation of the distances from one of the points to its neighbors. This adjustment would make distances in denser regions appear relatively larger, prioritizing local structures without completely ignoring the global one.\nSo basically we do something like\n\\[\n\\tilde{D}_{ij} = \\frac{D_{ij}}{\\sigma_i}\n\\]\nWhere:\n\n\\(D_{ij}\\) is the Euclidean distance between points \\(i\\) and \\(j\\),\n\\(\\sigma_i\\) is the standard deviation of the distances from point \\(i\\) to its neighbors.\n\nLet’s see how it looks like\n\n\nCode\n# Function to compute local standard deviation based on k-nearest neighbors\ncompute_sigma &lt;- function(dist_matrix, k_neighbors) {\n  n &lt;- nrow(dist_matrix)\n  sigma &lt;- numeric(n)\n  \n  for (i in 1:n) {\n    # Get distances from point i to all other points\n    dists &lt;- dist_matrix[i, ]\n    \n    # Sort the distances and select k nearest neighbors (excluding self)\n    nearest_neighbors &lt;- sort(dists, decreasing = FALSE)[2:(k_neighbors + 1)]\n    \n    # Compute standard deviation of distances to nearest neighbors\n    sigma[i] &lt;- sd(nearest_neighbors)\n    sigma[i] &lt;- max(sigma[i] , 1e-4)\n    # Avoid sigma being zero\n   # sigma &lt;- sigma+\n  }\n  \n  return(sigma)\n}\n\n# Compute sigma for original distances D (high-dimensional space)\nsigma_D &lt;- compute_sigma(D, 10)\n\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n       P[i, j] &lt;- (D[i, j] /( sigma_D[i]))\n    }\n  }\n}\nsorted_indices &lt;- order(D[1, ])\nsorted_D &lt;- D[1, sorted_indices]\nsorted_P &lt;- P[1, sorted_indices]\n\n\n\n# Plot  line with sorted x and y values\nplot_ly(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines') %&gt;%\n  layout(title = \"Line plot of D vs (D/sigma) 10 NN (Smoothed)\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"D/sigma\"))\n\n\n\n\n\n\nWe calculated the sigma based on 10 nearest neighbors for each point and divide the original distance by each sigma. I’m showing the distance of the point 1 to all other points here. Admittedly, it did not do much. We are just scaling the distances. Obviously it will have small impact on the lower dimension but still it is a global measure. What we are after is to decay the distances as they get larger and we want to do it smoothly. One option would be to apply exponential function here on the negative distances.\n\\[\nK(x, x') = \\exp\\left( -\\frac{\\|x - x'\\|}{\\sigma} \\right)\n\\]\n\n\nCode\n# Compute sigma for original distances D (high-dimensional space)\nsigma_D &lt;- compute_sigma(D, 10)\n\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n       P[i, j] &lt;- exp(-D[i, j] /( sigma_D[i]))\n    }\n  }\n}\nsorted_indices &lt;- order(D[1, ])\nsorted_D &lt;- D[1, sorted_indices]\nsorted_P &lt;- P[1, sorted_indices]\n\n\n\n# Plot  line with sorted x and y values\nplot_ly(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines') %&gt;%\n  layout(title = \"Line plot of D vs exp(-D/sigma) 10 NN (Smoothed)\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"exp(-D/sigma)\"))\n\n\n\n\n\n\nSo basically, \\(\\exp\\left(-\\frac{D[i,j]}{\\sigma_D[i]}\\right)\\) is converting the Euclidean distance into a similarity measure. The matrix \\(D[i,j]\\) represents the Euclidean distance between points \\(i\\) and \\(j\\). A larger value of \\(D[i,j]\\) means the points are farther apart, while a smaller value indicates they are closer. The exponential function \\(\\exp(-x)\\) decreases rapidly as \\(x\\) increases, meaning that larger distances result in smaller similarities. When the distance \\(D[i,j]\\) is small, \\(\\exp(-D[i,j])\\) is close to 1, indicating high similarity. When the distance is large, the exponential value approaches 0, indicating low similarity. As said before dividing by \\(\\sigma_D[i]\\) allows us to control the “spread” or sensitivity of the similarity function. If \\(\\sigma_D[i]\\) is small, the function decays quickly, meaning only very close points will be considered similar. If \\(\\sigma_D[i]\\) is large, the decay is slower, allowing points that are further away to still be considered somewhat similar. Let’s see the effect of \\(\\sigma_D[i]\\) based on the number of nearest neighbors.\n\n\nCode\n# Number of neighbors to test\nneighbor_values &lt;- c(5, 10, 20, 30,50)\n\n# Initialize an empty plotly object\np &lt;- plot_ly()\n\n# Loop over different neighbor values\nfor (neighbors in neighbor_values) {\n  \n  # Compute sigma for the current number of neighbors\n  sigma_D &lt;- compute_sigma(D, neighbors)\n  \n  # Compute P matrix based on sigma_D\n  P &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        P[i, j] &lt;- exp(-D[i, j] / sigma_D[i])\n      }\n    }\n  }\n  \n  # Sort the first row of D and P for plotting\n  sorted_indices &lt;- order(D[1, ])\n  sorted_D &lt;- D[1, sorted_indices]\n  sorted_P &lt;- P[1, sorted_indices]\n  \n  # Add a trace to the plot for the current number of neighbors\n  p &lt;- p %&gt;%\n    add_trace(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines',\n              name = paste0(neighbors, \" NN\"))\n}\n\n# Customize the layout\np &lt;- p %&gt;%\n  layout(title = \"Line plot of D vs exp(-D/sigma) for different neighbors\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"exp(-D/sigma)\"))\n\np\n\n\n\n\n\n\nAs we increase the number of neighbors, the standard deviation will increase which allows the distant points to be considered similar. The formula we just presented (\\(\\exp\\left(-\\frac{D[i,j]}{\\sigma_D[i]}\\right)\\)) looks very similar to the well-known Gaussian (or normal) distribution function. In statistics, the Gaussian distribution is used to describe data that cluster around a mean, with the probability of a value decreasing as it moves further from the center.\nThe Gaussian (or normal) distribution’s probability density function for a single-dimensional random variable \\(x\\) is given by:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n\\]\nNotice the key similarities:\n\nThe negative exponential term,\nThe squared difference \\((x - \\mu)^2\\),\nThe factor \\(2\\sigma^2\\) in the denominator.\n\nIn the Gaussian distribution, the distance between a value and the mean is squared, meaning that both positive and negative deviations contribute equally to the distribution. Squaring ensures that large deviations from the mean (or between points, in our case) are penalized more heavily. This same principle is applied when transforming distances into similarities: squaring the distance emphasizes larger distances, making the similarity decrease more sharply as the distance gets bigger.\nWe also have the factor \\(2\\sigma^2\\) in the Gaussian function. This normalizes the spread of the data based on the variance. In our case, this factor ensures that the similarity measure accounts for the relative spread of the distances. So, \\(\\sigma^2\\) controls how quickly the similarity decays as the distance increases. A small \\(\\sigma^2\\) causes the similarity to drop off quickly, meaning only very close points are considered similar. Conversely, a larger \\(\\sigma^2\\) results in a slower decay, making points further apart still somewhat similar. The factor of 2 in \\(2\\sigma^2\\) ensures that this behavior is aligned with the properties of the Gaussian function, which decays at a rate proportional to the variance.\nWe can now transform our similarity function into the more commonly recognized Gaussian kernel by incorporating the squared Euclidean distance and the factor \\(2\\sigma^2\\): \\[\n      K(i,j) = \\exp\\left( -\\frac{D[i,j]^2}{2\\sigma_D[i]^2} \\right)\n\\]\nThis will look like:\n\n\nCode\n# Initialize an empty plotly object\n\n\nsigma_D &lt;- compute_sigma(D, 10)\n\n   # Compute P matrix based on sigma_D\n  P &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      }\n    }\n  }\n  \n  # Sort the first row of D and P for plotting\n  sorted_indices &lt;- order(D[1, ])\n  sorted_D &lt;- D[1, sorted_indices]\n  sorted_P &lt;- P[1, sorted_indices]\n  \n\n  \n# Customize the layout\nplot_ly(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines')%&gt;%\n  layout(title = \"Line plot of D vs exp(-D^2/(2*sigma^2)) for different neighbors\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"exp(-D^2/(2*sigma^2))\"))\n\n\n\n\n\n\nSo in summary, we calculate the standard deviation of distances for each individual sample based on its neighborhood. This standard deviation, \\(\\sigma_D[i]\\), reflects how spread out the distances are around that specific point, giving us a local measure of distance variability for each sample.\nThen, we use the Gaussian kernel to measure the similarity between points. The Gaussian kernel transforms the Euclidean distance between two points into a similarity score that decays exponentially with increasing distance. The similarity between two points is determined based on how close they are relative to the local standard deviation of distances.\nThere is one more step to do here. The distances for each point are now based on its own standard deviation (SD), which means they are likely on different scales because each point has a different SD reflecting its local neighborhood density. This variability makes it challenging to compare distances directly across different points since each distance is relative to its local scale. It would be nice to have distances that have the same meaning and statistical propery throughout the dataset. To address this, we convert the distances into probabilities, which normalize the values to a common scale and ensure that they are comparable. The conversion to probabilities for each point (i) is done as follows:\n\\[\np_{j|i} = \\frac{\\exp\\left(-\\frac{D_{ij}^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{D_{ik}^2}{2\\sigma_i^2}\\right)}\n\\]\nWhere:\n\n\\(D_{ij}\\) is the distance between points \\(i\\) and \\(j\\),\n\\(\\sigma_i\\) is the standard deviation of distances for point \\(i\\),\n\\(p_{j|i}\\) is the probability of selecting point \\(j\\) given point \\(i\\).\n\nThe numerator \\(\\exp\\left(-\\frac{D_{ij}^2}{2\\sigma_i^2}\\right)\\) ensures that smaller distances (closer neighbors) are given higher probabilities, while larger distances contribute less. The denominator \\(\\sum_{k \\neq i} \\exp\\left(-\\frac{D_{ik}^2}{2\\sigma_i^2}\\right)\\) ensures that all probabilities for a given point (i) sum to 1, bringing all points onto a comparable scale.\nThe entire equation computes the probability \\(p_{j|i}\\), which represents the likelihood of selecting point \\(j\\), given point \\(i\\), based on how close point \\(j\\) is to point \\(i\\). This probability is higher for points that are close to \\(i\\) and lower for points that are far from \\(i\\).\n\n\nCode\n# Initialize an empty plotly object\n\n\nsigma_D &lt;- compute_sigma(D, 10)\n\n   # Compute P matrix based on sigma_D\n  P &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n        \n      }\n    }\n    P[i, ] &lt;- P[i, ] / sum(P[i, ])\n  }\n  \n  # Sort the first row of D and P for plotting\n  sorted_indices &lt;- order(D[1, ])\n  sorted_D &lt;- D[1, sorted_indices]\n  sorted_P &lt;- P[1, sorted_indices]\n  \n\n  \n# Customize the layout\nplot_ly(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines')%&gt;%\n  layout(title = \"Line plot of D vs exp(-D^2/(2*sigma^2)) for different neighbors\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"exp(-D^2/(2*sigma^2))\"))\n\n\n\n\n\n\nYou can play around with the number of nearest neighbors to get a feeling of how the number of neighbors affect the distances.\n\n\nCode\ndata_2d&lt;-data[,1:2]\n\n\n\n# Select point 1\npoint_1 &lt;- data_2d[1, ]\n\n# Create a grid of points around point 1 for the Gaussian kernel\nx_seq &lt;- seq(min(data_2d$gene1) - 1, max(data_2d$gene1) + 1, length.out = 100)\ny_seq &lt;- seq(min(data_2d$gene2) - 1, max(data_2d$gene2) + 1, length.out = 100)\n\n# Function to compute sigma (standard deviation of distances for the first point)\ncompute_sigma &lt;- function(D, nn) {\n  sigma &lt;- numeric(1)\n  dist_to_point1 &lt;- D[1, ]\n  nearest_neighbors &lt;- sort(dist_to_point1, decreasing = FALSE)[2:(nn + 1)]\n  sigma[1] &lt;- sd(nearest_neighbors)\n  return(sigma)\n}\n\n# Function to create Gaussian kernel based on nn\ncreate_gaussian_kernel &lt;- function(nn) {\n\n  \n  # Get sigma for point 1\n  sigma &lt;- compute_sigma(D, nn)\n  \n  # Compute probabilities for all data points (Gaussian kernel)\n  gaussian_probabilities &lt;- numeric(nrow(data_2d))\n  for (i in 1:nrow(data_2d)) {\n    dist_sq &lt;- (data_2d$gene1[i] - point_1$gene1)^2 + (data_2d$gene2[i] - point_1$gene2)^2\n    gaussian_probabilities[i] &lt;- exp(-dist_sq / (2 * sigma^2))\n  }\n  \n  # Normalize the probabilities (sum to 1)\n  gaussian_probabilities &lt;- gaussian_probabilities / sum(gaussian_probabilities)\n  \n  # Create kernel values for contour plot\n  gaussian_kernel &lt;- matrix(0, nrow = length(x_seq), ncol = length(y_seq))\n  for (i in 1:length(x_seq)) {\n    for (j in 1:length(y_seq)) {\n      dist_sq &lt;- (x_seq[i] - point_1$gene1)^2 + (y_seq[j] - point_1$gene2)^2\n      gaussian_kernel[i, j] &lt;- exp(-dist_sq / (2 * sigma^2))\n    }\n  }\n  \n  return(list(kernel = as.vector(gaussian_kernel), probabilities = gaussian_probabilities))\n}\n\n# Create steps for slider (each step corresponds to a nn value)\nnn_values &lt;- seq(2, 50, by = 1)\nsteps &lt;- list()\nfor (i in 1:length(nn_values)) {\n  nn &lt;- nn_values[i]\n  result &lt;- create_gaussian_kernel(nn)\n  \n  steps[[i]] &lt;- list(\n    args = list(\n      list(z = list(result$kernel), \"marker.color\" = list(result$probabilities))\n    ),\n    label = paste0(\"nn: \", nn),\n    method = \"restyle\"\n  )\n}\n\n# Initial nn and kernel\ninitial_nn &lt;- 10\ninitial_result &lt;- create_gaussian_kernel(initial_nn)\n\n# Convert grid and kernel matrix into a data frame for plotting\nkernel_df &lt;- expand.grid(x = x_seq, y = y_seq)\nkernel_df$z &lt;- initial_result$kernel\n\n# Plot 2D data points and Gaussian kernel around point 1\nfig &lt;- plot_ly() %&gt;%\n  add_trace(data = data_2d, x = ~gene1, y = ~gene2, type = 'scatter', mode = 'markers',\n            marker = list(size = 10, color = initial_result$probabilities, showscale = TRUE),\n            name = 'Data Points') %&gt;%\n  add_trace(x = kernel_df$x, y = kernel_df$y, z = kernel_df$z, type = 'contour',\n            contours = list(showlabels = TRUE), name = 'Gaussian Kernel', showscale = F) %&gt;%\n  layout(title = \"Interactive Gaussian Kernel\",\n         sliders = list(\n           list(\n             active = 8,  # Set the active index (for nn = 10 in our steps)\n             currentvalue = list(prefix = \"\"),\n             pad = list(t = 60),\n             steps = steps\n           )\n         ))\n\n# Show the interactive plot with the slider\nfig\n\n\n\n\n\n\nAs you adjust the number of nearest neighbors (nn) using the slider, you’ll notice a significant change in the probability assigned to the points, even though their positions remain fixed. This is because the Gaussian kernel adapts based on the local neighborhood of point_1, and its standard deviation (σ) increases with more neighbors included.\n\nSmaller Number of Neighbors (Low nn):\n\nWhen the number of nearest neighbors is small, the Gaussian kernel is tighter around point_1. This means the local neighborhood is defined narrowly, and only points that are very close to point_1 have high probabilities.\nAs a result, the probability for points near point_1 is high because these points are strongly influenced by the local structure around point_1. In the contour plot, this is reflected by the smaller, more concentrated yellow region, indicating that fewer points have high probabilities.\n\nLarger Number of Neighbors (High nn):\n\nAs you increase the number of nearest neighbors, the Gaussian kernel widens, taking more distant points into account. This effectively means the local neighborhood grows, and the probability is distributed across a larger area.\nFor the same point that previously had a high probability (when the neighborhood was small), its probability will now decrease because the influence of point_1 is spread out over a larger region. More points are included in the neighborhood, but the individual contribution from each point is reduced.\nThis is visually represented by the expansion of the yellow area in the contour plot, where more points now have non-negligible probabilities, but the magnitude of the probabilities for any specific point (including those close to point_1) is lower.\n\n\nThe probability assigned to an identical point is different under different numbers of nearest neighbors because the Gaussian kernel adapts based on the local neighborhood size. With fewer neighbors, the kernel is more focused, leading to higher probabilities for nearby points. As the number of neighbors increases, the kernel spreads out, and the probability for each individual point decreases, even though the point’s location remains unchanged.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#gaussian-kernel-for-low-dimentions",
    "href": "tsne.html#gaussian-kernel-for-low-dimentions",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.2 Gaussian kernel for low dimentions",
    "text": "5.2 Gaussian kernel for low dimentions\nNow we need to think about he low dimentional space. We can use the same formula:\n\\[\nq_{j|i} = \\frac{\\exp\\left(-Dz_{ij}^2\\right)}{\\sum_{k \\neq i} \\exp\\left(-Dz_{ik}^2\\right)}\n\\] Here \\(Dz\\) is the distances in the lower dimensional space (i will come to the point where we define the lower dimension). Here the normalization is over all data points.\nIt is important that once the data is projected into the low-dimensional space, the idea is to preserve local neighborhoods as faithfully as possible. In low-dimensional space, we are no longer working with highly varying densities like you have in high-dimensional space. Instead, points that are similar should already be close together in this space. Therfore, we don’t need dynamic standard deviation (SD) estimation. The aim here is to presernve the distance under this specific distribution.\n\n5.2.1 Making things symmetric\nSo far, we have measured how similar data points are in both high-dimensional and low-dimensional spaces. To do this, we computed conditional probabilities for each point, but these are inherently asymmetric. Basicaly, the conditional probability \\(p_{j|i}\\) is calculated based on the similarity between point \\(x_i\\) and point \\(x_j\\) in the high-dimensional space. This is done using a Gaussian kernel:\n\\[\np_{j|i} = \\frac{\\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)}\n\\]\nHowever, these conditional probabilities are not symmetric: \\(p_{j|i}\\) is not equal to \\(p_{i|j}\\) because they are computed separately for \\(x_i\\) and \\(x_j\\), with different reference points and sigma.\nFor example, if \\(x_i\\) is an outlier far from the rest of the data, \\(p_{j|i}\\) will be very small for all \\(j\\), but \\(p_{i|j}\\) might still be large if \\(x_j\\) is close to other points.\nTo deal with this asymmetry, we compute the joint probability \\(p_{ij}\\), which is symmetric:\n\\[\np_{ij} = \\frac{p_{i|j} + p_{j|i}}{2n}\n\\]\nThis ensures that \\(p_{ij} = p_{ji}\\), meaning the similarity between \\(x_i\\) and \\(x_j\\) is the same, regardless of the direction of comparison. Without symmetrization, outliers or points with unusual distances could have a disproportionately small or large influence on the our lower dimensions. This also ensure that sum of all probabilities is 1. We again need to calculate the gradient to update the lower dimension.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#similarity-between-higher-and-lower-dimentions",
    "href": "tsne.html#similarity-between-higher-and-lower-dimentions",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.3 Similarity between higher and lower dimentions",
    "text": "5.3 Similarity between higher and lower dimentions\nIn order to say how different our higher and lower dimentions are we are going to use KL (Kullback-Leibler) divergence. This is a measure of how one probability distribution is different from another. We have from before\n\n\\(P\\) represents similarities (or probabilities) in high-dimensional space.\n\\(Q\\) represents similarities in the lower-dimensional space (our reduced embedding).\n\nThe goal is to minimize the difference between these two distributions so that the lower-dimensional embedding represents the data as faithfully as possible.\nMathematically, the KL divergence between the high-dimensional and low-dimensional distributions is expressed as:\n\\[\nC = \\sum_i KL(P_i || Q_i) = \\sum_i \\sum_j p_{j|i} \\log \\frac{p_{j|i}}{q_{j|i}}\n\\]\n\n\\(p_{j|i}\\): Probability that point \\(j\\) is similar to point \\(i\\) in high-dimensional space.\n\\(q_{j|i}\\): Probability that point \\(j\\) is similar to point \\(i\\) in low-dimensional space.\n\nThis basically tells us how surprised we are by the difference between the two distributions.\nSimilar to before, we need to calculate the gradient to update the lower dimension. The gradient of the KL divergence with respect to \\(y_i\\) is:\n\\[\n   \\frac{\\delta C}{\\delta y_i} = 4 \\sum_j \\left( p_{j|i} - q_{j|i} \\right) (y_i - y_j)\n\\] Here \\(p_{j|i} - q_{j|i}\\) is showing the difference how much the high-dimensional similarity \\(p_{j|i}\\) differs from the low-dimensional similarity \\(q_{j|i}\\). If they differ significantly, the gradient will be larger. \\(y_i - y_j\\) is the distance between the points \\(y_i\\) and \\(y_j\\) in the low-dimensional space. It shows how far the two points are in the lower dimentions, and moving \\(y_i\\) toward \\(y_j\\) will help reduce the divergence.\n\n\n\n\n\n\nCost function\n\n\n\nI’m not going to include the cost function (and tolerance in the code) to speed things up. Maybe you want to do that and check for tolerance?\n\n\nAfter calculating the gradient, we can use it to update the position of \\(y_i\\) in the lower-dimensional space. The update rule looks like this:\n\\[\n   y_i \\leftarrow y_i - \\eta \\frac{\\delta C}{\\delta y_i}\n\\]\nWhere \\(\\eta\\) is the learning rate controlling the step size.\nLet’s try to implement this\n\n\nCode\n# calculate orignal distances\nD&lt;-as.matrix(dist(data[,1:3]))\n# Function to compute local standard deviation based on k-nearest neighbors\ncompute_sigma &lt;- function(dist_matrix, k_neighbors) {\n  n &lt;- nrow(dist_matrix)\n  sigma &lt;- numeric(n)\n  \n  for (i in 1:n) {\n    # Get distances from point i to all other points\n    dists &lt;- dist_matrix[i, ]\n    \n    # Sort the distances and select k nearest neighbors (excluding self)\n    nearest_neighbors &lt;- sort(dists, decreasing = FALSE)[2:(k_neighbors + 1)]\n    \n    # Compute standard deviation of distances to nearest neighbors\n    sigma[i] &lt;- sd(nearest_neighbors)\n    sigma[i] &lt;- max(sigma[i] , 1e-4)\n    # Avoid sigma being zero\n    # sigma &lt;- sigma+\n  }\n  \n  return(sigma)\n}\n\n# Compute sigma for original distances D (high-dimensional space)\nsigma_D &lt;- compute_sigma(D, 10)\n\n# Compute P matrix based on sigma_D\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n      P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      \n    }\n    \n  }\n   P[i, ] &lt;- P[i, ] / sum(P[i,])\n}\nP = (P + t(P))/(2*nrow(D))\n\n# Create random low dimention\nndim &lt;- 2\nset.seed(12345)\nY &lt;- matrix(rnorm(nrow(D) * ndim, sd = 1, mean = 0), nrow = nrow(D), ncol = ndim)\ncolnames(Y) &lt;- c(\"dim1\", \"dim2\")\n\n# define data for plot\n\nplot_frames_sne&lt;-c()\nfor (iter in 1:1000) {\n  Dy &lt;- as.matrix(dist(Y))\n  \n  Q &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        # Calculate conditional probability q_j|i using the same Gaussian kernel\n        Q[i, j] &lt;- exp(-(Dy[i, j])^2)\n        \n      }\n    }\n  }\n\nQ &lt;- Q / sum(Q)\n  grad &lt;- matrix(0, nrow = nrow(D), ncol = ndim)\n  # For each point i and j, compute the gradient of KL divergence\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n         diff_y &lt;- Y[i, ] - Y[j, ]\n        scaling_factor &lt;- (P[i, j] - Q[i, j])\n        grad[i, ] &lt;- grad[i, ] + scaling_factor * diff_y\n        \n        \n      }\n    }\n     grad[i, ]&lt;-4* grad[i, ]\n  }\n  Y &lt;- Y - 5 * (grad)\n\n  if (iter %% 50 == 0) {\n    plot_frames_sne&lt;-rbind(plot_frames_sne,data.frame(Y, cluster = data$cluster, subcluster = data$subcluster,iteration=iter))\n\n  }\n  \n}\n\n# \nplot_ly(\n  as.data.frame(plot_frames_sne), x = ~dim1, y = ~dim2,frame=~iteration,color=~cluster,symbol=~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  mode = \"markers\",size = 5\n)  %&gt;%\n  layout(showlegend = FALSE,\n    title = \"SNE 2D Data with Clusters (animation)\"\n  )\n\n\n\n\n\n\nClick on Play to see how this algorithm maps the higher dimention to the lower one by starting from random points in the lower dimention and progressively change them so they get more and more similar with respect to thier distance to the higher dimension. In fact that process that we saw is symmetric SNE algortihm. The results are promising but we still have work to do to make it better!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#perplexity-effective-number-of-neighbors",
    "href": "tsne.html#perplexity-effective-number-of-neighbors",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.4 Perplexity (Effective number of neighbors)",
    "text": "5.4 Perplexity (Effective number of neighbors)\nSo far we have been using standard deviation that has been directly calculated based on the data points’s nearest neighbors. This however has a potential problem. Using this kernel, we convert distances between points in high-dimensional space into probabilities that represent similarities between points. For two points \\(i\\) and \\(j\\), the probability that \\(j\\) is a neighbor of \\(i\\) is computed using a Gaussian distribution:\n\\[\np_{ij} = \\frac{\\exp \\left( - \\frac{d_{ij}^2}{2 \\sigma_i^2} \\right)}{\\sum_{k \\neq i} \\exp \\left( - \\frac{d_{ik}^2}{2 \\sigma_i^2} \\right)}\n\\]\nAgain here, \\(\\sigma_i\\) is the standard deviation used for point \\(i\\), and \\(d_{ij}\\) is the distance between points \\(i\\) and \\(j\\). The choice of \\(\\sigma_i\\) determines how much weight we assign to close vs. distant neighbors when computing these probabilities.\nSo far we have used the real standard deviation (based on actual distances to neighbors in high-dimensional space). The problem is in dense regions, where many points are packed closely together, the real standard deviation \\(\\sigma_i\\) will be small because the average distance to neighbors is small. A small \\(\\sigma_i\\) makes the Gaussian distribution very narrow. his means that only the very closest neighbors will have significant probabilities, while slightly more distant neighbors (even if they aren’t that far away) will have almost zero probability. This causes the probabilities $ p_{ij}$ to focus too much on the nearest neighbors, potentially ignoring important local structure beyond the immediate vicinity. With certian number neighbors selected, points in dense clusters might appear farther apart than they should in the lower dimensions. Local clusters that should be tight could become overly spread out.\nIn sparse regions, where points are more spread out, the real standard deviation \\(\\sigma_i\\) will be larger because the average distance to neighbors is larger. A large \\(\\sigma_i\\) makes the Gaussian distribution wider. This means that the probabilities will be more spread out across neighbors, and distant neighbors will get significant probabilities, even though they are far away. In this case, the algorithm might overestimate the importance of distant neighbors, leading to blurring of the local structure in these regions.\nLet’s have a look at an example:\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of points per subcluster\nn &lt;- c(8,6,4,2)\n\n# Manually define main cluster centers (global structure)\nmain_cluster_centers &lt;- data.frame(\n  x = c(0, 5, 10, 15),\n  y = c(0, 5, 10, 15),\n  z = c(0, 5, 10, 15),\n  cluster = factor(1:4)\n)\n\n# Manually define subcluster offsets relative to each main cluster\n# These small offsets will determine subcluster locations within each main cluster\nsubcluster_offsets &lt;- data.frame(\n  x_offset = c(-0.25, 0.25, -0.25, 0.25),\n  y_offset = c(-0.25, -0.25, 0.25, 0.25),\n  z_offset = c(0.25, -0.25, -0.25, 0.25),\n  subcluster = factor(1:4)\n)\n\n# Initialize an empty data frame to hold all data\ndata &lt;- data.frame()\n\n# Generate data for each main cluster with subclusters\n\nfor (i in 1:nrow(main_cluster_centers)) {\n  for (j in 1:nrow(subcluster_offsets)) {\n    # Calculate subcluster center by adding the offset to the main cluster center\n    subcluster_center &lt;- main_cluster_centers[i, 1:3] + subcluster_offsets[j, 1:3]\n    \n    # Generate points for each subcluster with a small spread (to form local clusters)\n    subcluster_data &lt;- data.frame(\n      gene1 = rnorm(n[i], mean = subcluster_center$x, sd = 2),  # Small spread within subclusters\n      gene2 = rnorm(n[i], mean = subcluster_center$y, sd = 2),\n      gene3 = rnorm(n[i], mean = subcluster_center$z, sd = 2),\n      cluster = main_cluster_centers$cluster[i],\n      subcluster = subcluster_offsets$subcluster[j]\n    )\n    \n    # Add generated subcluster data to the main data frame\n    data &lt;- rbind(data, subcluster_data)\n  }\n}\n\n\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\n\n\n\n\n\n\nDifferent clusters have the same standard deviation but i have sampled different number of points. Cluster one is the densest cluster and the cluster four is the sparse one.\nLet’s see what our algorithm does on this:\n\n\nCode\nD&lt;-as.matrix(dist(data[,1:3]))\n# Function to compute local standard deviation based on k-nearest neighbors\ncompute_sigma &lt;- function(dist_matrix, k_neighbors) {\n  n &lt;- nrow(dist_matrix)\n  sigma &lt;- numeric(n)\n  \n  for (i in 1:n) {\n    # Get distances from point i to all other points\n    dists &lt;- dist_matrix[i, ]\n    \n    # Sort the distances and select k nearest neighbors (excluding self)\n    nearest_neighbors &lt;- sort(dists, decreasing = FALSE)[2:(k_neighbors + 1)]\n    \n    # Compute standard deviation of distances to nearest neighbors\n    sigma[i] &lt;- sd(nearest_neighbors)\n    sigma[i] &lt;- max(sigma[i] , 1e-4)\n    # Avoid sigma being zero\n    # sigma &lt;- sigma+\n  }\n  \n  return(sigma)\n}\n\n#compute\nsigma_D  &lt;-compute_sigma(D,10)\n\n# Compute P matrix based on sigma_D\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n      P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      \n    }\n    \n  }\n  P[i, ] &lt;- P[i, ] / sum(P[i,])\n}\nP = (P + t(P))/(2*nrow(D))\n\nndim &lt;- 2\nset.seed(12345)\nY &lt;- matrix(rnorm(nrow(D) * ndim, sd = 1, mean = 0), nrow = nrow(D), ncol = ndim)\ncolnames(Y) &lt;- c(\"dim1\", \"dim2\")\n\n\nfor (iter in 1:2000) {\n  Dy &lt;- as.matrix(dist(Y))\n  \n  Q &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        # Calculate conditional probability q_j|i using the same Gaussian kernel\n        Q[i, j] &lt;- exp(-(Dy[i, j])^2)\n        \n      }\n    }\n  }\n  \n  Q &lt;- Q / sum(Q)\n  grad &lt;- matrix(0, nrow = nrow(D), ncol = ndim)\n  # For each point i and j, compute the gradient of KL divergence\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        diff_y &lt;- Y[i, ] - Y[j, ]\n        scaling_factor &lt;- (P[i, j] - Q[i, j])\n        grad[i, ] &lt;- grad[i, ] + scaling_factor * diff_y\n        \n        \n      }\n    }\n    grad[i, ]&lt;-4* grad[i, ]\n  }\n  Y &lt;- Y - 10 * (grad)\n\n  \n}\nplot_ly(\n  data.frame(Y), x = ~dim1, y = ~dim2,color = ~data$cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n) \n\n\n\n\n\n\nDespite that for the sparse cluster things look OK but for the denser regions we spread the points unnecessarily. They should infact form tight clusters. We need a better measure of nearest neighbors that is more or less adaptive to the density of each region in the map. In this specific case, we probably have to consider a bit more neighbors in denser part of the data.\nIn an ideal situation we for a specific point, we want to have neighbors that more or less have same influence on the current point. So basically, we need a method that ensures each point considers a consistent number of neighbors, significantly influencing it across the entire dataset, regardless of local density variations.\nOne intiuative way to do that is to rather than setting \\(\\sigma_i\\) solely based on nearest neighbor distances, we can aim to adjust \\(\\sigma_i\\) so that each point considers the same effective number of neighbors.\nNow consider this, if all neighbors contributed equally, the probabilities \\(p_{ij}\\) would be uniformly distributed. For instance, if there were \\(k\\) neighbors, and each contributed equally, then:\n\\[\np_{ij} = \\frac{1}{k}, \\quad \\text{for each } j\n\\]\nHowever, in reality, the distances between the points vary, which means the probabilities \\(p_{ij}\\) are not uniform. Our task is to find a way to measure how far the actual distribution \\(p_{ij}\\) is from being uniform. To measure this, we can introduce a simple criterion: if we want to have \\(k\\) neighbors contributing equally, the sum of the probabilities for those neighbors should behave as if they are uniformly distributed across the \\(k\\) neighbors.\nTo express this mathematically, we can look at the average contribution of each neighbor and compare it with how far off each neighbor’s actual contribution is. So, we want a measure that tells us ff all probabilities \\(p_{ij}\\) are equal, this measure should indicate that all neighbors are contributing equally. And also If one or a few \\(p_{ij}\\)’s dominate, this measure should indicate that some neighbors are contributing more than others.\nLet’s consider the concept of information content associated with each probability \\(p_{ij}\\). The smaller \\(p_{ij}\\), the more surprising (or informative) it is that neighbor \\(j\\) has a significant influence. The information content for each neighbor \\(j\\) can be defined as:\n\\[\n\\text{Information content of } p_{ij} = -\\log(p_{ij})\n\\]\nThis makes sense because if \\(p_{ij}\\) is large (close neighbors), the information content is small, indicating that these neighbors are not surprising. But if \\(p_{ij}\\) is small (distant neighbors), the information content is large, indicating that these neighbors are more surprising.\nNow, to get a measure of the average information content of all the neighbors, we can take the weighted sum of these individual information contents, weighted by the probabilities ( p_{ij} ) themselves:\n\\[\n\\text{Average information content} = - \\sum_j p_{ij} \\log_2(p_{ij})\n\\]\nThis expression tells us the average uncertainty or spread in the probability distribution \\(P_i\\), based on the Gaussian distances. This is exactly the formula for Shannon entropy (\\(H\\)).\nNow let’s consider a super good scenario where all the neighbors are contributing euqally to the current point.\n\\[\nH(P_i) = -\\sum_{i=1}^{n} \\frac{1}{n} \\log_2 \\frac{1}{n} = \\log_2 n\n\\]\nThis is straightforward: the entropy is just the logarithm of the number of outcomes, which makes sense because more outcomes increase uncertainty. How do we transfomr this \\(log_2(n)\\) to the number of neighbors? We simply take \\(2^{H(P_i)}\\). In this case this is going to give us exactly \\(n\\). In cases where the distances are not equal, \\(H(P_i)\\) is no longer a simple logarithm of the number of outcomes. Instead, the value of \\(H(P_i)\\) reflects the uncertainty considering the actual probability distribution. However, we can still think about how many neighbors with equal contribution would generate the same amount of uncertainty. So we can still use \\(2^{H(P_i)}\\) which is going to give us the effective number of neighbors for a particular point. In our case, this parameter is called perplexity. The perplexity tells us the effective number of equally contributing neighbors and we want to adjust \\(\\sigma_i\\) so perplexity matches a desired number. That number is our number of neighbors (A smooth one!).\nHow do we find all \\(\\sigma_i\\)? Well, we have to search really. Either grid search or binary search etc can help us to figure out all the standard deviation for every single data point such that perplexity matches the desired value.\nHere i have changed our function to do that.\n\n\nCode\n# Function to compute perplexity given distances D and sigma\ncompute_perplexity &lt;- function(D, sigma) {\n  # Compute the conditional probabilities P_{j|i}\n  P &lt;- exp(-D^2 / (2 * sigma^2))\n  # Avoid division by zero\n  sumP &lt;- sum(P) + 1e-10\n  P &lt;- P / sumP\n  # Compute Shannon entropy H(P_i)\n  H &lt;- -sum(P * log2(P + 1e-10))\n  # Compute perplexity\n  perplexity &lt;- 2^H\n  return(perplexity)\n}\n\n# Function to find sigma given distances D and target perplexity\nfind_sigma &lt;- function(D, target_perplexity, tol = 1e-5, max_iter = 100, sigma_min = 1e-20, sigma_max = 1000) {\n  # Initialize sigma bounds\n  sigma_lower &lt;- sigma_min\n  sigma_upper &lt;- sigma_max\n  # Initial sigma guess\n  sigma &lt;- (sigma_lower + sigma_upper) / 2\n  for (i in 1:max_iter) {\n    # Compute perplexity with current sigma\n    perp &lt;- compute_perplexity(D, sigma)\n    # Check if perplexity is within tolerance\n    if (abs(perp - target_perplexity) &lt; tol) {\n      break\n    }\n    # Adjust sigma bounds based on perplexity\n    if (perp &gt; target_perplexity) {\n      # Perplexity too big, decrease sigma\n      sigma_upper &lt;- sigma\n    } else {\n      # Perplexity too small, increase sigma\n      sigma_lower &lt;- sigma\n    }\n    # Update sigma\n    sigma &lt;- (sigma_lower + sigma_upper) / 2\n  }\n  return(sigma)\n}\ncompute_sigma&lt;-function(D,perplexity)\n{\n  sigmas&lt;-c()\n  for(i in 1:nrow(D)){\n  sigmas&lt;-c(sigmas,find_sigma(D[i,-i], perplexity,max_iter = 1000))\n}\nsigmas\n}\n\n#compute\nD&lt;-as.matrix(dist(data[,1:3]))\nsigma_D  &lt;-compute_sigma(D,10)\n\n# Compute P matrix based on sigma_D\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n      P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      \n    }\n    \n  }\n  P[i, ] &lt;- P[i, ] / sum(P[i,])\n}\nP = (P + t(P))/(2*nrow(D))\n\nndim &lt;- 2\nset.seed(12345)\nY &lt;- matrix(rnorm(nrow(D) * ndim, sd = 1, mean = 0), nrow = nrow(D), ncol = ndim)\ncolnames(Y) &lt;- c(\"dim1\", \"dim2\")\n\n# define data for plot\n\nplot_frames_sne&lt;-c()\nfor (iter in 1:2000) {\n  Dy &lt;- as.matrix(dist(Y))\n  \n  Q &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        # Calculate conditional probability q_j|i using the same Gaussian kernel\n        Q[i, j] &lt;- exp(-(Dy[i, j])^2)\n        \n      }\n    }\n  }\n  \n  Q &lt;- Q / sum(Q)\n  grad &lt;- matrix(0, nrow = nrow(D), ncol = ndim)\n  # For each point i and j, compute the gradient of KL divergence\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        diff_y &lt;- Y[i, ] - Y[j, ]\n        scaling_factor &lt;- (P[i, j] - Q[i, j])\n        grad[i, ] &lt;- grad[i, ] + scaling_factor * diff_y\n        \n        \n      }\n    }\n    grad[i, ]&lt;-4* grad[i, ]\n  }\n  Y &lt;- Y - 5 * (grad)\n}\nplot_ly(\n  data.frame(Y), x = ~dim1, y = ~dim2,color = ~data$cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n) \n\n\n\n\n\n\nWe now have got much nicer clustering of the data. The gradients are kept and most things seem to be in place. We are very close in wrapping things up but there is still one thing left we need to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#crowding-problem",
    "href": "tsne.html#crowding-problem",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.5 Crowding problem",
    "text": "5.5 Crowding problem\nThe crowding problem arises because high-dimensional data points, when reduced to a lower-dimensional space, often become “crowded” in a way that fails to preserve the relative distances properly. This happens because it is difficult to accurately represent the larger pairwise distances in the lower-dimensional map, causing too many points to cluster closely together. So far our idea of dimensionality reduction has been to preserve the relative distances between points by converting distances into probabilities in both the high-dimensional and low-dimensional spaces. The relationship between probability and distance is important. Basicaly, the closer points in the high-dimensional spaceshould have higher probabilities of being close in the lower-dimensional map, while distant points should have lower probabilities of being near each other in the lower-dimensional map.\nWe used a Gaussian kernel in the lower-dimensional space, in this kernel the probabilities for distant points decrease rapidly because the Gaussian distribution assigns very small probabilities to pairs of points that are even moderately far apart. This means that even points that are moderately distant in the high-dimensional space will be assigned a very low probability of being far apart in the lower-dimensional map.\nNow, since our algorithm tries to match the probabilities (reduce the pariwise difference) between the high-dimensional and low-dimensional spaces, if the Gaussian assigns very low probabilities for moderately distant points in the low-dimensional map, the gradient will try to pull those points closer together to match this. The problem is that these points are not meant to be so close (since they are distant in high-dimensional space), but the fast decay of the Gaussian distribution forces them closer than they should be to match the low probability.\nWe bassicaly need another kernel that is decay more slowly in the lower dimension. The t-distribution is the one! It assigns a bit larger probabilities to distant points compared to a Gaussian. This allows moderately distant points to remain appropriately separated in the low-dimensional map. The slower decay helps to prevent crowding because it doesn’t force moderately distant points to come together as much as the Gaussian does.\nSo we are going to use t-distribution with one degree of freedom (Student’s t-distribution) in the low-dimensional space instead of a Gaussian. T-distribution has heavier tails, which allow for larger distances between points. Mathematically, the probability of a pair of points \\(i\\) and \\(j\\) in the low-dimensional map is computed as:\n\\[\nq_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l}(1 + ||y_k - y_l||^2)^{-1}}\n\\] Our gradient is also going to change to\n\\[\n   \\frac{\\delta C}{\\delta y_i} = 4 \\sum_j \\left( p_{j|i} - q_{j|i} \\right) (y_i - y_j)(1+||y_i-y_j||^2)^-1\n\\] All the notations are the same as we present before. We can go ahead and implement this in our algorithm:\n\n\nCode\n#compute\nD&lt;-as.matrix(dist(data[,1:3]))\nsigma_D  &lt;-compute_sigma(D,10)\n\n# Compute P matrix based on sigma_D\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n      P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      \n    }\n    \n  }\n  P[i, ] &lt;- P[i, ] / sum(P[i,])\n}\nP = (P + t(P))/(2*nrow(D))\n\nndim &lt;- 2\nset.seed(12345)\nY &lt;- matrix(rnorm(nrow(D) * ndim, sd = 1, mean = 0), nrow = nrow(D), ncol = ndim)\ncolnames(Y) &lt;- c(\"dim1\", \"dim2\")\n\nfor (iter in 1:2000) {\n  Dy &lt;- as.matrix(dist(Y))\n  \n  Q &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        # Calculate conditional probability q_j|i using the same t-dist kernel\n        Q[i, j] &lt;- (1+(Dy[i, j])^2)^-1\n        \n      }\n    }\n  }\n  \n  Q &lt;- Q / sum(Q)\n  grad &lt;- matrix(0, nrow = nrow(D), ncol = ndim)\n  # For each point i and j, compute the gradient of KL divergence\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        diff_y &lt;- Y[i, ] - Y[j, ]\n        scaling_factor &lt;- (P[i, j] - Q[i, j])\n        grad[i, ] &lt;- grad[i, ] + scaling_factor * diff_y *((1+(Dy[i, j])^2)^-1)\n        \n        \n      }\n    }\n    grad[i, ]&lt;-4* grad[i, ]\n  }\n  Y &lt;- Y - 5 * (grad)\n  \n}\nplot_ly(\n  data.frame(Y), x = ~dim1, y = ~dim2,color = ~data$cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#t-sne-summary",
    "href": "tsne.html#t-sne-summary",
    "title": "1  t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.6 t-SNE (summary)",
    "text": "5.6 t-SNE (summary)\nThe algorithm that we derived together is t-SNE. Obviously we have missed/omitted a lot of implemntational details but the general way that this algorithm works is more or less the same as we went through. t-SNE works by find finding standard deviations for each data point based on the perplexity parameter. It then uses the gaussian kernel to convert the distances between the data points to probability. It does the same thing for the lower dimention and tries to match the higher and lower dimensional probabilities. This is done by minimizing the Kullback-Leibler (KL) divergence between the two distributions. By iteratively adjusting the points in the lower-dimensional space, t-SNE captures both local and global structures in the data, with an emphasis on preserving local neighborhoods. The result is a visually intuitive low-dimensional embedding that reflects the high-dimensional relationships between points, allowing us to observe clusters and patterns that may not have been apparent in the original space.\nIn order to use t-SNE effectively, it is important to carefully choose the perplexity parameter, as it controls the balance between local and global structure preservation. A higher perplexity value emphasizes larger neighborhoods, capturing more global relationships, while a lower value focuses on smaller, local clusters. Additionally, t-SNE can be sensitive to initialization and learning rate, so experimenting with these parameters can help avoid poor embeddings. Preprocessing the data, such as normalizing or reducing dimensions beforehand (e.g., with PCA), can also improve performance and stability of the algorithm.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "umap.html",
    "href": "umap.html",
    "title": "2  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)",
    "section": "",
    "text": "3 UMAP in R\nThere is a few good implementation on UMAP in R. We are going to use uwot package. It should work on most platforms. An alternative would be umap package uwot gives more flexebility in terms of functions it can perform and parameters etc.\nwe are going to use the same data that we have been using for t-SNE\nCode\n# Visualize the original data in 3D, distinguishing clusters and subclusters\nlibrary(dplyr)\nlibrary(plotly)  # For interactive 3D plots\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of points per subcluster\nn &lt;- 25\n\n# Manually define main cluster centers (global structure)\nmain_cluster_centers &lt;- data.frame(\n  x = c(0, 5, 10, 15),\n  y = c(0, 5, 10, 15),\n  z = c(0, 5, 10, 15),\n  cluster = factor(1:4)\n)\n\n# Manually define subcluster offsets relative to each main cluster\n# These small offsets will determine subcluster locations within each main cluster\nsubcluster_offsets &lt;- data.frame(\n  x_offset = c(-0.25, 0.25, -0.25, 0.25),\n  y_offset = c(-0.25, -0.25, 0.25, 0.25),\n  z_offset = c(0.25, -0.25, -0.25, 0.25),\n  subcluster = factor(1:4)\n)\n\n# Initialize an empty data frame to hold all data\ndata &lt;- data.frame()\n\n# Generate data for each main cluster with subclusters\nfor (i in 1:nrow(main_cluster_centers)) {\n  for (j in 1:nrow(subcluster_offsets)) {\n    # Calculate subcluster center by adding the offset to the main cluster center\n    subcluster_center &lt;- main_cluster_centers[i, 1:3] + subcluster_offsets[j, 1:3]\n    \n    # Generate points for each subcluster with a small spread (to form local clusters)\n    subcluster_data &lt;- data.frame(\n      gene1 = rnorm(n, mean = subcluster_center$x, sd = 0.25),  # Small spread within subclusters\n      gene2 = rnorm(n, mean = subcluster_center$y, sd = 0.25),\n      gene3 = rnorm(n, mean = subcluster_center$z, sd = 0.25),\n      cluster = main_cluster_centers$cluster[i],\n      subcluster = subcluster_offsets$subcluster[j]\n    )\n    \n    # Add generated subcluster data to the main data frame\n    data &lt;- rbind(data, subcluster_data)\n  }\n}\n\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~cluster,symbol=~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\nThis data has just three dimentions for 400 samples (4 major clusters within each we have subclusters). We are now going to do a UMAP (using umap function) on this data and see the results. We also do t-SNE and show the results side by side. There are parameters to set but we just go for the default ones except for n_neighbors in UMAP which we set to 30 to be similar to t-SNE.\nCode\nlibrary(Rtsne)\nlibrary(uwot)\n\nset.seed(123)\ntsne_results &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),perplexity = 30,\n)\n\nset.seed(123)\numap_results &lt;- umap(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),n_neighbors = 30,\n)\n\n\ntsne_plot &lt;- plot_ly(\n  as.data.frame(tsne_results$Y), x = ~V1, y = ~V2, color = ~data$cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\numap_plot &lt;- plot_ly(\n  as.data.frame(umap_results), x = ~V1, y = ~V2, color = ~data$cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n  \nsubplot(tsne_plot%&gt;%layout(showlegend = FALSE), umap_plot%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"t-SNE\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"UMAP\", showarrow = F, xref='paper', yref='paper'))\n)\nWhat we see here is lower dimension repsentation of our data (from 3D to 2D). Both methods did a great job (as expected!) to capture the clusters. We are going to leave t-SNE for now and only focus on how we can use UMAP and what we can do with it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)</span>"
    ]
  },
  {
    "objectID": "umap.html#most-important-parameters",
    "href": "umap.html#most-important-parameters",
    "title": "2  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)",
    "section": "3.1 Most important parameters",
    "text": "3.1 Most important parameters\nThis implementation of UMAP actually offers many parameters to configure. Fortunately, most of them can be left at their default values. However, to get started, there are three key parameters that we need to focus on for now:\n\nX: This represents the input data that we want to reduce in dimensionality. It can be any numerical dataset (or factor, more on this later), such as a matrix or data frame in R.\nn_components: The dimension of the space to transform the original data into.This is basically, the size of the lower dimension.\nn_neighbors: This parameter controls the size of the local neighborhood that UMAP uses for each point. It essentially determines how many nearby points influence the embedding of a given point, with smaller values focusing more on local structure and larger values considering broader relationships.\n\nWe can run the function by calling the umap(data,n_components=2,n_neighbors=10). Remember to set the seed for reproducibility.\nThe n_neighbors parameter is the most important here. We can visualize how it affects the outcome of dimensionality reduction.\n\n\nCode\numap_data_frames&lt;-c()\nfor(nn in c(5,10,15,20,30,50,100,200,300,400))\n{\n  \nset.seed(123)\numap_results &lt;- umap(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),n_neighbors = nn,\n)\n  umap_data_frames&lt;-rbind(umap_data_frames,data.frame(umap_results,data[,c(4,5)],n_neighbors=nn))\n}\n\n\numap_plot&lt;-plot_ly(\n  umap_data_frames, x = ~X1, y = ~X2, color = ~subcluster,frame=~n_neighbors,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\numap_plot\n\n\n\n\n\n\nHere i have plotted the data and colored the sub clusters. Surprisingly, the algorithm is quite robust when it comes to little changes to n_neighbors parameters but as we use larger number of neighbors the algorithm starts capturing more of a global structure than local. This is similar behavior to t-SNE but is less sensitive to the changes.\nThere are a few other parameters to play with, namely spread and min_dist. We are going to talk a bit about them later in the math section but for now let’s give you an intiutaive explanation for them. The spread parameter sets how much “space” the UMAP algorithm has to distribute points across the lower-dimensional space. A higher value of spread allows points to be distributed more widely, meaning that the embedding will appear more “spread out.” Conversely, a lower spread value will make the points more tightly packed together across the entire embedding. min_dist on the other hand, sets how close points can be to each other. Smaller min_dist values lead to tighter local clusters in the embedding. Obviousyl, this has to be set relative to spread.\nFor an example let’s focus on one our highest n_neighbors results (n_neighbors=200).\n\n\nCode\numap_data_frames&lt;-c()\nfor(spr in c(1:10))\n{\n\n    set.seed(123)\numap_results &lt;- umap(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),n_neighbors = 200,min_dist = spr/100,spread = spr\n)\n  umap_data_frames&lt;-rbind(umap_data_frames,data.frame(umap_results,data[,c(4,5)],spread=spr))\n        \n  \n\n}\n\numap_plot&lt;-plot_ly(\n  umap_data_frames, x = ~X1, y = ~X2, color = ~subcluster,frame=~spread,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\numap_plot\n\n\n\n\n\n\nAs you can can see UMAP changes the scales of how the points are spreads out the points in the lower dimension. We can now see the effect of min_dist:\n\n\nCode\numap_data_frames&lt;-c()\nfor(mn_dst in 1:8)\n{\n\n    set.seed(123)\numap_results &lt;- umap(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),n_neighbors = 200,min_dist = mn_dst,spread = 10\n)\n  umap_data_frames&lt;-rbind(umap_data_frames,data.frame(umap_results,data[,c(4,5)],min_dist=mn_dst))\n        \n  \n\n}\n\numap_plot&lt;-plot_ly(\n  umap_data_frames, x = ~X1, y = ~X2, color = ~subcluster,frame=~min_dist,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\numap_plot\n\n\n\n\n\n\nIn this plot instead of umap “streaching out” the space, the distance between the points are now increasing. Although it might be a bit confusing, you can think of as if spread focuses on global structure whereas min_dist focuses on local relationships. In any case there is no best value for these parameters. One has to experiment with them to get an desired output.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)</span>"
    ]
  },
  {
    "objectID": "umap.html#mixed-data-types",
    "href": "umap.html#mixed-data-types",
    "title": "2  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)",
    "section": "3.2 Mixed data types",
    "text": "3.2 Mixed data types\nSometimes it is possible to encounter mixed data types, combinations of numeric, categorical, and even binary data. This implementation of UMAP offers flexibility for handling such datasets through its metric parameter. It can handle multiple types of distance metrics simultaneously, meaning we can tune the way UMAP processes different subsets of our data based on their characteristics. For instance, we might use Euclidean distance for continuous variables, Hamming distance for binary data, or categorical distance for factor variables.\nThese are basically the distances that are supported now:\n\nEuclidean: Best suited for continuous numeric data.\nCosine: Useful when you want to measure the angle between vectors (common in text or word embedding tasks).\nManhattan: Focuses on absolute differences between values, which can be useful for certain types of numeric data.\nHamming: Ideal for binary or bit-wise data.\nCategorical: Designed specifically for factor variables, ensuring that categorical data is treated appropriately.\n\nIf our dataset is a data frame or matrix with multiple data types, UMAP allows us to specify a list of metrics, each applied to specific columns. For example, we could assign Euclidean distance to a block of numeric columns and Manhattan distance to another block. In our case, we have used default parameters which is euclidean distance. Have a look at help page of umap function to see how you can incorporate different distance metric.\n\n3.2.1 Machine learning\nDespite the fact that the most common application of UMAP, at least in life sciences, is to visualize data and confirm clusters, its utility goes beyond visualization and can sometimes significantly improve machine learning workflows. The idea here is to project high-dimensional data into a lower-dimensional space and train the model on these limited space. Basically, what we want to do is to mitigate the curse of dimensionality, reduces computational costs, and eliminates noise, so we get cleaner and more compact data representations.\nTraining a model is streight forward, we could just take the umap latent scores and use them in the model as features. In our case, let’s see if we can use Random Forest to do that. We are going to predict the subclusters in our data using both the original and transformed features:\nUsing the orignal features\n\n\nCode\nset.seed(123)\numap_results &lt;- umap(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),n_neighbors = 10,\n)\n\n\nX &lt;- data[, c(\"gene1\", \"gene2\", \"gene3\")]  # Predictor variables (genes)\ny &lt;- data$subcluster  # Target variable\n\n# Set seed for reproducibility\nset.seed(123)\nrf_model &lt;- randomForest::randomForest(y ~ ., data = data.frame(X, y), family = binomial)\n\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = y ~ ., data = data.frame(X, y), family = binomial) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 26.25%\nConfusion matrix:\n   1  2  3  4 class.error\n1 72 14  6  8        0.28\n2  8 73  9 10        0.27\n3  7 11 74  8        0.26\n4  6 10  8 76        0.24\n\n\nThe performance is OK. We can now perform the same modeling but using the UMAP scores instead of the original data.\n\n\nCode\nset.seed(123)\numap_results &lt;- umap(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),n_neighbors = 10,ret_model = TRUE\n)\n\n\nX &lt;- umap_results$embedding # Get the lower dimension\ny &lt;- data$subcluster  # Target variable\n\n# Set seed for reproducibility\nset.seed(123)\nrf_model &lt;- randomForest::randomForest(y ~ ., data = data.frame(X, y), family = binomial)\n\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = y ~ ., data = data.frame(X, y), family = binomial) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 25.25%\nConfusion matrix:\n   1  2  3  4 class.error\n1 72 12  6 10        0.28\n2 10 74  9  7        0.26\n3  6  9 75 10        0.25\n4  4  9  9 78        0.22\n\n\nThe performance became slightly better and we are using one less dimension. Please note that in calling the umap function, i have used ret_model = TRUE. This is not because we want to do modeling on the scores (it does not change anything) but rather because retaining the model allows us to reuse the trained UMAP model on new, unseen data. This is an important feature of UMAP, which allows us to project new data that has not been part of the original dataset onto the same lower-dimensional space. We want to do that because when testing our machine learning model, we need to have consistency between the training and testing data representations. By projecting the test data onto the same UMAP-reduced space as the training data, we ensure that the relationships and structures are preserved, making the model’s performance evaluation more reliable. This is essential for maintaining the integrity of the testing phase, as the model can only make valid predictions if the test data is represented in the same lower-dimensional space as the data it was trained on. Without this, differences in dimensionality reduction could lead to misleading results and reduced model performance.\nLet’s generate some test data and see how UMAP does the projection.\n\n\nCode\nset.seed(123)\ngene1&lt;-rnorm(20,mean = mean(data$gene1),sd = sd(data$gene1))\ngene2&lt;-rnorm(20,mean = mean(data$gene2),sd = sd(data$gene2))\ngene3&lt;-rnorm(20,mean = mean(data$gene3),sd = sd(data$gene3))\ndata_test &lt;- data.frame(gene1=gene1,gene2=gene2,gene3=gene3\n  \n)\n\nplot_ly(\n  data_test, x = ~gene1, y = ~gene2, z = ~gene3,\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\n\n\n\n\n\n\nCode\nuwot::umap_transform(data_test,model = umap_results)\n\n\n            [,1]      [,2]\n [1,] -4.4998679 -8.202740\n [2,] -2.5597382 -7.825222\n [3,] -3.9082253 -8.502620\n [4,]  2.0265243  4.171502\n [5,]  1.8976605  3.944753\n [6,] -3.8584678 -8.442724\n [7,]  0.4156643  5.762831\n [8,] -4.6260238 -6.109046\n [9,] -2.6771843 -6.303789\n[10,]  0.6436780  6.043564\n[11,]  2.3308625  5.574963\n[12,]  1.1052934  4.172650\n[13,]  0.5121345  6.176522\n[14,]  2.6370437  6.252563\n[15,]  0.4070784  5.693460\n[16,] 14.2367573 -2.238247\n[17,] -4.5859871 -6.968265\n[18,] -3.1925490 -5.590302\n[19,]  1.1979758  4.566453\n[20,] -2.6560514 -7.135177\n\n\nWe can now use umap_transform function to project the data onto the UMAP space.\n\n\nCode\nset.seed(123)\nprojected_test &lt;- umap_transform(data_test,model = umap_results)\ncolnames(projected_test)&lt;-c(\"X1\",\"X2\")\ncombined_data&lt;-rbind(data.frame(umap_results$embedding,data_type=\"training\"),\n      data.frame(projected_test,data_type=\"testing\"))\n\numap_plot &lt;- plot_ly(\n  combined_data, x = ~X1, y = ~X2, color = ~data_type,\n  colors = c(\"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\nThis testing data is now in the latent space. We can use the transfomred features to make prediction using our model\n\npredict(rf_model,projected_test)\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \n 2  4  2  1  1  2  3  3  1  3  4  2  3  4  3  2  3  1  2  1 \nLevels: 1 2 3 4\n\n\nWe are not going to measure the performance etc because we just randomly generated these points but in real application we need to do that.\nSo to summarize, we can use the projection capability of UMAP in order to map new unseen data points onto the latent structure and use for variety of tasks including predictive modelling etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)</span>"
    ]
  },
  {
    "objectID": "umap.html#data-integration",
    "href": "umap.html#data-integration",
    "title": "2  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)",
    "section": "3.3 Data integration",
    "text": "3.3 Data integration\nUMAP’s flexibility and ability to handle multiple data types make it an excellent tool for data integration, meaning when combining datasets from different sources or modalities. In many machine learning and bioinformatics applications, datasets may come from varied experiments or measurement techniques (e.g., RNA-seq, metabolomics, clinical data), and we might want to integrate into a common analysis space.\n\n\n\n\n\n\nYou need to have the same samples across the different modalities to be able to perform intergration. It is possible to modify UMAP to do integration without having corresponding samples across the data sources but this is not the standard capability of UMAP.\n\n\n\nIn order to demonstrate data integration, we need to simulate some more data. In this case, i have simulated a dataset that has six genes. There are some correlation between these genes but the key differences is that three of these are showing some form of clustering while the other three show different clusters.\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of points per subcluster\nn &lt;- 25\n\n# Function to introduce correlation between gene1:gene3 and gene4:gene6\ngenerate_correlated_data &lt;- function(n, main_cluster_id, subcluster_offset) {\n  \n  # Generate subcluster-visible data in gene1:gene3\n  gene1 &lt;- rnorm(n, mean = subcluster_offset[1], sd = 0.25)  # Subcluster-visible in gene1:gene3\n  gene2 &lt;- rnorm(n, mean = subcluster_offset[2], sd = 0.25)\n  gene3 &lt;- rnorm(n, mean = subcluster_offset[3], sd = 0.25)\n  \n  # Introduce correlation between gene1:gene3 and gene4:gene6 to ensure full visibility in all 6 genes\n  gene4 &lt;- 0.8 * gene1 + rnorm(n, mean = main_cluster_id * 2, sd = 0.25)  # Main clusters visible in gene4:gene6\n  gene5 &lt;- 0.8 * gene2 + rnorm(n, mean = main_cluster_id * 2, sd = 0.25)\n  gene6 &lt;- 0.8 * gene3 + rnorm(n, mean = main_cluster_id * 2, sd = 0.25)\n  \n  return(data.frame(gene1, gene2, gene3, gene4, gene5, gene6))\n}\n\n# Define subcluster offsets (subclusters in gene1:gene3)\nsubcluster_offsets &lt;- list(\n  c(-0.25, -0.25, 0.25),\n  c(0.25, -0.25, 0.25),\n  c(-0.25, 0.25, -0.25),\n  c(0.25, 0.25, -0.25)\n)\n\n# Initialize an empty data frame to hold all data\ndata &lt;- data.frame()\n\n# Generate data for each main cluster and subcluster\nfor (main_cluster_id in 1:4) {  # 4 main clusters\n  for (subcluster_id in 1:4) {  # 4 subclusters per main cluster\n    # Generate data for the current subcluster\n    subcluster_data &lt;- generate_correlated_data(n, main_cluster_id, subcluster_offsets[[subcluster_id]])\n    \n    # Add the cluster and subcluster labels\n    subcluster_data$main_cluster &lt;- factor(main_cluster_id)\n    subcluster_data$subcluster &lt;- factor(subcluster_id)\n    \n    # Combine with the main data\n    data &lt;- rbind(data, subcluster_data)\n  }\n}\n\n\n\nset.seed(123)\nfull_model&lt;-umap(data[,1:6], n_neighbors = 30)\n\nset.seed(123)\nD1&lt;-umap(data[,1:3], n_neighbors = 30)\n\n\nset.seed(123)\nD2&lt;-umap(data[,4:6], n_neighbors = 30)\n\n\n\nfull_data&lt;-plot_ly(\n  as.data.frame(full_model), x = ~V1, y = ~V2, color = ~data$main_cluster,symbol = ~data$subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)\n\nD1_data&lt;-plot_ly(\n  as.data.frame(D1), x = ~V1, y = ~V2, color = ~data$main_cluster,symbol = ~data$subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)\n\nD2_data&lt;-plot_ly(\n  as.data.frame(D2), x = ~V1, y = ~V2, color = ~data$main_cluster,symbol = ~data$subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)\n\n\nsubplot(full_data%&gt;%layout(showlegend = FALSE), D1_data%&gt;%layout(showlegend = FALSE),\n        D2_data%&gt;%layout(showlegend = FALSE),nrows = 2,\n        titleX = T,titleY = T,margin = 0.03)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"Full data\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"Dataset 1\", showarrow = F, xref='paper', yref='paper'),\n   list(x = 0.13 , y = 0.3, text = \"Dataset 2\", showarrow = F, xref='paper', yref='paper'))\n)\n\n\n\n\n\n\nIn the three subplots above, you see the results of UMAP on the full data (all six variables), Dataset 1 (first three variables) and Dataset 2 (last three variables). In all the plots, the colors show one type of clusters and shapes show a different type. Considering the full data, it is clear that colors are separated and within each color we have relatively good separation of shapes. However, if we do UMAP on each dataset in isolation, none of the datasets can show us a good separation of both cluster types.\nNow the point here is that for some reasons we don’t want to merge our data sources. It could be because they come from different distributions, each dataset might have some hidden pattern, or we want to preserve the original structure of each dataset. In fact, in most cases, we really don’t want to do the merging and apply UMAP on the merged data directly. What we are actually interested in doing is to map each dataset to an intermediate space that preserves as much information as possible from each dataset individually, while having the exact same statistical properties.\nIn this case, this intermediate stage is a graph that represents similarities between the samples. similarity_graph function from uwot allows us to do such a mapping. It is important to note that similar to t-SNE, this similarity is affected by so many parameters but most importantly n_neighbors. In fact, you can think about the approach as follow, we are going to do UMAP on each of datasets in isolation, extract the similarity graph from the UMAP object and merge these similarity graphs into a single one and then only visiualize the merged one.\nSo let’s start creating the similarity graphs first:\n\nset.seed(123)\nD1_sim = similarity_graph(data[,1:3], n_neighbors = 30)\nD2_sim = similarity_graph(data[,4:6], n_neighbors = 30)\n\nWe can now easily merge these graphs using simplicial_set_intersect or simplicial_set_union function. In many applications we want to use simplicial_set_intersect as it focuses on the shared structures between the datasets but if the total structure is of interest, we could use union. After building the merged similartiy we can use optimize_graph_layout function to map these similarty onto a lower dimensional space and visualize it.\n\nset.seed(123)\n# Combine the two representations into one\ninteg_umap &lt;- simplicial_set_intersect(x = D1_sim, y = D2_sim)\nset.seed(123)\numap_scores &lt;- optimize_graph_layout(integ_umap)\n\nplot_ly(\n  as.data.frame(umap_scores), x = ~V1, y = ~V2, color = ~data$main_cluster,symbol = ~data$subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\",showlegend = FALSE\n\n  )\n\n\n\n\n\nIn the figure you see the integrated UMAP space. This acutally looks really good. But the question is can we make this a bit better by for example trying to make color clusters a bit tighter?\nFortunately, UMAP gives us weight parameter in simplicial_set_intersect function which can be used to tune relative influence each of the dataset on the final embedding. When we called the function above we assigned dataset 1 to x and dataset 2 to y. A weight of 0.5 gives equal influence. Values smaller than 0.5 put more weight on x, meaning dataset 1. Values greater than 0.5 put more weight on y, that is dataset 2.\nIn this specific case, sine i want tighter colors, i want to give slightly more weight to the dataset 2.\n\nset.seed(123)\n# Combine the two representations into one\ninteg_umap &lt;- simplicial_set_intersect(x = D1_sim, y = D2_sim,weight = 0.6)\nset.seed(123)\numap_scores &lt;- optimize_graph_layout(integ_umap)\n\nplot_ly(\n  as.data.frame(umap_scores), x = ~V1, y = ~V2, color = ~data$main_cluster,symbol = ~data$subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\",showlegend = FALSE\n\n  )\n\n\n\n\n\nWe can see the results, we clusters are now much tighter than before and still we have OK clustering of the shapes. Can you find a better value for the weight? This takes a bit of experimenting with weight. But you can also use different paramters when contructing the initial similarity graph. For example you might suspect that the clusters are more refined in dataset 1 vs dataset 2, then you can use smaller number of neighbors. Other parameters can also be changed depending on the specific dataset. This wraps up the data integration approach of UMAP.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)</span>"
    ]
  },
  {
    "objectID": "umap.html#supervised",
    "href": "umap.html#supervised",
    "title": "2  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)",
    "section": "3.4 Supervised",
    "text": "3.4 Supervised\nAlthough UMAP is an unsupervised method, it can be adapted in a way that mimics supervised learning. This doesn’t mean UMAP maps data from an input space to a specific output space. Instead, the constructed lower-dimensional space is adjusted to reflect class separability, tuning the embedding so that data points from the same class are closer together, while points from different classes are more distinct. In this case, we need some target data. This is often the group of interest or some numerical value but it can also be a matrix with a lot of different targets of interest. In anycase, we can use the y argument from umap function to pass the target of interest. For example here we use subclusters as the target. Please be aware that we need to convert categorical variables to factors.\n\nset.seed(123)\nsupervised_score&lt;-umap(data[,1:6],n_neighbors = 30,y =data$subcluster,spread = 7,min_dist = 3)\n\nplot_ly(\n  as.data.frame(supervised_score), x = ~V1, y = ~V2, color = ~data$main_cluster,symbol = ~data$subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\",showlegend = FALSE\n\n  )\n\n\n\n\n\nHere i had to spread the points a bit so we can see the results a bit better. Compared to the results we saw before, this looks a bit better at least some of the sub clusters (shapes) they moved closer to each other which basically reflects their relationship that we wanted to capture using y. Given some test data we could perform the projection of the test datapoints onto this space and use some form of similarity to perform prediction but we are going to leave this to you to do :)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)</span>"
    ]
  },
  {
    "objectID": "umap.html#should-i-trust-umap",
    "href": "umap.html#should-i-trust-umap",
    "title": "2  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)",
    "section": "3.5 Should i trust UMAP?",
    "text": "3.5 Should i trust UMAP?\nWhile UMAP is a powerful tool that preserves both local and global structures in your data, it’s important to approach the results with caution. UMAP does not guarantee perfect global preservation, and it may sometimes produce artifacts, such as random noise clusters, that do not necessarily reflect meaningful patterns in the data. These clusters can arise due to random noise or overfitting during the optimization process, especially when tuning the parameters too aggressively.\nUMAP is primarily designed to preserve local relationships, meaning that points close to each other in the high-dimensional space should remain close in the low-dimensional embedding. However, global structure may be distorted, and distant relationships may not always be accurately represented. This is particularly true when using smaller values for the n_neighbors parameter, which can make UMAP focus too much on local clusters, potentially missing broader trends.\nIt’s also important to avoid tuning UMAP’s parameters to match a desired outcome or preconceived conclusion. Over-tuning the algorithm, especially by manipulating n_neighbors or min_dist to achieve specific patterns, can lead to misleading visualizations that don’t reflect the true structure of the data. Instead, parameter tuning should be guided by the nature of the data and the goal of the analysis, not by an attempt to force specific patterns or clusters.\nLastly, similary to t-SNE avoid doing distance based clustering methods on UMAP. Density based clustering might work but there is no guarantee for that!\n\n\nCode\nlibrary(Rtsne)\nlibrary(uwot)\n\ncat_dt&lt;-read.table(\"https://raw.githubusercontent.com/PayamEmami/pca_basics/refs/heads/master/data/cat.tsv\")\n\nset.seed(123)\ntsne_results_cat &lt;- Rtsne(\n  as.matrix(cat_dt),perplexity = 30,\n)\n\nset.seed(123)\numap_results_cat &lt;- umap(\n  as.matrix(cat_dt),n_neighbors = 30,\n)\n\npca&lt;-prcomp(cat_dt)$x[,1:2]\n\npca_plot &lt;- plot_ly(\n  as.data.frame(pca), x = ~PC1, y = ~PC2,\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\ntsne_plot &lt;- plot_ly(\n  as.data.frame(tsne_results_cat$Y), x = ~V1, y = ~V2, \n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\numap_plot &lt;- plot_ly(\n  as.data.frame(umap_results_cat), x = ~V1, y = ~V2, \n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\ncolnames(cat_dt)&lt;-c(\"V1\",\"V2\",\"V3\")\noriginal_data &lt;- plot_ly(\n  as.data.frame(cat_dt), x = ~V1, y = ~V2,z=~V3,\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original data\"\n\n  )\n\noriginal_data\n\n\n\n\n\n\nCode\nsubplot(pca_plot%&gt;%layout(showlegend = FALSE),\n        tsne_plot%&gt;%layout(showlegend = FALSE),\n        umap_plot%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.05)%&gt;% layout(annotations = list(\n           list(x = 0.13 , y = 1.035, text = \"PCA\", showarrow = F, xref='paper', yref='paper'),\n list(x = 0.53 , y = 1.035, text = \"t-SNE\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"UMAP\", showarrow = F, xref='paper', yref='paper'))\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)</span>"
    ]
  },
  {
    "objectID": "umap.html#math",
    "href": "umap.html#math",
    "title": "2  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)",
    "section": "3.6 Math",
    "text": "3.6 Math\nSimilar to t-SNE, UMAP can also be split into three stages: building a notion of distances both in higher and lower dimension and optimizing the low-dimensional embedding to preserve both local and global structures.\nThe biggest difference in my opinion is that UMAP is based on much stronger algebraic and topological foundations, using concepts from category theory and simplicial sets to create a more flexible representation of local and global relationships, whereas t-SNE relies primarily on probability distributions to measure similarity between points.\nWe have actually introduce much of the math in the t-SNE section. So here i’m going to give an overview of what is going on in UMAP.\n\n3.6.1 Manifold of the data\nOne thing to note is that we believe our data is sitting on a lower-dimensional manifold embedded within a higher-dimensional space. A manifold is a mathematical structure that, at every small enough region, looks like a flat space (Euclidean space). In simpler terms, even though the data may exist in a high-dimensional space, it behaves like a simpler, lower-dimensional shape when viewed locally. For example, the surface of a sphere (like the Earth) is a 2D manifold, because if you zoom in enough on any part of it, it looks flat like a piece of paper, even though globally it’s curved. In the context of our data, this means that while the data might appear high-dimensional and complex, its structure can be understood through a simpler geometric object. This manifold provides a more compact and interpretable representation, capturing both the local details (where the data behaves like flat space) and the global shape (which can have curves and twists). So to sum it up, the manifold is an overall shape of our data. We don’t know this manifold but we are trying to figure out how it looks. For example look at the following curve which visualize a circadian-like trend:\n\n\nCode\n# Load necessary libraries\nlibrary(plotly)\n\n# Function to generate points on a circadian-like curve\ngenerate_points_circadian &lt;- function(n_dense, n_sparse) {\n  # Dense region (more points)\n  t_dense &lt;- seq(0, 1 * pi, length.out = n_dense)\n  \n  # Sparse region (fewer points, covering a similar interval)\n  t_sparse &lt;- seq(1 * pi, 2 * pi, length.out = n_sparse)\n  \n  # Generate the circadian curve (oscillating sine-cosine)\n  x_dense &lt;- t_dense\n  y_dense &lt;- sin(t_dense) * cos(t_dense / 2)  \n  \n  x_sparse &lt;- t_sparse\n  y_sparse &lt;- sin(t_sparse) * cos(t_sparse / 2)\n  \n  # Combine the dense and sparse points\n  x &lt;- c(x_dense, x_sparse)\n  y &lt;- c(y_dense, y_sparse)\n  \n    # Normalize x and y values to the same scale\n  x &lt;- (x - min(x)) / (max(x) - min(x))  # Normalize x to [0, 1]\n  y &lt;- (y - min(y)) / (max(y) - min(y))  # Normalize y to [0, 1]\n  \n  # Create a data frame with the results\n  data &lt;- data.frame(x = x, y = y, density = c(rep(\"Dense\", n_dense), rep(\"Sparse\", n_sparse)))\n  return(data)\n}\n\n# Function to generate the entire manifold (for plotting the line)\ngenerate_manifold_curve &lt;- function(n_points) {\n  t &lt;- seq(0, 2 * pi, length.out = n_points)\n  x &lt;- t\n  y &lt;- sin(t) * cos(t / 2)\n  \n  # Normalize x and y values to the same scale\n  x &lt;- (x - min(x)) / (max(x) - min(x))  # Normalize x to [0, 1]\n  y &lt;- (y - min(y)) / (max(y) - min(y))  # Normalize y to [0, 1]\n  \n  data &lt;- data.frame(x = x, y = y)\n  return(data)\n}\n\n# Set number of points in dense and sparse regions\nn_dense &lt;- 100\nn_sparse &lt;- 100\n\n# Generate the points\ndata &lt;- generate_points_circadian(n_dense, n_sparse)\n\n# Generate the manifold curve\nmanifold &lt;- generate_manifold_curve(500)  # 500 points for a smooth curve\n\n# Plot using plotly\np &lt;- plot_ly() %&gt;%\n  # Add the manifold line\n  add_trace(data = manifold, x = ~x, y = ~y, type = 'scatter', mode = 'lines',\n            line = list(color = 'black', width = 2), name = 'Manifold') %&gt;%\n  \n  # Add the points with varying density\n  add_trace(data = data, x = ~x, y = ~y, type = 'scatter', mode = 'markers', \n            color = ~density, colors = c('blue', 'red'),\n            marker = list(size = 10, opacity = 0.7), name = 'Points') %&gt;%\n  \n  # Layout\n  layout(title = \"\",\n         xaxis = list(title = \"X\"),\n         yaxis = list(title = \"Y\"),showlegend = FALSE)\n\n# Show the plot\np\n\n\n\n\n\n\nThe idea here is that we are interested in characterizing the underlying structure of the data, which in this case follows a periodic, wavelike pattern. In this context, we can interpret the curve as lying on a manifold. A manifold is a special type of topological space. A topological space is an abstract way of talking about the “shape” of a set of points. If we have a set \\(X\\), which is just a collection of points (for example, the real numbers, or points on a piece of paper), in order to study this space, we need a way to talk about which points are “close” to each other. Instead of using specific distances, we use something called open sets. An open set is a special kind of subset of \\(X\\). Think of it like a region around a point where everything inside the region is considered “close” to that point, but the edges of the region are not included. For a set of points to be considered “open” in a topological space, it has to follow these three rules: 1. The whole space \\(X\\) and the empty set \\(\\emptyset\\) must be open. This just means that there’s a way to think of everything and nothing as being “open” in a sense. 2. If you take any number of open sets and combine them, the result should still be open. This allows us to talk about combining neighborhoods around points. 3. If you take a finite number of open sets and look at their overlap (where they share points), this overlap must also be an open set.\nAn example of an open set in the context of topology and metric spaces is called open ball. In a metric space, an open ball is defined as the set of all points that are within a certain distance (radius) from a given point (the center of the ball). Formally, if we have a metric space \\((X, d)\\), where \\(d\\) is the distance function (or metric), the open ball centered at a point \\((x \\in X)\\) with radius \\(r &gt; 0\\) is defined as:\n\\[\nB(x, r) = \\{ y \\in X \\mid d(x, y) &lt; r \\}\n\\]\nThis set contains all the points \\(y\\) whose distance from \\(x\\) is strictly less than \\(r\\). This basically satisfies the definition of an open set: For every point \\(p\\) inside the open ball \\(B(x, r)\\), we can find a smaller open ball around \\(p\\) that is still entirely contained within \\(B(x, r)\\).\nHere is an example of open ball for our data:\n\n\nCode\nchosen_point &lt;- data[50, ] \nball_radius &lt;- 0.05  # Set the radius of the open ball\n\n# Create the open ball as a circle around the chosen point\ntheta &lt;- seq(0, 2 * pi, length.out = 100)\ncircle_x &lt;- chosen_point$x + ball_radius * cos(theta)\ncircle_y &lt;- chosen_point$y + ball_radius * sin(theta)\n\n# Plot using plotly\np &lt;- plot_ly() %&gt;%\n  # Add the manifold line\n  add_trace(data = manifold, x = ~x, y = ~y, type = 'scatter', mode = 'lines',\n            line = list(color = 'black', width = 2), name = 'Manifold') %&gt;%\n  \n  # Add the points with varying density\n  add_trace(data = data, x = ~x, y = ~y, type = 'scatter', mode = 'markers', \n            color = ~density, colors = c('blue', 'red'),\n            marker = list(size = 10, opacity = 0.7), name = 'Points') %&gt;%\n  \n  # Add the open ball (circle) around the chosen point\n  add_trace(x = circle_x, y = circle_y, type = 'scatter', mode = 'lines',\n            line = list(color = 'green', dash = 'dash', width = 2), \n            name = 'Open Ball') %&gt;%\n  \n  # Highlight the chosen point\n  add_trace(x = ~chosen_point$x, y = ~chosen_point$y, type = 'scatter', mode = 'markers',\n            marker = list(size = 15, color = 'green', symbol = 'circle'), name = 'Chosen Point') %&gt;%\n  \n  # Layout with equal axis scaling\n  layout(title = \"\",\n         xaxis = list(title = \"X\", scaleanchor = \"y\"),  # Link x and y axes for equal scaling\n         yaxis = list(title = \"Y\", scaleratio = 1),showlegend = FALSE)  # Enforce equal aspect ratio for x and y axes\n\n# Show the plot\np\n\n\n\n\n\n\nOnce we have open sets or in this case open balls, we can use them to study a topological space. One way to do this is by breaking the space into smaller, manageable pieces using open covers. An open cover is a collection of open sets that completely covers the entire space. The idea behind open covers is that we can study complex spaces by looking at how these smaller open sets relate to each other. Formally an open cover of a topological space \\(X\\) is a collection of open sets \\(\\{ U_i \\}_{i \\in I}\\) such that: \\[\n  X = \\bigcup_{i \\in I} U_i\n\\] Every point in \\(X\\) is contained in at least one open set \\(U_i\\).\nWe can see an example of cover here:\n\n\nCode\nchosen_indices &lt;- seq(1,200,by=2)  # Choose several points for the open covers\nball_radius &lt;- 0.05  # Set the radius of the open covers\n\n# Initialize the plot\np &lt;- plot_ly() %&gt;%\n  # Add the manifold line\n  add_trace(data = manifold, x = ~x, y = ~y, type = 'scatter', mode = 'lines',\n            line = list(color = 'black', width = 2), name = 'Manifold') %&gt;%\n  \n  # Add the points with varying density\n  add_trace(data = data, x = ~x, y = ~y, type = 'scatter', mode = 'markers', \n            color = ~density, colors = c('blue', 'red'),\n            marker = list(size = 10, opacity = 0.7), name = 'Points')\n\n# Add the open covers (circles) around the chosen points\nfor (index in chosen_indices) {\n  chosen_point &lt;- data[index, ]\n  theta &lt;- seq(0, 2 * pi, length.out = 100)\n  circle_x &lt;- chosen_point$x + ball_radius * cos(theta)\n  circle_y &lt;- chosen_point$y + ball_radius * sin(theta)\n  \n  # Add each open cover to the plot\n  p &lt;- p %&gt;%\n    add_trace(x = circle_x, y = circle_y, type = 'scatter', mode = 'lines',\n              line = list(color = 'green', dash = 'dash', width = 2), \n              name = 'Open Cover')\n}\n\n# Highlight the chosen points for the open covers\np &lt;- p %&gt;%\n  add_trace(x = ~data[chosen_indices, ]$x, y = ~data[chosen_indices, ]$y, type = 'scatter', mode = 'markers',\n            marker = list(size = 15, color = 'green', symbol = 'circle'), name = 'Cover Centers')\n\n# Layout with equal axis scaling\np &lt;- p %&gt;%\n  layout(title = \"\",\n         xaxis = list(title = \"X\", scaleanchor = \"y\"),  # Link x and y axes for equal scaling\n         yaxis = list(title = \"Y\", scaleratio = 1),showlegend = FALSE)  # Enforce equal aspect ratio for x and y axes\n\n# Show the plot\np\n\n\n\n\n\n\nWhen we look at an open cover, one important thing to study is how the open sets overlap. The nerve of an open cover is a mathematical structure that tells us about these overlaps. Nerve Theorem says that If \\(\\mathcal{U}\\) is an open cover of a good topological space (like a paracompact space), and all finite intersections of open sets in \\(\\mathcal{U}\\) are contractible or empty, then the nerve \\(\\mathcal{N(U)}\\) has the same homotopy type (two spaces have the same homotopy type if they can be continuously deformed into each other. They share the same “shape” in a topological sense) as \\(X\\). This simply means that if the open sets in the cover overlap in a “nice” way, then the nerve of the cover has the same essential shape as the original topological space.\nSo how do we make nerves?! The nerve \\(\\mathcal{N(U)}\\) of an open cover \\(\\mathcal{U} = \\{ U_i \\}\\) is an abstract simplicial complex constructed as follows:\n\nVertices: Each open set \\(U_i\\) corresponds to a vertex \\(v_i\\).\nSimplices: A finite subset \\(\\{ v_{i_0}, v_{i_1}, \\dots, v_{i_k} \\}\\) spans a \\(k\\)-simplex if the intersection of the corresponding open sets is non-empty:\n\n\\[\n    U_{i_0} \\cap U_{i_1} \\cap \\dots \\cap U_{i_k} \\neq \\emptyset\n\\]\nThis nerve records the way open sets overlap, translating continuous topological information into a discrete combinatorial object.\n\n\n\n\n\n\nThe fact that a nerve is simplicial complex brings us to the main concept behind UMAP and that is simplicial complexes. A simplicial complex \\(K\\) is a collection of simplices (points, line segments, triangles, and their higher-dimensional counterparts).\nWe can think about the simplices as strcutures that connect the data points together:\n\n0-simplex: A point.\n1-simplex: A line segment between two points.\n2-simplex: A filled triangle formed by three points.\n3-simplex: A filled tetrahedron formed by four points.\nk-simplex: The convex hull of \\(k+1\\) affinely independent points.\n\nThe geometric realization of a simplicial complex is a topological space formed by “gluing together” the simplices in a way that matches their combinatorial connections. We are going to get back to this later\n\n\n\nHere we are not going to create a nerve in classical sense but rather we want to follow its steps conceptually. We want to create a cover through open sets and then fix the interaction (overlap) between these sets. Let’s go step by step:\n\n\n3.6.2 Defining the Cover (Open Balls/Open sets)\nThe first and probably the most important thing here is to create open sets. If you remember, we need to create these sets whose cover is going to involve the entire space. In the best case scenario we want to have infinite data points to be uniformly distributed over the manifold. In this case, we just create open balls with some radios around each point and anything within would be included in a set. This is visualized in the previous plot. In reality however, we are dealing with finite number of data points which are non-uniformly distributed on the manifold:\n\n\nCode\n# Load necessary libraries\nlibrary(plotly)\n\n# Function to generate points on a circadian-like curve\ngenerate_points_circadian &lt;- function(n_dense, n_sparse) {\n  # Dense region (more points)\n  t_dense &lt;- seq(0, 1 * pi, length.out = n_dense)\n  \n  # Sparse region (fewer points, covering a similar interval)\n  t_sparse &lt;- seq(1 * pi, 2 * pi, length.out = n_sparse)\n  \n  # Generate the circadian curve (oscillating sine-cosine)\n  x_dense &lt;- t_dense\n  y_dense &lt;- sin(t_dense) * cos(t_dense / 2)  \n  \n  x_sparse &lt;- t_sparse\n  y_sparse &lt;- sin(t_sparse) * cos(t_sparse / 2)\n  \n  # Combine the dense and sparse points\n  x &lt;- c(x_dense, x_sparse)\n  y &lt;- c(y_dense, y_sparse)\n  \n    # Normalize x and y values to the same scale\n  x &lt;- (x - min(x)) / (max(x) - min(x))  # Normalize x to [0, 1]\n  y &lt;- (y - min(y)) / (max(y) - min(y))  # Normalize y to [0, 1]\n  \n  # Create a data frame with the results\n  data &lt;- data.frame(x = x, y = y, density = c(rep(\"Dense\", n_dense), rep(\"Sparse\", n_sparse)))\n  return(data)\n}\n\n# Function to generate the entire manifold (for plotting the line)\ngenerate_manifold_curve &lt;- function(n_points) {\n  t &lt;- seq(0, 2 * pi, length.out = n_points)\n  x &lt;- t\n  y &lt;- sin(t) * cos(t / 2)\n  \n  # Normalize x and y values to the same scale\n  x &lt;- (x - min(x)) / (max(x) - min(x))  # Normalize x to [0, 1]\n  y &lt;- (y - min(y)) / (max(y) - min(y))  # Normalize y to [0, 1]\n  \n  data &lt;- data.frame(x = x, y = y)\n  return(data)\n}\n\n# Set number of points in dense and sparse regions\nn_dense &lt;- 100\nn_sparse &lt;- 20\n\n# Generate the points\ndata &lt;- generate_points_circadian(n_dense, n_sparse)\n\n# Generate the manifold curve\nmanifold &lt;- generate_manifold_curve(500)  # 500 points for a smooth curve\n\n# Plot using plotly\np &lt;- plot_ly() %&gt;%\n  # Add the manifold line\n  add_trace(data = manifold, x = ~x, y = ~y, type = 'scatter', mode = 'lines',\n            line = list(color = 'black', width = 2), name = 'Manifold') %&gt;%\n  \n  # Add the points with varying density\n  add_trace(data = data, x = ~x, y = ~y, type = 'scatter', mode = 'markers', \n            color = ~density, colors = c('blue', 'red'),\n            marker = list(size = 10, opacity = 0.7), name = 'Points') %&gt;%\n  \n  # Layout\n  layout(title = \"\",\n         xaxis = list(title = \"X\"),\n         yaxis = list(title = \"Y\"),showlegend = FALSE)\n\n# Show the plot\np\n\n\n\n\n\n\nHere the points are often not distributed evenly across the input space. Instead, the data has regions of high density (where many points are close together) and regions of low density (where points are sparse and far apart). If we use a fixed-radius open ball for neighborhood construction (e.g., \\(r\\)-balls in a metric space), it doesn’t adapt well to non-uniform data. In dense regions, the fixed radius might include too many points, creating very large and noisy neighborhoods. In sparse regions, the fixed radius might include too few or even no points, making it hard to capture the structure of the data. So we basically need to change our defintion of open set slightly. This means that instead of having\n\\[\n  B(x_0, y_0, r) =\n  \\begin{cases}\n  \\text{include in the set} & \\text{if} \\ (x - x_0)^2 + (y - y_0)^2 &lt; r^2 \\\\\n  \\text{exclude} & \\text{otherwise}\n  \\end{cases}\n\\]\nWe want to allow for a gradual decay of membership in the set depending how far is it from the center, meaning that points far from the center of the ball have some small degree of membership in the set. This we are going to denote as a fuzzy open set. This basically means that instead of data points being exactly present or absent in a set, we will have a weight or probability showing how likely it is for a point to belong to that set.\nNow, since we are working on a manifold, a space that can be locally approximated by Euclidean space but may have a more complex global structure, we need to adjust our notion of distance to account for the manifold’s curvature and geometry (Riemannian geometry).\nOn a Riemannian manifold \\(M\\), each point \\(x \\in M\\) is associated with a local metric (a way of measuring distances) that depends on the point’s position. The distance between two points \\(x\\) and \\(x_0\\) is not simply the straight-line (Euclidean) distance as in flat space but is instead the geodesic distance, denoted by \\(d_M(x, x_0)\\). This geodesic distance measures the shortest path on the manifold between \\(x\\) and \\(x_0\\), taking into account any curvature or distortions in the space.\nThis is an example of how geodesic distance is different from Euclidean:\n\n\nCode\nlibrary(plotly)\n\n# Function to generate points for the manifold\ngenerate_manifold_surface &lt;- function(n_points) {\n  t &lt;- seq(0, pi, length.out = n_points)\n  x &lt;- t\n  y &lt;- sin(t)\n  z &lt;- 0.3 * cos(2 * t)  # Curved shape for manifold\n  \n  data &lt;- data.frame(x = x, y = y, z = z)\n  return(data)\n}\n\n# Function to generate the geodesic curve (curved path)\ngenerate_geodesic_curve &lt;- function() {\n  t &lt;- seq(0, pi, length.out = 100)\n  x &lt;- t\n  y &lt;- sin(t)\n  z &lt;- 0.3 * cos(2 * t)  # Geodesic follows the manifold surface\n  \n  data &lt;- data.frame(x = x, y = y, z = z)\n  return(data)\n}\n\n# Function to generate the Euclidean line\ngenerate_euclidean_line &lt;- function() {\n  x &lt;- c(0, pi)\n  y &lt;- c(0, sin(pi))  # Points A (0,0) to B (pi, sin(pi))\n  z &lt;- c(0.3 * cos(2 * 0), 0.3 * cos(2 * pi))  # Straight line between A and B\n  \n  data &lt;- data.frame(x = x, y = y, z = z)\n  return(data)\n}\n\n# Function to generate start and end points (A and B)\ngenerate_points_A_B &lt;- function() {\n  x &lt;- c(0, pi)  # Start (A) and end (B) points\n  y &lt;- c(0, sin(pi))\n  z &lt;- c(0.3 * cos(2 * 0), 0.3 * cos(2 * pi))\n  \n  data &lt;- data.frame(x = x, y = y, z = z, label = c(\"A\", \"B\"))\n  return(data)\n}\n\n# Generate the manifold surface, geodesic curve, Euclidean line, and points A and B\nmanifold &lt;- generate_manifold_surface(100)\ngeodesic &lt;- generate_geodesic_curve()\neuclidean &lt;- generate_euclidean_line()\npoints_AB &lt;- generate_points_A_B()\n\n# Plot using plotly\np &lt;- plot_ly() %&gt;%\n  # Add the manifold surface\n  add_trace(data = manifold, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'lines',\n            line = list(color = 'lightgrey', width = 100), name = 'Manifold',opacity=0.5) %&gt;%\n  \n  # Add the geodesic curve\n  add_trace(data = geodesic, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'lines',\n            line = list(color = 'green', width = 5), name = 'Geodesic Distance') %&gt;%\n  \n  # Add the Euclidean straight line\n  add_trace(data = euclidean, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'lines',\n            line = list(color = 'blue', dash = 'dash', width = 4), name = 'Euclidean Distance') %&gt;%\n  \n  # Add points A and B\n  add_trace(data = points_AB, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'markers+text',\n            marker = list(color = 'red', size = 8), text = ~label, textposition = 'top middle',\n            name = 'Points A and B') %&gt;%\n  \n  # Layout\n  layout(scene = list(xaxis = list(title = \"X\"),\n                      yaxis = list(title = \"Y\"),\n                      zaxis = list(title = \"Z\")),\n         title = \"Geodesic vs Euclidean Distance with Points A and B\",\n         showlegend = TRUE)\n\n# Show the plot\np\n\n\n\n\n\n\nSo when we want to define a fuzzy open set on a manifold, the distance function \\(d_M(x, x_0)\\) will be used in the membership function to reflect how distances are measured in the curved space of the manifold. Mathematically, we define a fuzzy open set \\(A\\) on the manifold \\(M\\) by specifying a membership function \\(\\mu_A: M \\to [0,1]\\), where the degree of membership of a point \\(x \\in M\\) is determined by the geodesic distance \\(d_M(x, x_0)\\) from a central point \\(x_0 \\in M\\). A common choice for the membership function is an exponential decay based on the geodesic distance:\n\\[\n      \\mu_A(x)= \\exp\\left( -\\alpha d_M(x, x_0) \\right)\n\\]\nwhere: - \\(d_M(x, x_0)\\) is the geodesic distance between \\(x\\) and the center \\(x_0\\) of the fuzzy set. - \\(\\alpha &gt; 0\\) is a constant controlling the rate of decay of membership.\nif we take \\(\\alpha\\) as a fraction then this formula looks quite similar to that of t-SNE:\n\\[\n      \\mu_A(x)= \\exp\\left( -\\frac{d_M(x, x_0)}{\\alpha}  \\right)\n\\]\nSo similar to t-SNE this formula describes how the membership of a point decays smoothly as its geodesic distance from the center \\(x_0\\) increases. The key difference from the Euclidean case is that the distance \\(d_M(x, x_0)\\) now respects the curvature of the manifold.\nOne of the cool features of Riemannian geometry is that the manifold behaves locally like Euclidean space (zoom in on the manifold to see). In a small neighborhood around \\(x_0\\), the geodesic distance \\(d_M(x, x_0)\\) closely approximates the Euclidean distance. This allows us to use the same intuition as in Euclidean space for points near \\(x_0\\), where membership decays smoothly. However, as we move farther from \\(x_0\\), the global structure of the manifold can cause the geodesic distance to behave quite differently from the Euclidean distance. This basically means that on a curved manifold like the plot above, the geodesic distance between two points wraps around the curve, and the decay in membership reflects this curvature. On more complex manifolds, the distance might vary in ways that reflect the manifold’s underlying structure, and the fuzzy membership decays accordingly.\nBut assuming that we want to focus on local structure (neighborhood), we can change this formula to reseble that of t-SNE just for the sake of making more clear:\n\\[\n\\mu_A(x) = \\exp\\left(-\\frac{(x - x_0)^2 + (y - y_0)^2}{\\sigma}\\right)\n\\]\nHere i just replaced \\(\\alpha\\) with \\(\\sigma\\) and plugged in the equation for Euclidean distance. This gives us a formula similar to the Gaussian kernel, which essentially provides the probability that a point \\(x\\) belongs to the fuzzy set \\(A\\). In other words, the function assigns higher probabilities to points closer to \\(x_0\\), and lower probabilities to those farther away, with the parameter \\(\\sigma\\) controlling the spread of these probabilities.\nWe can write this in a more general way let’s say that for point \\(x_i\\) we want create an open ball, to calculate the probability of \\(x_j\\) being part of the open ball \\(i\\) we can write\n\\[\nw(x_j,x_{ij}) = \\exp\\left(-\\frac{d(x_j,x_{ij})}{\\sigma}\\right)\n\\] where \\(d(x_j,x_{ij})\\) is some function measuring distance between \\(x_i\\) and \\(x_j\\).\nSo far so good, we have a measure that works on local neighborhoods which more or less follows Riemannian metric in local parts of the manifold. Now that we are working on a manifold, we have to make sure that we satifiy its topological properties. We already talked about that a manifold must resemble Euclidean space \\(\\mathbb{R}^n\\) in small neighborhoods around each point. We however, have to make sure that within any small neighborhood of a point on the manifold, we can draw continuous paths between any two points within that neighborhood. That means that if we zoom in on a super small local neighborhood (like a single point), there is always a neighboring point close enough that we can put them part of a set. This in turn means that we for \\(x_i\\) we believe is that there is always a point close enough with any distance \\(\\rho_i\\) such that there is 100% probability that they are part of the same set. Therefore just to make sure that we satisfy this property, we are going to adjust the probablity formula above so that the closet neighbro will always get a probablity of 1.\n\\[\nw(x_j,x_{ij}) = \\exp\\left(-\\frac{max(0,d(x_j,x_{ij})-\\rho_i)}{\\sigma_i}\\right)\n\\]\nIn this formula, as before \\(d(x_j, x_{ij})\\) represents the distance between point \\(x_j\\) and its neighbor \\(x_{ij}\\). The term \\(\\rho_i\\) is the distance to the nearest neighbor of \\(x_j\\), ensuring that the closest neighbor always has a maximum probability of 1. By subtracting \\(\\rho_i\\) from the distance, we ensure that any distance less than or equal to \\(\\rho_i\\) results in a probability of 1, as \\(\\max(0, d(x_j, x_{ij}) - \\rho_i)\\) will be zero for the closest neighbors.\nAs said before \\(\\sigma_i\\) parameter controls the spread or sharpness of the decay of the probability for neighbors that are further away. As the distance between \\(x_j\\) and \\(x_{ij}\\) increases, the probability decreases exponentially, following the Riemannian structure of the local neighborhood. This adjustment guarantees that the local-connectivity property is satisfied, ensuring that the nearest neighbors remain tightly connected and thus preserving the manifold’s local geometry while allowing the global structure to be represented appropriately. Therefore, specially in very high dimensional data where points tend to become isolated, we measure that that they are at least connect to one neighbor.\nWith this adjustment comes a really cool effect: normalizing distances in sparse regions of the manifold where data points are very far from each other. In that case, everything will be measured relative to the nearest neighbor (which can local very far also), ensuring that even when points are sparsely distributed, the structure remains consistent. This normalization prevents distances from blowing up in sparse areas, which would otherwise distort the local geometry.\nThe above equation is exactly what UMAP uses to measure the distances between the data points. We are however not done here is another property we need to take care of. That is to set \\(\\sigma_i\\).\nIf we pause for a second and think about what we have done now, we wanted to create open sets meaning that we need to focus on small little region around each point. We can select \\(k\\) closest points to the current one and then calculate \\(\\sigma\\) and assign the probability based on these \\(k\\) distances and then create our open set based on these distances. If you remember, we said this topological approach would work if data is uniformly distributed on the manifold. This basically means that we expect that for each data point, neighbors affect it similarly on average.\nMathematically, we are dealing with a finite set of distances, we can simply sum the probabilities (weights) for each set and compare the sums directly.\nFor set \\(i\\), the total probability (sum of weights) can be written as:\n\\[\nS_i = \\sum_{j=1}^{n_i} \\exp\\left(-\\frac{\\max(0, d(x_j, x_{ij}) - \\rho_i)}{\\sigma_i}\\right)\n\\]\nFor set \\(l\\), the total probability (sum of weights) would be:\n\\[\nS_l = \\sum_{j=1}^{n_l} \\exp\\left(-\\frac{\\max(0, d(x_l, x_{kl}) - \\rho_l)}{\\sigma_l}\\right)\n\\]\nHere, \\(n_i\\) and \\(n_k\\) are the number of neighbors considered in sets \\(i\\) and \\(l\\), respectively.\nIf the data was uniformly distributed we would expect\n\\[\nS_i=S_l=C\n\\]\nThat is the expectation of weights in each set would be equal to the same constant. This constant is global and does not change no matter where on the manifold we build our sets.\nIf all the neighbors contribute equally to \\(S_i\\), the most straightforward interpretation would imply that the distances \\(d(x_j, x_{ij})\\) are relatively similar for all \\(j\\), meaning that \\(d(x_j, x_{ij}) \\approx \\rho_i\\). As a result: \\[\n   S_i = \\sum_{j=1}^{n_i} 1 = n_i.\n\\] Each term contributes equally and fully, leading to the sum being exactly equal to \\(n_i\\), the number of neighbors.\nSo does this mean that we should put the value equal to the number of neighbors \\(k\\)? It turned out that this is a bad idea. If we think about it for a second, if we use \\(k\\), because in dense regions there are a lot of neighboring points, we tend to quickly pick a few and get to the \\(k\\), meaning that the \\(\\sigma\\) will be relatively small. In sparse regions, however, we need to expand the neighborhood much further to reach \\(k\\) neighbors, resulting in a much larger \\(\\sigma\\). This creates an imbalance: in dense regions, small \\(\\sigma\\) values cause the model to overemphasize local structure, while in sparse regions, large \\(\\sigma\\) values cause distant points to exert too much influence, leading to distortions.\nTo address this, we use \\(\\log_2(k)\\) instead. The logarithmic scaling grows much more slowly than linear scaling, which moderates the influence of neighbors. For example, if \\(k = 4\\), \\(\\log_2(4) = 2\\), but if \\(k = 16\\), \\(\\log_2(16) = 4\\). Notice how \\(\\log_2(k)\\) grows much slower as \\(k\\) increases. This ensures that in dense regions, the sum of influences doesn’t explode, and in sparse regions, neighbors still contribute meaningfully without overwhelming the embedding. This smoother scaling prevents dense regions from becoming too tightly packed and allows sparse regions to remain connected, maintaining a balanced and meaningful representation of both local and global structure in the embedding.\nSo to summarize this section, we are going to create fuzzy open sets, with one set for each data point. The membership strength is going to be measured by\n\\[\nw(x_j, x_{ij}) = \\exp\\left(-\\frac{\\max(0, d(x_j, x_{ij}) - \\rho_i)}{\\sigma_i}\\right)\n\\]\nwhere \\(d(x_j, x_{ij})\\) is the distance between the data point \\(x_j\\) and its \\(i\\)-th neighbor up to \\(k\\) neighbors, and \\(\\rho_i\\) represents the distance to the nearest neighbor. The parameter \\(\\sigma_i\\) controls the spread of the membership function. We will adjust \\(\\sigma_i\\) such that the sum of the weights across the neighborhood equals \\(\\log_2(k)\\), where \\(k\\) is the number of neighbors, ensuring that each point’s neighborhood contributes smoothly and consistently to the embedding.\nMore formally, we set \\(\\sigma_i\\) such that:\n\\[\n\\sum_{j=1}^{n} \\exp\\left(-\\frac{\\text{knn-dists}_i - \\rho_i}{\\sigma_i}\\right) = \\log_2(k)\n\\]\nThis wraps up our section on creating fuzzy open sets. Let’s try to implement this:\n\n# Load required packages\nlibrary(FNN)  # For exact k-nearest neighbors\nlibrary(matrixStats)  # For row operations\n\n# Step 1: Find k-nearest neighbors (KNN) and distances using exact method\nget_knn_exact &lt;- function(X, k) {\n  knn_result &lt;- get.knn(X, k = k, algorithm = \"brute\")\n  list(knn = knn_result$nn.index, dists = knn_result$nn.dist)\n}\n\n# Step 2: Perform binary search to find sigma such that sum(exp(...)) = log2(k)\nfind_sigma &lt;- function(dists, rho, k, tolerance = 1e-5, max_iter = 1000) {\n  log2_k &lt;- log2(k)\n  low &lt;- 0\n  high &lt;- 1000\n  sigma &lt;- 1\n  target &lt;- log2_k\n  \n  for (i in 1:max_iter) {\n    psum &lt;- sum(exp(-(pmax(0, dists - rho)) / sigma))\n    if (abs(psum - target) &lt; tolerance) {\n      break\n    }\n    if (psum &gt; target) {\n      high &lt;- sigma\n      sigma &lt;- (low + high) / 2\n    } else {\n      low &lt;- sigma\n      if (high == 1000) {\n        sigma &lt;- sigma * 2\n      } else {\n        sigma &lt;- (low + high) / 2\n      }\n    }\n  }\n  return(sigma)\n}\n\n# Step 3: Calculate fuzzy membership strength\ncalculate_membership_strength &lt;- function(dists, sigmas, rhos) {\n  exp(-(pmax(0, dists - rhos)) / sigmas)\n}\n\n# Main function to compute fuzzy open sets and adjust sigma\nfuzzy_open_sets &lt;- function(X, k) {\n  knn_data &lt;- get_knn_exact(X, k)\n  knn_dists &lt;- knn_data$dists\n  knn &lt;- knn_data$knn\n  \n  n &lt;- nrow(knn_dists)\n  sigmas &lt;- numeric(n)\n  rhos &lt;- rowMins(knn_dists)  # The nearest neighbor distance\n  \n  # Adjust sigma for each point\n  for (i in 1:n) {\n    sigmas[i] &lt;- find_sigma(knn_dists[i, ], rhos[i], k)\n  }\n  \n  # Compute membership strength\n  memberships &lt;- matrix(0, n, n)\n  for (i in 1:n) {\n    memberships[i, knn[i, ]] &lt;- calculate_membership_strength(knn_dists[i, ], sigmas[i], rhos[i])\n  }\n  \n  return(list(memberships = memberships, sigmas = sigmas, rhos = rhos))\n}\n\n\n\nk &lt;- 5  # Number of neighbors\nresult &lt;- fuzzy_open_sets(data[,1:2], k)\n\n# Output the results\n#print(result$memberships)  # Fuzzy membership matrix\n#print(result$sigmas)  # Sigma values\n#print(result$rhos)  # Rho values (distance to nearest neighbor)\n\nThis effectively gives us multiple sets, one for each data points. for example if we set \\(k=5\\) for data point 1 we have set a set of data points [2, 3, 4, 5, 6] and each of these data points have a membership probability of being part of this set ([2: 1, 3: 0.603, 4: 0.364, 5: 0.221, 6: 0.134]). Similarly we have a set for data point 2 we have [1: 0.998, 3: 1, 4: 0.247, 5: 0.062, 6: 0.015]. Notice that the closes neighbor will always get the probability of \\(1\\).\nAt this stage, we begin by considering open sets in a topological sense. In topology, an open set is a collection of points where, for any given point within the set, we can find a surrounding neighborhood that still lies entirely within the set.\nNow, let’s think of each point in our data as being surrounded by an open set, and the overlap between these open sets represents some degree of similarity or connection between the points. When two points have significant overlap in their open sets, they are likely to be strongly connected, and when the overlap is small, the connection is weaker.\nTo represent these overlapping open sets in a more structured way, we use simplicial sets. A simplicial set is a mathematical structure made up of points (called vertices) and their connections (called simplices) in different dimensions.\nA 0-simplex represents a single point, or vertex, in our simplicial set. It’s the most basic building block. In our case, the 0-simplices correspond to the individual points in the data. We aren’t yet considering any relationships between the points but just the points themselves. Each point can be thought of as its own 0-simplex. A 1-simplex represents a connection between two 0-simplices, or an edge between two points. In the context of fuzzy simplicial sets, these edges are not just binary (either existing or not); they are weighted to reflect the strength of the connection. If two points have a lot of overlap between their open sets (meaning they are very similar or close to each other), the 1-simplex between them will have a higher weight. Conversely, if the overlap is smaller, the weight of the 1-simplex will be lower.\nThese connections between points can be naturally represented as graphs. In graph theory each point (or vertex) in our set is represented as a node in the graph (this corresponds to our 0-simplices). The relationships or connections between the points (1-simplices) are represented as edges in the graph.\nTo make things more concrete, let’s focus on two sets of points. For each set, we can represent the points as 0-simplices and their connections as 1-simplices.\n\n\nCode\n# Load necessary package\nlibrary(igraph)\n\n# Assuming `result$memberships` is a matrix where rows are vertices and columns are connections\n\n# Connections and weights for vertex 1\nconnections_v1 &lt;- which(result$memberships[1,] != 0)\nweights_v1 &lt;- (result$memberships[1, connections_v1])\n\n# Connections and weights for vertex 2\nconnections_v2 &lt;- which(result$memberships[2,] != 0)\nweights_v2 &lt;- (result$memberships[2, connections_v2])\n\n# Create edge lists for vertex 1 and vertex 2\nedges_v1 &lt;- c(rbind(rep(1, length(connections_v1)), connections_v1))\nedges_v2 &lt;- c(rbind(rep(2, length(connections_v2)), connections_v2))\n\n# Create the graphs for vertex 1 and vertex 2\ng_v1 &lt;- graph(edges = edges_v1, directed = FALSE)\ng_v2 &lt;- graph(edges = edges_v2, directed = FALSE)\n\n# Set edge weights as attributes\nE(g_v1)$weight &lt;- weights_v1\nE(g_v2)$weight &lt;- weights_v2\n\nvertex_colors_v1&lt;-rep(\"lightblue\",length(V(g_v1)))\n\nvertex_colors_v1[V(g_v1)==1] &lt;- \"red\"\nvertex_colors_v1[V(g_v1)==2] &lt;- \"blue\"\nvertex_colors_v2&lt;-rep(\"lightblue\",length(V(g_v1)))\n\nvertex_colors_v2[V(g_v2)==1] &lt;- \"red\"\nvertex_colors_v2[V(g_v2)==2] &lt;- \"blue\"\n\nvertex_sizes_v1 &lt;- ifelse(V(g_v1) %in% c(1, 2), 25, 15)  # Larger for vertex 1 and 2\nvertex_sizes_v2 &lt;- ifelse(V(g_v2) %in% c(1, 2), 25, 15)  # Larger for vertex 1 and 2\n\n# Set up two-panel plotting\npar(mfrow = c(1, 2))\n\n# Plot\nlayout_v1 &lt;- layout_with_dh(g_v1)\nplot(g_v1, \n     layout = layout_v1,              # Use a stable layout\n     edge.label = E(g_v1)$weight,     # Display weights on edges\n     vertex.label = V(g_v1),          # Show vertex numbers\n     edge.width = E(g_v1)$width,      # Adjust edge thickness\n     vertex.color = vertex_colors_v1, # Highlight vertices 1 and 2\n     vertex.size = vertex_sizes_v1,   # Increase size of vertices 1 and 2\n     main = \"Connections of point 1\")  # Title for panel 1\n\nlayout_v2 &lt;- layout_with_dh(g_v2)\nplot(g_v2, \n     layout = layout_v2,              # Use a stable layout\n     edge.label = E(g_v2)$weight,     # Display weights on edges\n     vertex.label = V(g_v2),          # Show vertex numbers\n     edge.width = E(g_v2)$width,      # Adjust edge thickness\n     vertex.color = vertex_colors_v2, # Highlight vertices 1 and 2\n     vertex.size = vertex_sizes_v2,   # Increase size of vertices 1 and 2\n     main = \"Connections of point 2\")  # Title for panel 2\n\n\n\n\n\n\n\n\n\nIn our case, since we are working with fuzzy simplicial sets, the edges in the graph are weighted. This means that the strength of the connection between two points is reflected in the weight of the edge that links them. Points with stronger overlaps in their open sets will have edges with larger weights, while weaker connections will have smaller weights.\nWe are only showing two opens sets which we built for point 1 and point 2. Each of these simplicial sets capture certain part of the manifold. However they are disconnected to have unified topological representation (a nerve) we need to fix how these two sets are interacting and we have to do that for all the pairs of sets.\n\n\n3.6.3 Fixing the interaction\nAs shown in the previous graph, we have a bunch of simplicial sets, and we want to “fix” their integration by determining a single weight between them. If you look closely at the graph for point 1, you will see that there is an edge from point 1 to point 2 with a certain probability. Similarly, in the simplicial set for point 2, there is an edge going from point 2 back to point 1, but this edge has a different probability.\nAt this stage, we are not concerned about other points; our focus is on the simplicial sets for point 1 and point 2 only. What we are trying to achieve is a single weight that connects the simplicial set for point 1 to the simplicial set for point 2. These weights, in a fuzzy simplicial set, represent the probability of the points being part of the same set or, more concretely, the likelihood of an edge existing between the two points.\nNow, given that we have two edges between point 1 and point 2—one going from point 1 to point 2 with some probability, and the other going from point 2 to point 1 with a different probability—we are interested in determining the likelihood of at least one edge existing between these two points.\nFrom probability theory, we know that the probability of at least one event (in this case, an edge existing) is complementary to the probability of no event (no edge existing). Specifically, the probability of having at least one edge between point 1 and point 2 is:\n\\[\nP(\\text{at least one edge}) = 1 - P(\\text{no edge exists between points 1 and 2})\n\\]\nFor simplicity, let’s define the probabilities of the edges as follows: - \\(P_1\\) is the probability of an edge going from point 1 to point 2. - \\(P_2\\) is the probability of an edge going from point 2 to point 1.\nThe probability that no edge exists between point 1 and point 2 is the product of the probabilities that neither edge is present. That is, the probability of no edge from point 1 to point 2 is \\(1 - P_1\\), and the probability of no edge from point 2 to point 1 is \\(1 - P_2\\). Therefore, the combined probability of no edges between points 1 and 2 is:\n\\[\nP(\\text{no edges}) = (1 - P_1)(1 - P_2)\n\\]\nThus, the probability that at least one edge exists between point 1 and point 2 is:\n\\[\nP(\\text{at least one edge}) = 1 - (1 - P_1)(1 - P_2) = P1+P2-P1P2\n\\]\nThis formula is called probabilistic t-conorm (like OR-like operations but in fuzzy logic) and it gives us a single probability that combines the two directional probabilities into one overall likelihood of there being a connection between the simplicial sets for point 1 and point 2. If we keep can keep doing this for all the pairs of sets we end up with a simplicial complex that according to nerve theory will capture the topology of the data\n\n\nCode\n# Load necessary library\nlibrary(igraph)\n\nt_conorm_matrix&lt;-result$memberships + result$memberships - (result$memberships*t(result$memberships))\nt_conorm_matrix[lower.tri(t_conorm_matrix)]&lt;-0\n\n# Create an edge list based on non-zero t-conorm values\nedges &lt;- which(t_conorm_matrix != 0, arr.ind = TRUE)\nedge_weights &lt;- t_conorm_matrix[edges]\n# Create the graph\ng &lt;- graph_from_edgelist(as.matrix(edges), directed = FALSE)\nE(g)$weight &lt;- edge_weights  # Assign the t-conorm values as weights\n\n# Set node and edge properties for plotting\nvertex_color &lt;- \"lightblue\"  # Set vertex color\n\nplot(g,\n     layout=as.matrix(data[,1:2]),\n     vertex.label = V(g),             # Show vertex numbers\n     vertex.size = 5,       # Vertex size\n     main = \"Final simplicial complex\")  # Title\n\n\n\n\n\n\n\n\n\nThis process is similar to t-SNE when it make the probabilities symmetric but uses much more statistically valid approach. We are now ready to move to the second part of the algorithm.\n\n\n3.6.4 Low dimensional similarities\nSimilar to t-SNE, we need to come up with a function that measures the similarity between points in the low-dimensional space. Let’s start with the following function:\n\\[\nq_{ij} = \\left(1 + \\| y_i - y_j \\|^2 \\right)^{-1}\n\\]\nThis is a rational function, meaning that it is a ratio of two polynomials. in this case, the numerator is a constant (1), and the denominator is a polynomial of degree 2 in terms of the distance between points \\(y_i\\) and \\(y_j\\). Rational functions are benfitial because they are computationally efficient. This means that they involve only basic arithmetic operations, are easy to compute, and their derivatives are straightforward to calculate. This simplicity is beneficial during the optimization process of dimensionality reduction algorithms, where computational efficiency is important.\nHowever, there is a problem with using this function. If you remember, in the high-dimensional space, the similarities between points are defined using an exponential decay function:\n\\[\np_{ij} = \\exp\\left(-d \\right)\n\\]\nWhere \\(d\\) is a distance. This exponential function decays rapidly with distance, meaning that the similarity between points decreases quickly as the distance increases. The rate of decay of the exponential function is such that distant points have negligible similarity, which helps in preserving the local structure of the data.\nIn contrast, our initial low-dimensional similarity function $ q_{ij} $ decays polynomially with distance:\n\\[\nq_{ij} = \\left(1 + d^2 \\right)^{-1}\n\\]\nThis function decreases at a slower rate compared to the exponential function. Specifically, as the distance \\(d\\) becomes large, \\(q_{ij}\\) behaves like \\(\\| y_i - y_j \\|^{-2}\\). The slower decay means that distant points in the low-dimensional space might still have significant similarity values, which can lead to distortions in the embedding. This mismatch in decay rates between $ p_{ij}$ and \\(q_{ij}\\) can cause the low-dimensional representation to inaccurately reflect the true relationships in the high-dimensional data.\nLet’s see how the decay compares between these two function:\n\n\nCode\n# Define the vector of distances\nd &lt;- as.matrix(dist(data[,1:2]))[1,-1]\n\n# Compute exponential decay values\nexp_decay &lt;- exp(-d)\n\n# Compute UMAP similarity values\numap_similarity &lt;- 1 / (1 + d^2)\n\n# Plot the exponential decay and UMAP similarity\nplot(d, exp_decay, type = \"o\", col = \"blue\", pch = 16, lty = 1,\n     ylim = c(0, 1), xlab = \"Distance d\", ylab = \"Similarity\",\n     main = \"Comparison of Decay Rates: Exponential vs. UMAP\")\nlines(d, umap_similarity, type = \"o\", col = \"red\", pch = 17, lty = 2)\nlegend(\"topright\", legend = c(\"Exponential Decay\", \"t-SNE distance\"),\n       col = c(\"blue\", \"red\"), pch = c(16, 17), lty = c(1, 2))\n\n\n\n\n\n\n\n\n\nTo address this issue, we need to adjust our low-dimensional similarity function to match the rate of decay of the high-dimensional similarities. One way to achieve this is by introducing parameters \\(a\\) and \\(b\\) into the function:\n\\[\nq_{ij} = \\left(1 + a \\| y_i - y_j \\|^{2b} \\right)^{-1}\n\\]\nBy carefully selecting the parameters \\(a\\) and \\(b\\), we can control the rate at which \\(q_{ij}\\) decays with distance. Parameter \\(b\\) adjusts the exponent of the distance term. By changing \\(b\\), we can make the decay of \\(q_{ij}\\) faster or slower. A smaller \\(b\\) leads to a faster decay, making \\(q_{ij]\\) decrease more rapidly with distance. Parameter \\(a\\) scales the distance term. Adjusting \\(a\\) shifts the rate of decay horizontally, affecting how quickly the similarity decreases as distance increases.\nThe goal is to find values of \\(a\\) and \\(b\\) such that the decay of \\(q_{ij}\\) approximates the exponential decay of \\(p_{ij}\\) over the range of distances that are most relevant for preserving the data’s structure. By matching the decay rates, we ensure that the similarities in the low-dimensional space reflect those in the high-dimensional space. This is basically what UMAP uses to measure the similarities. Let’s see an example of this:\n\n\nCode\n# Define the vector of distances\nd &lt;- as.matrix(dist(data[,1:2]))[1,-1]\n\n# Set parameters for the UMAP function\nb &lt;- 0.79\na &lt;- 1.93  # Approximate value from UMAP settings\n\n# Compute exponential decay values\nexp_decay &lt;- exp(-d)\n\n# Compute UMAP similarity values\numap_similarity &lt;- 1 / (1 + a * d^(2 * b))\n\n# Plot the exponential decay and UMAP similarity\nplot(d, exp_decay, type = \"o\", col = \"blue\", pch = 16, lty = 1,\n     ylim = c(0, 1), xlab = \"Distance d\", ylab = \"Similarity\",\n     main = \"Comparison of Decay Rates: Exponential vs. UMAP\")\nlines(d, umap_similarity, type = \"o\", col = \"red\", pch = 17, lty = 2)\nlegend(\"topright\", legend = c(\"Exponential Decay\", \"UMAP Similarity\"),\n       col = c(\"blue\", \"red\"), pch = c(16, 17), lty = c(1, 2))\n\n\n\n\n\n\n\n\n\nThe question is how we are going to set \\(a\\) and \\(b\\). Let’s start with our most important assumption that is the data (or distances) are uniformly spread on the manifold. The only thing that we need to set is the spread (remember the spread parameter in UMAP) Let’s generate such data:\n\nspread &lt;- 1\n\n# Generate a range of distances in low-dimensional space\nx &lt;- seq(0, spread * 3, length.out = 300)\n\nhist(x)\n\n\n\n\n\n\n\n\nWe now mimic the UMAP distance function. If you remember: \\[\nw(x_j, x_{ij}) = \\exp\\left(-\\frac{\\max(0, d(x_j, x_{ij}) - \\rho_i)}{\\sigma_i}\\right)\n\\] As we said in t-SNE section, we don’t need \\(\\sigma\\) in the lower dimension as we assume things to be uniformly distrubted. So we have\n\\[\nw(x_j, x_{ij}) = \\exp\\left(-{\\max(0, d(x_j, x_{ij}) - \\rho_i)}\\right)\n\\] If you remember again, any data point with distance lower than \\(\\rho_i\\) is going to get a value of \\(w(x_j, x_{ij}) =1\\) so we need to set \\(\\rho_i\\). Again because of our assumption, since everything is uniformly distrubted, we can set just one global value for \\(\\rho_i\\), let’s call it min_dist (remember the parameter of UMAP!!). Now we have\n\\[\nw(x_j, x_{ij}) = \\exp\\left(-{\\max(0, d(x_j, x_{ij}) - \\text{min_dist})}\\right)\n\\]\nGiven our distances we can now do the transformation:\n\nmin_dist&lt;-0.01\ny &lt;- numeric(length(x))\nfor (i in seq_along(x)) {\ny[i] &lt;- exp(-max(0,x[i] - min_dist))\n  }\nhist(y)\n\n\n\n\n\n\n\n\nNow there is only one thing left that is we need to change \\(a\\) and \\(b\\) in \\(\\left(1 + a \\| y_i - y_j \\|^{2b} \\right)^{-1}\\) so \\(x\\) and \\(y\\) because very similar (UMAP uses non-linear least squere but we are going to just define a cost function sum of suqered and minimize with respect to \\(a\\) and \\(b\\).\n\n# Initialize a data frame to store parameter values\nparams_history &lt;- data.frame(a = numeric(), b = numeric())\n\n  # The low-dimensional distance function\n  dist_low_dim &lt;- function(x, a, b) {\n    1 / (1 + a * x^(2 * b))\n  }\n  \n  # Define the fitting function to minimize\n  fit_function &lt;- function(params) {\n    a &lt;- params[1]\n    b &lt;- params[2]\n    predicted &lt;- dist_low_dim(x, a, b)\n     params_history &lt;&lt;- rbind(params_history, data.frame(a = a, b = b))\n    return(sum((predicted - y)^2))  # Sum of squared errors\n  }\n  \n    # optimize\n  opt_result &lt;- optim(c(1, 1), fit_function)\n\nThis optimization tries to make the come up with \\(a\\) and \\(b\\) such that the decay looks like an exponential one. You can see the results here:\n\n\nCode\nlibrary(plotly)\n\n# Compute the distance vector 'd'\nd &lt;- as.matrix(dist(data[,1:2]))[1,-1]\n\n# Compute the exponential decay reference line\nexp_decay &lt;- exp(-d)\n\n# Create a data frame to store the frames\nZ_frames &lt;- data.frame(\n  dim1 = c(),  # Placeholder for first dimension\n  dim2 = c(),  # Placeholder for second dimension\n  iteration = c()  # Placeholder for iteration (time step)\n)\n\n# Define a grid of 'a' and 'b' values over time\na_values &lt;- params_history$a\nb_values &lt;- params_history$b\n\n# Create frames by looping through the a and b values\nfor (i in seq_along(a_values)) {\n  a &lt;- a_values[i]\n  b &lt;- b_values[i]\n  \n  # Compute the UMAP similarity for this iteration\n  sm &lt;- 1 / (1 + a * d^(2 * b))\n  \n  # Add the computed data for this iteration to the dataframe\n  Z_frames &lt;- rbind(Z_frames, data.frame(dim1 = d, dim2 = sm, \n                                         iteration = i,col=\"transfomed d\"))\n   Z_frames &lt;- rbind(Z_frames, data.frame(dim1 = d, dim2 = exp(-d), \n                                         iteration = i,col=\"exp(-d)\"))\n}\n\n# Now create the plotly animation with the exp(-d) line\np &lt;- plot_ly(\n  data = Z_frames, \n  x = ~dim1, \n  y = ~dim2,\n  color=~col,\n  frame = ~iteration,  # Animate through iterations\n  type = 'scatter', \n  mode = 'lines+markers', \n  marker = list(size = 10)\n) %&gt;%\n  layout(\n    title = \"UMAP Similarity Animation with exp(-d) Reference\",\n    xaxis = list(title = \"Distance\"),\n    yaxis = list(title = \"Transformed distance\"),\n    showlegend = TRUE\n  )\n\np\n\n\n\n\n\n\nWe can create a function so later we can use it in the UMAP implmentation\n\nfit_ab &lt;- function(min_dist, spread) {\n  # The function for piecewise similarity transformation\n  f_transform &lt;- function(x, min_dist) {\n    y &lt;- numeric(length(x))\n    for (i in seq_along(x)) {\n  y[i] &lt;- exp(-max(0,x[i] - min_dist))\n    }\n    return(y)\n  }\n  \n  # The low-dimensional distance function\n  dist_low_dim &lt;- function(x, a, b) {\n    1 / (1 + a * x^(2 * b))\n  }\n  \n  # Generate a range of distances in low-dimensional space\n  x &lt;- seq(0, spread * 3, length.out = 300)\n  \n  # Apply the piecewise function to the distances\n  y &lt;- f_transform(x, min_dist)\n  \n  # Define the fitting function to minimize\n  fit_function &lt;- function(params) {\n    a &lt;- params[1]\n    b &lt;- params[2]\n    predicted &lt;- dist_low_dim(x, a, b)\n    return(sum((predicted - y)^2))  # Sum of squared errors\n  }\n  \n  # Optimize the parameters a and b \n  # Starting with initial guesses for a and b\n  opt_result &lt;- optim(c(1, 1), fit_function)\n  \n  # Extract the fitted a and b\n  a &lt;- opt_result$par[1]\n  b &lt;- opt_result$par[2]\n  \n  return(list(a = a, b = b))\n}\n\nJust to give you a summary, we came up with the function \\((1 + a(||y_i - y_j||^2)^b)^{-1}\\) that measures the similarity in the lower dimension. We use an optimization algorithm to select the best \\(a\\) and \\(b\\). This optimization is irrespective of the data we are working with. It is more capturing the relationship between the polynomial and exponential decay.\nWe now basically need two more steps to go wrap up UMAP algorithm that is how to optimize lower dimension to look like the high dimension and then where to start optimizing. Let’s start with the optimization.\n\n\n3.6.5 Optimization (cross-entropy)\nLet’s repeated that the primary objective of UMAP is to find a low-dimensional representation of data that preserves the local and global structure present in the high-dimensional space. To achieve this, UMAP constructs a high-dimensional graph representing the data’s structure and then optimizes the low-dimensional embeddings to reflect this structure.\nIn order to compute how well we are doing in low-dimensional embeddings compared to high-dimensional space, we need a measure. Cross-entropy loss is a measure of how well the low-dimensional embedding represents the high-dimensional data’s relationships. It quantifies the difference between the probabilities of connections (or similarities) in the high-dimensional space and those in the low-dimensional embedding. By minimizing the cross-entropy loss, we ensure that points close together in high-dimensional space remain close in the embedding, and those far apart stay distant.\nThe cross-entropy loss \\(C\\) used in UMAP is given by:\n\\[\nC = \\sum_{i \\neq j} \\left[ \\text{CE}_{\\text{attractive}}(i, j) + \\text{CE}_{\\text{repulsive}}(i, j) \\right]\n\\]\nwhere:\n\n\\(\\text{CE}_{\\text{attractive}}(i, j) = -p_{ij} \\log q_{ij}\\)\n\\(\\text{CE}_{\\text{repulsive}}(i, j) = -(1 - p_{ij}) \\log (1 - q_{ij})\\)\n\\(p_{ij}\\) is the probability of points \\(i\\) and \\(j\\) being connected in high-dimensional space.\n\\(q_{ij}\\) is the probability in the low-dimensional embedding.\n\nWhat are attractive and repulsive? The forces that pull similar points closer together in the embedding are called attractive. They ensure that points that are neighbors in the high-dimensional space remain close in the low-dimensional embedding. On the opposite side, repulsive forces push dissimilar points apart in the embedding. They essentially prevent points that are not neighbors from clustering together in the embedding. The idea here is that if we have a lower dimension \\(Y\\), it should provide a good balance between the attractive and repulsive forces. This means that we are going to start with some random \\(Y\\) and then update it iteratively based on attractive and repulsive balance so that we can minimize the cross-entropy loss and achieve a meaningful low-dimensional embedding.\nSimilar to t-SNE To update the low-dimensional embedding \\(Y\\) we need to calculate the gradient of the cross-entropy loss with respect to \\(Y\\). The optimization process involves iteratively adjusting \\(Y\\) using gradient descent, where the attractive and repulsive forces guide the updates.\nIt turned out that calculating gradients over all pairs \\((i, j)\\) is computationally expensive for large datasets. We can however try to approximate this using a sampling method. This pretty much means that we select some pairs of points that we think they are neighbours in high dimensional space and optimize the attractive part for them. We then sample some random points (negative samples) and optimize the repulsive part (we believe most pairs in high-dimensional data are dissimilar in high dimensional space). This approach let’s us calculate two gradients one for attractive and one for repulsive pairs.\nThe attractive Part of the Loss was:\n\\[\nC_{\\text{attr}} = -\\sum_{(i,j) \\in \\text{edges}} p_{ij} \\log q_{ij}\n\\] \\[\nq_{ij}=1 / (1 + a * d^{2 * b})\n\\] The derivative with respect to \\(d_{ij}\\) is given by\n\\[\n\\frac{\\partial C_{\\text{attr}}}{\\partial d_{ij}} = - \\frac{2b \\cdot d_{ij}^{2b - 1}}{1 + a \\cdot d_{ij}^{2b}}\n\\]\nSimilarly for the repulsive part we have\n\\[\nC_{\\text{rep}} = -\\sum_{(i,j) \\notin \\text{edges}} (1 - p_{ij}) \\log (1 - q_{ij})\n\\]\nwith a gradient of:\n\\[\n\\frac{\\partial C_{\\text{rep}}}{\\partial d_{ij}} = -  \\frac{2b}{d_{ij} \\left( 1 + a \\cdot d_{ij}^{2b} \\right)}\n\\]\nLet’s see how we can implement these two:\n\n# Attractive force update for a pair (i, j)\nupdate_attractive &lt;- function(y_i, y_j, a, b) {\n  \n  m2ab = -2*a*b\n  codiff &lt;- y_i - y_j\n  codist2 &lt;- sum(codiff*codiff)\n  bm1 = b-1\n  if (codist2 &gt; 0) {\n grad_coeff = (m2ab*codist2^bm1) / ((1+a*(codist2^b)))\n    \n    # Gradient\n    grad &lt;- grad_coeff * codiff\n  } else {\n    grad &lt;- 0 * codiff  # Zero gradient if distance is zero\n  }\n  grad[grad &lt; -4] = -4\n  grad[grad &gt; 4] = 4\n  return(grad)\n}\n\n# Repulsive force update for a pair (i, k)\nupdate_repulsive &lt;- function(y_i, y_k, a, b, repulsion_strength) {\n  m2ab = -2*a*b\n  codiff &lt;- y_i - y_k\n  codist2 &lt;- sum(codiff*codiff)\n  bm1 = b-1\n  p2gb&lt;-2*repulsion_strength*b\n  \n  if (codist2 &gt; 0) {\n   grad_coeff = p2gb / ((0.001+codist2)*(1+a*(codist2^b)))\n    \n    # Gradient\n    grad &lt;- grad_coeff * codiff\n  } else {\n    grad &lt;- 0 * codiff\n  }\n  if(grad_coeff&gt;0)\n  {\n    grad[grad &lt; -4] = -4\ngrad[grad &gt; 4] = 4\n  }else{\n    grad&lt;-4\n  }\n\n\n  return(grad)\n}\n\nGiven this now we can go ahead with our sampling-Based Optimization:\n\nInitialize Embeddings: Start with random low-dimensional positions for each data point.\nIterate Over Epochs: Repeat the optimization for a fixed number of epochs.\nAttractive Updates:\n\nFor each neighbor \\(j\\) of \\(i\\), compute the attractive gradient and update embeddings.\n\nRepulsive Updates:\n\nFor each point \\(i\\), sample a few negative samples \\(k\\) and compute the repulsive gradient.\n\n\n\n\nCode\n# Get our high dimensional distances:\ngraph&lt;-result$memberships + t(result$memberships) - (result$memberships*t(result$memberships))\n\nN&lt;-nrow(graph)\n# set a and b\na &lt;- 1.895526\nb &lt;- 0.8005876\n\n# set learning rate\nalpha &lt;- 1\n\n# number of components\nn_components &lt;- 2\n# negative_sample_rate\nnegative_sample_rate &lt;- 5\n\n# Step 1: Initialize embeddings\nset.seed(10)\nY &lt;- 20*matrix(runif(N * n_components), nrow = N, ncol = n_components)\n\nn_epochs &lt;- 200\n\n# Step 2: Optimization loop (Epochs)\n      # Attractive forces\nedges &lt;- which(graph != 0, arr.ind = TRUE)\n  n_edges &lt;- nrow(edges)\nfor (epoch in 1:n_epochs) {\n    for (e_idx in (1:n_edges)) {\n      \n      i &lt;- edges[e_idx, 1]\n      j &lt;- edges[e_idx, 2]\n      p_ij_value &lt;- graph[i, j]\n      if(runif(1)&lt;=p_ij_value){\n    # Compute attractive gradient\n      grad_attr &lt;- update_attractive(Y[i, ], Y[j, ], a, b)\n      \n      # Update embeddings\n      Y[i, ] &lt;- Y[i, ] + alpha * grad_attr\n      Y[j, ] &lt;- Y[j, ] - alpha * grad_attr  # Symmetric update\n      \n      \n       for (n in 1:negative_sample_rate) {\n        k &lt;- sample(setdiff(1:N, i), 1)  # Ensure k != i\n          grad_rep &lt;- update_repulsive(Y[i, ], Y[k, ], a, b, 1)\n          \n          # Update embedding\n          Y[i, ] &lt;- Y[i, ] + alpha * grad_rep\n        \n      }\n      \n      }\n      \n  \n    }\n alpha&lt;-1*((1-epoch/n_epochs))\n}\nplot(Y)\n\n\n\n\n\n\n\n\n\nWe can see that the embedding starts having a good shape. There is, however, still room for improvement. While the random initialization provides a starting point, it can sometimes lead to suboptimal positioning of points, especially in capturing the global structure of the data. As optimization progresses, the local relationships between points will likely become more accurate, but achieving a better global alignment might require more iterations. Using a more structured initialization could potentially speed up convergence and provide a more balanced representation of both local and global features from the outset.\n\n\n3.6.6 Spectral embedding\nThere are many methods that can be used as initiation of UMAP but spectral embedding is probably one the main choices. The idea is to embed the data into a lower-dimensional space based on the eigenvectors of a graph Laplacian matrix. This matrix encodes the relationships between all data points, offering a globally-aware embedding. We are not going to cover too much of this but to give some intuition about it. Let’s say that \\(A\\) represents the high-dimensional similarity graph from UMAP. In this case, we want to quickly create a lower dimensonal represetnation of the data using a cost function that shows how well the low-dimensional embedding preserves the structure of the high-dimensional data based on the similarities encoded in \\(A\\).\nThe cost function is:\n\\[\nC(\\mathbf{X}) = \\frac{1}{2} \\sum_{i,j} A_{ij} || \\mathbf{x}_i - \\mathbf{x}_j ||^2\n\\]\nWhere: - \\(A_{ij}\\) represents the high-dimensional similarity between points \\(i\\) and \\(j\\), as computed by UMAP. - \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) are the coordinates of points \\(i\\) and \\(j\\) in the low-dimensional embedding. - \\(|| \\mathbf{x}_i - \\mathbf{x}_j ||^2\\) is the squared Euclidean distance between these points in the low-dimensional space.\nThe cost function is a penalty for how much the low-dimensional embedding \\(\\mathbf{X}\\) distorts the high-dimensional relationships encoded in \\(A\\). If two points \\(i\\) and \\(j\\) are highly similar in the high-dimensional space (i.e., \\(A_{ij}\\) is large), the cost function wants these points to be close together in the low-dimensional space (i.e., \\(|| \\mathbf{x}_i - \\mathbf{x}_j ||^2\\) should be small). If \\(A_{ij}\\) is large and the distance \\(|| \\mathbf{x}_i - \\mathbf{x}_j ||^2\\) is large, the cost function will assign a high penalty, indicating that the low-dimensional embedding is distorting the relationship between these points. If points \\(i\\) and \\(j\\) are not very similar in the high-dimensional space (i.e., \\(A_{ij}\\) is small or zero), the cost function doesn’t care much about their distance in the low-dimensional space. Therefore, the term \\(A_{ij} || \\mathbf{x}_i - \\mathbf{x}_j ||^2\\) will be small even if \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) are far apart in the embedding. We can optimize this using iterative methods like gradient descend or something but there is an even more straightforward way of doing it.\nLet’s define:\n\n\\(\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n]^T\\) as the matrix of low-dimensional coordinates.\n\\(D\\) as the diagonal degree matrix with \\(D_{ii} = \\sum_j A_{ij}\\).\n\\(A\\) as the adjacency matrix.\n\nThe cost function becomes:\n\\[\nC(\\mathbf{X}) = \\text{Tr}(\\mathbf{X}^T D \\mathbf{X}) - \\text{Tr}(\\mathbf{X}^T A \\mathbf{X})\n\\]\nWhere \\(\\text{Tr}\\) denotes the trace of a matrix. Using the graph Laplacian \\(L = D - A\\), we can express the cost function as:\n\\[\nC(\\mathbf{X}) = \\text{Tr}(\\mathbf{X}^T L \\mathbf{X})\n\\]\nThis is the final form of the cost function in terms of the Laplacian matrix.\nhe Laplacian matrix \\(L\\) is symmetric and positive semi-definite, meaning it has real non-negative eigenvalues and can be decomposed into its eigenvectors and eigenvalues:\n\\[\nL = U \\Lambda U^T\n\\]\nWhere: - \\(U\\) is the matrix of eigenvectors of \\(L\\), - \\(\\Lambda\\) is the diagonal matrix of eigenvalues.\nThe goal of spectral embedding is to minimize \\(C(\\mathbf{X})\\). To minimize this cost, we find the eigenvectors corresponding to the smallest non-zero eigenvalues of \\(L\\), which provide the smoothest variation across the graph.\nWe can calculate this using the following code:\n\n# Compute the degree matrix\nD &lt;- diag(rowSums(graph))\n\n# Compute the Laplacian matrix\nL &lt;- D - graph\n\n# Eigen decomposition of the Laplacian matrix\neigen_decomp &lt;- eigen(L)\n\n# Extract the eigenvectors corresponding to the smallest non-zero eigenvalues\n# The first eigenvalue corresponds to the smallest eigenvalue (which is 0), so skip it.\nk &lt;- 2  # Number of dimensions for the embedding\nembedding &lt;- eigen_decomp$vectors[, 2:(k+1)]  # Take the next k eigenvectors\n\nWe can now use this embedding instread of random initialization.\n\n\nCode\n# set a and b\na &lt;- 1.895526\nb &lt;- 0.8005876\n\n# set learning rate\nalpha &lt;- 1\n\n# number of components\nn_components &lt;- 2\n# negative_sample_rate\nnegative_sample_rate &lt;- 5\n\n# Step 1: Initialize embeddings using spectral\nset.seed(10)\nY &lt;- embedding\n\nn_epochs &lt;- 200\n\n# Step 2: Optimization loop (Epochs)\n      # Attractive forces\nedges &lt;- which(graph != 0, arr.ind = TRUE)\n  n_edges &lt;- nrow(edges)\nfor (epoch in 1:n_epochs) {\n    for (e_idx in (1:n_edges)) {\n      \n      i &lt;- edges[e_idx, 1]\n      j &lt;- edges[e_idx, 2]\n      p_ij_value &lt;- graph[i, j]\n      if(runif(1)&lt;=p_ij_value){\n    # Compute attractive gradient\n      grad_attr &lt;- update_attractive(Y[i, ], Y[j, ], a, b)\n      \n      # Update embeddings\n      Y[i, ] &lt;- Y[i, ] + alpha * grad_attr\n      Y[j, ] &lt;- Y[j, ] - alpha * grad_attr  # Symmetric update\n      \n      \n       for (n in 1:negative_sample_rate) {\n        k &lt;- sample(setdiff(1:N, i), 1)  # Ensure k != i\n          grad_rep &lt;- update_repulsive(Y[i, ], Y[k, ], a, b, 1)\n          \n          # Update embedding\n          Y[i, ] &lt;- Y[i, ] + alpha * grad_rep\n        \n      }\n      \n      }\n      \n  \n    }\n alpha&lt;-1*((1-epoch/n_epochs))\n}\nplot(Y)\n\n\n\n\n\n\n\n\n\nThe embedding is now much better embedding and the algorithm converges much faster. With this we wrap up the UMAP section.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)</span>"
    ]
  },
  {
    "objectID": "umap.html#summary",
    "href": "umap.html#summary",
    "title": "2  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)",
    "section": "3.7 Summary",
    "text": "3.7 Summary\nUMAP, which we derived together, works by preserving both local and global structures in a more efficient and scalable way compared to t-SNE. The algorithm began by constructing a graph based on the relationships in the high-dimensional space, using the n_neighbors parameter to determine how many neighbors to consider for each data point. UMAP then converted these distances into a fuzzy topological structure, where the graph’s weights represented the probability of points being connected. This graph approximated the local manifold structure of the high-dimensional data.\nFor the low-dimensional space, UMAP created another graph and sought to match the two graphs by minimizing a cross-entropy loss function. This function measured the difference between the high- and low-dimensional fuzzy topological structures. The optimization process adjusted the positions of the points in the low-dimensional embedding to minimize the loss, balancing the need to preserve local details while also capturing broader global structures.\nBy using this approach, UMAP efficiently captured both local and global structures of the data. Its reliance on topology and manifold learning allowed for more flexible and scalable dimensionality reduction, especially for large datasets.\nThis overall process made UMAP an effective tool for revealing clusters, patterns, and global structures in the data, while also providing more computational efficiency than t-SNE.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)</span>"
    ]
  }
]