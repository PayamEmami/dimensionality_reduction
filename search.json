[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dimensionality reduction methods",
    "section": "",
    "text": "Introduction\nIn this chapter we are going to learn about a few commonly used dimensionality reduction methods in life science research. We are first going to define what dimensionality reduction is, why and when we should use it. We will then learn how to use these methods and how they work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-reducing-dimentions",
    "href": "index.html#why-reducing-dimentions",
    "title": "Dimensionality reduction methods",
    "section": "Why reducing dimentions",
    "text": "Why reducing dimentions\nThere are many reasons why we might want to reduce data dimensions.\n\nData Visualization: This is probably one of the most common reasons we do dimensionality reduction. High-dimensional data is difficult to visualize in its raw form. Dimensionality reduction techniques like PCA and t-SNE allow us to reduce this data into two or three dimensions. We can then plot the data and see trends, clusters, or outliers. Obviously, having our data summarized to a couple of variables is easier for the human eye to comprehend.\nRemoving Noise and Redundancy: Despite that we have measured many variables, not all of them contribute equally to the information contained in the data. Some variables may be noisy, irrelevant, or redundant. Dimensionality reduction methods can help eliminate these less useful dimensions, giving us a cleaner, more informative dataset. Similary, one can use these method to adjust for unwanted trends in the data such as batch effects etc.\nUncovering Patterns and Trends: Having a lot of variables is not always good, often, the true underlying structure of the data is hidden within many dimensions. Dimensionality reduction helps to reveal the most important patterns and trends by summarizing the data by some form of combinition of the raw variables making it easier to detect relationships between samples and uncover valuable insights.\nImproving Model Performance: We know that in machine learning, too many variables can lead to overfitting, where a model performs well on training data but poorly on unseen data. Dimensionality reduction can help prevent this by focusing on the most important features. So we can improve the model‚Äôs generalizability and predictive performance.\n\nThere might also be other reasons to reduce the dimention of the data. For example working with large, high-dimensional datasets can be computationally expensive. Dimensionality reduction lowers the number of variables, which reduces the memory and processing power needed to analyze the data.\nIn general, specially in OMICS data analysis, dimensionality reduction is often performed at some point in the analysis workflow. It might be the case that the results of it might not be the main interest but still might affect the overall decision making process. The example can be quality check, outlier detection etc.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-dimentionality-reduction-works",
    "href": "index.html#how-dimentionality-reduction-works",
    "title": "Dimensionality reduction methods",
    "section": "How dimentionality reduction works",
    "text": "How dimentionality reduction works\nAdmittedly, the answer to this question is not so simple. There are many different approaches to dimensionality reduction, each with its own principles and techniques. Some methods, like Principal Component Analysis (PCA), focus on finding directions in the data that capture the most variance. Others, such as t-SNE and UMAP, are more concerned with preserving the local structure and distances between data points. Some like autoencoders learn compact representations of the data by compressing it into a lower-dimensional form and reconstructing the original inputs. There are even methods like Linear Discriminant Analysis (LDA) and Non-negative Matrix Factorization (NMF) also offer unique ways to reduce dimensions by focusing on class separation or non-negative decomposition.\nMost of these methods, however, work in one way or another with the concept of distances or similarities between data points. For example, PCA seeks to maximize the variance (which is linked to the spread, or ‚Äúdistance,‚Äù between data points in the dataset), while t-SNE and UMAP preserve relative distances so that points close together in high-dimensional space remain close after dimensionality reduction. Even methods like autoencoders rely on optimization processes that capture patterns of similarity in the data.\nIt is however very important to pay attend to what the selected method is seeking to show in lower dimentions. This will directly affect the interpretation and usage of the lower dimentional space. So while the goal of these methods remains the same (preserving the structure in the original dataset), the definition of ‚Äústructure‚Äù varies between this methods. For example, PCA is more focused on capturing global structure, meaning it seeks to maximize the overall variance across the entire dataset. It tries to find the directions in which the data varies the most, but it might overlook subtle local relationships between data points. Methods like t-SNE and UMAP focus on local structure, meaning they try to keep the relative distances between nearby points, which is great for understanding clusters but may not accurately show large-scale patterns in the data.\nThe point that i wanted to make is that the choice of dimensionality reduction method directly influences what information from the original data are retained and what are lost. Understanding each method‚Äôs ‚Äústructure‚Äù definition is very important for making decisions about how to interpret and use the reduced data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-we-are-going-to-cover",
    "href": "index.html#what-we-are-going-to-cover",
    "title": "Dimensionality reduction methods",
    "section": "What we are going to cover?",
    "text": "What we are going to cover?\nPCA is probably the most well-known dimensionality reduction method. You can learn more about PCA (https://payamemami.com/pca_basics/). In the rest of this book, we are going to learn some other useful dimensionality reduction methods such as t-SNE and UMAP. We might bring up PCA just to compare the results and mathematical formulation. So if you are still learning about dimensionality reduction, please have a look athe PCA chapter first.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "tsne.html",
    "href": "tsne.html",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "",
    "text": "2 t-sne in R\nWithout going further into the details, we are going to start using t-SNE in R. The main function used for this is Rtsne() from the Rtsne package. Before that i want to simulate some data so we can check later how t-SNE is doing.\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of points per subcluster\nn &lt;- 25\n\n# Manually define main cluster centers (global structure)\nmain_cluster_centers &lt;- data.frame(\n  x = c(0, 5, 10, 15),\n  y = c(0, 5, 10, 15),\n  z = c(0, 5, 10, 15),\n  cluster = factor(1:4)\n)\n\n# Manually define subcluster offsets relative to each main cluster\n# These small offsets will determine subcluster locations within each main cluster\nsubcluster_offsets &lt;- data.frame(\n  x_offset = c(-0.25, 0.25, -0.25, 0.25),\n  y_offset = c(-0.25, -0.25, 0.25, 0.25),\n  z_offset = c(0.25, -0.25, -0.25, 0.25),\n  subcluster = factor(1:4)\n)\n\n# Initialize an empty data frame to hold all data\ndata &lt;- data.frame()\n\n# Generate data for each main cluster with subclusters\nfor (i in 1:nrow(main_cluster_centers)) {\n  for (j in 1:nrow(subcluster_offsets)) {\n    # Calculate subcluster center by adding the offset to the main cluster center\n    subcluster_center &lt;- main_cluster_centers[i, 1:3] + subcluster_offsets[j, 1:3]\n    \n    # Generate points for each subcluster with a small spread (to form local clusters)\n    subcluster_data &lt;- data.frame(\n      gene1 = rnorm(n, mean = subcluster_center$x, sd = 0.25),  # Small spread within subclusters\n      gene2 = rnorm(n, mean = subcluster_center$y, sd = 0.25),\n      gene3 = rnorm(n, mean = subcluster_center$z, sd = 0.25),\n      cluster = main_cluster_centers$cluster[i],\n      subcluster = subcluster_offsets$subcluster[j]\n    )\n    \n    # Add generated subcluster data to the main data frame\n    data &lt;- rbind(data, subcluster_data)\n  }\n}\nThis data has just three dimentions for 400 samples (4 major clusters). We can plot the data here:\nCode\n# Visualize the original data in 3D, distinguishing clusters and subclusters\nlibrary(dplyr)\nlibrary(plotly)  # For interactive 3D plots\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\nFor this data, the features are in the column and samples in the row.\nWe are now going to do a t-SNE on this data and see the results. There are some parameters to set but we just go for the default ones. We also do PCA and show the results side by side\nCode\nlibrary(Rtsne)\nlibrary(ggplot2)\nlibrary(cowplot)\n\nset.seed(123)\ntsne_results_30 &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")])\n)\n\n\ndata$tsne_x_30 &lt;- tsne_results_30$Y[, 1]\ndata$tsne_y_30 &lt;- tsne_results_30$Y[, 2]\n\n\ntsne_plot &lt;- plot_ly(\n  data, x = ~tsne_x_30, y = ~tsne_y_30, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n\npca_results &lt;- prcomp(data[, c(\"gene1\", \"gene2\", \"gene3\")], scale. = FALSE)\ndata$pca_x &lt;- pca_results$x[, 1]\ndata$pca_y &lt;- pca_results$x[, 2]\n\n\npca_plot &lt;- plot_ly(\n  data, x = ~pca_x, y = ~pca_y, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n  \nsubplot(pca_plot%&gt;%layout(showlegend = FALSE), tsne_plot%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"PCA\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"t-SNE\", showarrow = F, xref='paper', yref='paper'))\n)\nThe plot shows a side-by-side comparison of results obtained using PCA (left) and t-SNE (right) for dimensionality reduction. In the PCA plot, the data points are clearly separated by their clusters along the horizontal axis, but the clusters themselves are quite stretched and not very compact, indicating that PCA is apturing global variance sources but doesn‚Äôt reveal tight local groupings.\nOn the other hand, the t-SNE plot reveals well-separated, compact clusters, showing how good t-SNE is at preserving local structures within the data. The clusters are clearly distinct from one another, and their circular, tightly packed shapes indicate that t-SNE effectively keeps points that are similar (close in high-dimensional space) together in the low-dimensional projection. However, the distances between clusters may not reflect global relationships as well as PCA does (we will get back to this later).",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#input-parameters",
    "href": "tsne.html#input-parameters",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "2.1 Input parameters",
    "text": "2.1 Input parameters\nLike any other method, t-SNE also requires some input parameters. Let‚Äôs start with the most essential ones and later go through the rest.\nThe data matrix (X) is the core input for t-SNE, where each row represents an observation, and each column is a variable or feature. This matrix is typically high-dimensional, and t-SNE‚Äôs job is to map it into a lower-dimensional space (usually 2D or 3D) to make patterns more interpretable.\nNext, dims defines the dimensionality of the output. Typically, we choose 2D for visualization purposes, but 3D is also possible if more complexity is required.\nThe perplexity parameter is probably the most important one. It controls how t-SNE balances local and global structures of the data. You can think of it as determining how many ‚Äúneighbors‚Äù each data point should consider when projecting into the low-dimensional space. Choosing the right perplexity is very important because it affects how t-SNE interprets the relationships between data points.\nAnother key parameter in this specific implementation is theta, which adjusts the balance between accuracy and computational speed. For large datasets, using a larger theta can make t-SNE run faster but at the cost of some accuracy. If you prioritize precision, especially for smaller datasets, you can set this parameter to 0 for exact t-SNE.\nWe also have the max_iter parameter, which controls the number of iterations the algorithm goes through during optimization. More iterations give t-SNE time to better fine-tune the output, though in practice, the default is often sufficient unless you notice the algorithm hasn‚Äôt converged.\nAfter these essential parameters, additional settings like PCA preprocessing, momentum terms, and learning rate can further fine-tune the performance of t-SNE for different types of data. We will talk about these later.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#perplexity",
    "href": "tsne.html#perplexity",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "2.2 Perplexity",
    "text": "2.2 Perplexity\nI had to devote a small section to this parameter because it is probably the most important one (if you already have your data üòâ). Perplexity essentially defines how t-SNE balances attention between local and global structures in your data. You can think of it as controlling the size of the neighborhood each data point considers when positioning itself in the lower-dimensional space.\nA small perplexity value (e.g., 5‚Äì30) means that t-SNE will focus more on preserving the local structure, i.e., ensuring that close neighbors in the high-dimensional space stay close in the low-dimensional projection. This is great for datasets with small clusters or when you‚Äôre particularly interested in capturing details.\nA larger perplexity (e.g., 30‚Äì50 or even higher) encourages t-SNE to preserve global structure, considering more far away points as part of the neighborhood. This can be useful for larger datasets or when you want to capture broader relationships between clusters.\nFinding the right perplexity often involves some experimentation. If it‚Äôs too small, t-SNE might overfit to the local structure and fail to reveal larger patterns in the data. If it‚Äôs too large, you might lose relationships between nearby data points. t-SNE is retively robust to different perplexity values, so changing this parameter slightly usually won‚Äôt result in big changes, but it can make the difference between a good visualization and a great one.\nA rule of thumb is to set the perplexity such that \\(3 \\times \\text{perplexity} &lt; n-1\\), where \\(n\\) is the number of data points. Testing several values across this range will help you find the best fit for your data.\n\n2.2.1 See the impact of perplexity\nI did not tell you before (you might have realized it from the code though) but there are substructures in the original data. We are now going to plot them again.\n\n\nCode\nlibrary(dplyr)\nlibrary(plotly)  # For interactive 3D plots\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~subcluster,#,symbol=~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\n\n\n\n\n\n\nYou should now see that within each cluster we have subclusters. Let‚Äôs see if our original t-SNE was successful in separating them.\n\n\nCode\ntsne_plot2&lt;-plot_ly(\n  data, x = ~tsne_x_30, y = ~tsne_y_30, color = ~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n\npca_plot2 &lt;- plot_ly(\n  data, x = ~pca_x, y = ~pca_y, color = ~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n  \nsubplot(pca_plot2%&gt;%layout(showlegend = FALSE), tsne_plot2%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"PCA\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"t-SNE\", showarrow = F, xref='paper', yref='paper')))\n\n\n\n\n\n\nCompared to PCA we have actually done a good job. Most clusters seems to be well separated. But what we want to do is to change perplexity to see if we can make this better.\n\n\nCode\nset.seed(123)\ntsne_results_20 &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),perplexity = 20\n)\ndata$tsne_x_20 &lt;- tsne_results_20$Y[, 1]\ndata$tsne_y_20 &lt;- tsne_results_20$Y[, 2]\n\ntsne_plot2&lt;-plot_ly(\n  data, x = ~tsne_x_30, y = ~tsne_y_30, color = ~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n\ntsne_plot20&lt;-plot_ly(\n  data, x = ~tsne_x_20, y = ~tsne_y_20, color = ~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n  \nsubplot(tsne_plot2%&gt;%layout(showlegend = FALSE), tsne_plot20%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"t-SNE (30)\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"t-SNE (20)\", showarrow = F, xref='paper', yref='paper')))\n\n\n\n\n\n\nI have now decreased perplexity to 20. What we can see is that at 30 perplexity, t-SNE is accounting for a larger neighborhood of points when embedding the data. This results in clearer separation between clusters, with well-defined compact clusters. At 20 however, each cluster also appears distinctly separated in space, maintaining a reasonable balance between local and global structure. The substructures within the clusters are more prominent, with some separation and internal grouping within each main cluster, suggesting that t-SNE is better at capturing smaller-scale local variations with a lower perplexity.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#output-of-rtsne",
    "href": "tsne.html#output-of-rtsne",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "2.3 Output of Rtsne",
    "text": "2.3 Output of Rtsne\nThis is relatively streightforward. The most important output of this function is \\(Y\\). You can extract using tsne_results_20$Y. This is a matrix that has the exact same number of rows as your orignal data and the number of columns is the same as dims parameter. So basically it is your data transformed into the lower dimention of size dims. In our case, it was 2.\nThe rest of the output is basically either information about the optimization process or the summary of the input paramters. We are going to ignore them for now and will get back to it later if needed.\n\n\n\n\n\n\nDo it yourself\n\n\n\nPlay around with these parameters (perplexity, theta, and max_iter) to see if you can get better results",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#between-cluster-distances-and-densities-might-not-be-accurate",
    "href": "tsne.html#between-cluster-distances-and-densities-might-not-be-accurate",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "3.1 between cluster distances and densities might not be accurate!",
    "text": "3.1 between cluster distances and densities might not be accurate!\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of points per subcluster\nn &lt;- 25\n\n# Manually define main cluster centers (global structure)\nmain_cluster_centers &lt;- data.frame(\n  x = c(0, 10, 15, 35),\n  y = c(0, 10, 15, 35),\n  z = c(0, 10, 15, 35),\n  cluster = factor(1:4)\n)\n\n# Manually define subcluster offsets relative to each main cluster\n# These small offsets will determine subcluster locations within each main cluster\nsubcluster_offsets &lt;- data.frame(\n  x_offset = c(-0.25, 0.25, -0.25, 0.25),\n  y_offset = c(-0.25, -0.25, 0.25, 0.25),\n  z_offset = c(0.25, -0.25, -0.25, 0.25),\n  subcluster = factor(1:4)\n)\n\n# Initialize an empty data frame to hold all data\ndata &lt;- data.frame()\n\n# Generate data for each main cluster with subclusters\n\nfor (i in 1:nrow(main_cluster_centers)) {\n  for (j in 1:nrow(subcluster_offsets)) {\n    # Calculate subcluster center by adding the offset to the main cluster center\n    subcluster_center &lt;- main_cluster_centers[i, 1:3] + subcluster_offsets[j, 1:3]\n    \n    # Generate points for each subcluster with a small spread (to form local clusters)\n    subcluster_data &lt;- data.frame(\n      gene1 = rnorm(n, mean = subcluster_center$x, sd = 0.25*i*i),  # Small spread within subclusters\n      gene2 = rnorm(n, mean = subcluster_center$y, sd = 0.25*i*i),\n      gene3 = rnorm(n, mean = subcluster_center$z, sd = 0.25*i*i),\n      cluster = main_cluster_centers$cluster[i],\n      subcluster = subcluster_offsets$subcluster[j]\n    )\n    \n    # Add generated subcluster data to the main data frame\n    data &lt;- rbind(data, subcluster_data)\n  }\n}\n\n\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\n\n\n\n\n\n\nWhat i have done now is to give different density to each larger cluster and also different distances between the clusters. We can do t-SNE and compare the results to PCA.\n\n\nCode\nset.seed(123)\ntsne_results_30 &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")])\n)\n\n\ndata$tsne_x_30 &lt;- tsne_results_30$Y[, 1]\ndata$tsne_y_30 &lt;- tsne_results_30$Y[, 2]\n\n\ntsne_plot &lt;- plot_ly(\n  data, x = ~tsne_x_30, y = ~tsne_y_30, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n\n\npca_results &lt;- prcomp(data[, c(\"gene1\", \"gene2\", \"gene3\")], scale. = FALSE)\ndata$pca_x &lt;- pca_results$x[, 1]\ndata$pca_y &lt;- pca_results$x[, 2]\n\n\npca_plot &lt;- plot_ly(\n  data, x = ~pca_x, y = ~pca_y, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\n  \nsubplot(pca_plot%&gt;%layout(showlegend = FALSE), tsne_plot%&gt;%layout(showlegend = FALSE),\n        titleX = T,titleY = T,margin = 0.2)%&gt;% layout(annotations = list(\n list(x = 0.13 , y = 1.035, text = \"PCA\", showarrow = F, xref='paper', yref='paper'),\n  list(x = 0.85 , y = 1.035, text = \"t-SNE\", showarrow = F, xref='paper', yref='paper'))\n)\n\n\n\n\n\n\nPCA did a great job in preserving the relative distances between clusters, reflecting the original distribution of the data. The density of the clusters is also proportional to the original data, with tighter clusters remaining dense and more spread-out clusters maintaining their looser arrangement. In contrast, t-SNE naturally equalizes the densities of the clusters, making them appear more uniform. This is not an artifact, but a deliberate feature of t-SNE, where dense clusters are expanded and sparse ones contracted. As a result, the distances between clusters in the t-SNE plot may appear more similar, and the clusters themselves more evenly distributed, which can distort the true global relationships.\n\n\n\n\n\n\nClustering on t-SNE results\n\n\n\nAvoid doing density and distance based clustering on t-SNE space. In vast majority of the cases the distances and density don‚Äôt have much meaning!\n\n\nThe last thing i want to mention here is that having loo little perplexity might cause misinterpretation of noise as clusters. In our previous example we know that we have four major clusters in our data but look what happens if we decrease the perplexity too much.\n\n\nCode\ntnse_data_frames&lt;-c()\nfor(it in c(2:10))\n{\n  \n  set.seed(123)\ntsne_results_it &lt;- Rtsne(\n  as.matrix(data[, c(\"gene1\", \"gene2\", \"gene3\")]),perplexity = it,\n)\n  tnse_data_frames&lt;-rbind(tnse_data_frames,data.frame(tsne_results_it$Y,data[,c(4,5)],perplexity=it))\n}\n\n\ntsne_plot2&lt;-plot_ly(\n  tnse_data_frames, x = ~X1, y = ~X2, color = ~subcluster,frame=~perplexity,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"\"\n\n  )\n\ntsne_plot2\n\n\n\n\n\n\nSmall clusters start to appear, which don‚Äôt exist in the original data. Unfortunately, this is largely dependent on the perplexity parameter. Lower perplexities overemphasize local structures, making t-SNE susceptible to identifying random noise as distinct clusters. This can lead to false interpretations, especially when exploring new datasets where the true structure is not well known. Therefore, it‚Äôs important to experiment with different perplexity values and validate findings with complementary methods.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#gaussian-or-normal-distribution",
    "href": "tsne.html#gaussian-or-normal-distribution",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.1 Gaussian (or normal) distribution",
    "text": "5.1 Gaussian (or normal) distribution\nOne issue with the Euclidean distance is that it does not inherently prioritize nearby points over distant ones. Every pair of points, whether close or far apart, is treated equally when computing the overall data structure. While this is not necessarily a flaw, our goal here is to capture the local structure rather than the global one. To address this, we can explore modifications to the Euclidean distance to make it slightly more local, such as incorporating a scaling factor like the standard deviation, which emphasizes nearby points without completely discarding global relationships.\nMore specifically we could divide the Euclidean distance between two points by the standard deviation of the distances from one of the points to its neighbors. This adjustment would make distances in denser regions appear relatively larger, prioritizing local structures without completely ignoring the global one.\nSo basically we do something like\n\\[\n\\tilde{D}_{ij} = \\frac{D_{ij}}{\\sigma_i}\n\\]\nWhere:\n\n\\(D_{ij}\\) is the Euclidean distance between points \\(i\\) and \\(j\\),\n\\(\\sigma_i\\) is the standard deviation of the distances from point \\(i\\) to its neighbors.\n\nLet‚Äôs see how it looks like\n\n\nCode\n# Function to compute local standard deviation based on k-nearest neighbors\ncompute_sigma &lt;- function(dist_matrix, k_neighbors) {\n  n &lt;- nrow(dist_matrix)\n  sigma &lt;- numeric(n)\n  \n  for (i in 1:n) {\n    # Get distances from point i to all other points\n    dists &lt;- dist_matrix[i, ]\n    \n    # Sort the distances and select k nearest neighbors (excluding self)\n    nearest_neighbors &lt;- sort(dists, decreasing = FALSE)[2:(k_neighbors + 1)]\n    \n    # Compute standard deviation of distances to nearest neighbors\n    sigma[i] &lt;- sd(nearest_neighbors)\n    sigma[i] &lt;- max(sigma[i] , 1e-4)\n    # Avoid sigma being zero\n   # sigma &lt;- sigma+\n  }\n  \n  return(sigma)\n}\n\n# Compute sigma for original distances D (high-dimensional space)\nsigma_D &lt;- compute_sigma(D, 10)\n\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n       P[i, j] &lt;- (D[i, j] /( sigma_D[i]))\n    }\n  }\n}\nsorted_indices &lt;- order(D[1, ])\nsorted_D &lt;- D[1, sorted_indices]\nsorted_P &lt;- P[1, sorted_indices]\n\n\n\n# Plot  line with sorted x and y values\nplot_ly(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines') %&gt;%\n  layout(title = \"Line plot of D vs (D/sigma) 10 NN (Smoothed)\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"D/sigma\"))\n\n\n\n\n\n\nWe calculated the sigma based on 10 nearest neighbors for each point and divide the original distance by each sigma. I‚Äôm showing the distance of the point 1 to all other points here. Admittedly, it did not do much. We are just scaling the distances. Obviously it will have small impact on the lower dimension but still it is a global measure. What we are after is to decay the distances as they get larger and we want to do it smoothly. One option would be to apply exponential function here on the negative distances.\n\\[\nK(x, x') = \\exp\\left( -\\frac{\\|x - x'\\|}{\\sigma} \\right)\n\\]\n\n\nCode\n# Compute sigma for original distances D (high-dimensional space)\nsigma_D &lt;- compute_sigma(D, 10)\n\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n       P[i, j] &lt;- exp(-D[i, j] /( sigma_D[i]))\n    }\n  }\n}\nsorted_indices &lt;- order(D[1, ])\nsorted_D &lt;- D[1, sorted_indices]\nsorted_P &lt;- P[1, sorted_indices]\n\n\n\n# Plot  line with sorted x and y values\nplot_ly(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines') %&gt;%\n  layout(title = \"Line plot of D vs exp(-D/sigma) 10 NN (Smoothed)\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"exp(-D/sigma)\"))\n\n\n\n\n\n\nSo basically, \\(\\exp\\left(-\\frac{D[i,j]}{\\sigma_D[i]}\\right)\\) is converting the Euclidean distance into a similarity measure. The matrix \\(D[i,j]\\) represents the Euclidean distance between points \\(i\\) and \\(j\\). A larger value of \\(D[i,j]\\) means the points are farther apart, while a smaller value indicates they are closer. The exponential function \\(\\exp(-x)\\) decreases rapidly as \\(x\\) increases, meaning that larger distances result in smaller similarities. When the distance \\(D[i,j]\\) is small, \\(\\exp(-D[i,j])\\) is close to 1, indicating high similarity. When the distance is large, the exponential value approaches 0, indicating low similarity. As said before dividing by \\(\\sigma_D[i]\\) allows us to control the ‚Äúspread‚Äù or sensitivity of the similarity function. If \\(\\sigma_D[i]\\) is small, the function decays quickly, meaning only very close points will be considered similar. If \\(\\sigma_D[i]\\) is large, the decay is slower, allowing points that are further away to still be considered somewhat similar. Let‚Äôs see the effect of \\(\\sigma_D[i]\\) based on the number of nearest neighbors.\n\n\nCode\n# Number of neighbors to test\nneighbor_values &lt;- c(5, 10, 20, 30,50)\n\n# Initialize an empty plotly object\np &lt;- plot_ly()\n\n# Loop over different neighbor values\nfor (neighbors in neighbor_values) {\n  \n  # Compute sigma for the current number of neighbors\n  sigma_D &lt;- compute_sigma(D, neighbors)\n  \n  # Compute P matrix based on sigma_D\n  P &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        P[i, j] &lt;- exp(-D[i, j] / sigma_D[i])\n      }\n    }\n  }\n  \n  # Sort the first row of D and P for plotting\n  sorted_indices &lt;- order(D[1, ])\n  sorted_D &lt;- D[1, sorted_indices]\n  sorted_P &lt;- P[1, sorted_indices]\n  \n  # Add a trace to the plot for the current number of neighbors\n  p &lt;- p %&gt;%\n    add_trace(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines',\n              name = paste0(neighbors, \" NN\"))\n}\n\n# Customize the layout\np &lt;- p %&gt;%\n  layout(title = \"Line plot of D vs exp(-D/sigma) for different neighbors\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"exp(-D/sigma)\"))\n\np\n\n\n\n\n\n\nAs we increase the number of neighbors, the standard deviation will increase which allows the distant points to be considered similar. The formula we just presented (\\(\\exp\\left(-\\frac{D[i,j]}{\\sigma_D[i]}\\right)\\)) looks very similar to the well-known Gaussian (or normal) distribution function. In statistics, the Gaussian distribution is used to describe data that cluster around a mean, with the probability of a value decreasing as it moves further from the center.\nThe Gaussian (or normal) distribution‚Äôs probability density function for a single-dimensional random variable \\(x\\) is given by:\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n\\]\nNotice the key similarities:\n\nThe negative exponential term,\nThe squared difference \\((x - \\mu)^2\\),\nThe factor \\(2\\sigma^2\\) in the denominator.\n\nIn the Gaussian distribution, the distance between a value and the mean is squared, meaning that both positive and negative deviations contribute equally to the distribution. Squaring ensures that large deviations from the mean (or between points, in our case) are penalized more heavily. This same principle is applied when transforming distances into similarities: squaring the distance emphasizes larger distances, making the similarity decrease more sharply as the distance gets bigger.\nWe also have the factor \\(2\\sigma^2\\) in the Gaussian function. This normalizes the spread of the data based on the variance. In our case, this factor ensures that the similarity measure accounts for the relative spread of the distances. So, \\(\\sigma^2\\) controls how quickly the similarity decays as the distance increases. A small \\(\\sigma^2\\) causes the similarity to drop off quickly, meaning only very close points are considered similar. Conversely, a larger \\(\\sigma^2\\) results in a slower decay, making points further apart still somewhat similar. The factor of 2 in \\(2\\sigma^2\\) ensures that this behavior is aligned with the properties of the Gaussian function, which decays at a rate proportional to the variance.\nWe can now transform our similarity function into the more commonly recognized Gaussian kernel by incorporating the squared Euclidean distance and the factor \\(2\\sigma^2\\): \\[\n      K(i,j) = \\exp\\left( -\\frac{D[i,j]^2}{2\\sigma_D[i]^2} \\right)\n\\]\nThis will look like:\n\n\nCode\n# Initialize an empty plotly object\n\n\nsigma_D &lt;- compute_sigma(D, 10)\n\n   # Compute P matrix based on sigma_D\n  P &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      }\n    }\n  }\n  \n  # Sort the first row of D and P for plotting\n  sorted_indices &lt;- order(D[1, ])\n  sorted_D &lt;- D[1, sorted_indices]\n  sorted_P &lt;- P[1, sorted_indices]\n  \n\n  \n# Customize the layout\nplot_ly(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines')%&gt;%\n  layout(title = \"Line plot of D vs exp(-D^2/(2*sigma^2)) for different neighbors\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"exp(-D^2/(2*sigma^2))\"))\n\n\n\n\n\n\nSo in summary, we calculate the standard deviation of distances for each individual sample based on its neighborhood. This standard deviation, \\(\\sigma_D[i]\\), reflects how spread out the distances are around that specific point, giving us a local measure of distance variability for each sample.\nThen, we use the Gaussian kernel to measure the similarity between points. The Gaussian kernel transforms the Euclidean distance between two points into a similarity score that decays exponentially with increasing distance. The similarity between two points is determined based on how close they are relative to the local standard deviation of distances.\nThere is one more step to do here. The distances for each point are now based on its own standard deviation (SD), which means they are likely on different scales because each point has a different SD reflecting its local neighborhood density. This variability makes it challenging to compare distances directly across different points since each distance is relative to its local scale. It would be nice to have distances that have the same meaning and statistical propery throughout the dataset. To address this, we convert the distances into probabilities, which normalize the values to a common scale and ensure that they are comparable. The conversion to probabilities for each point (i) is done as follows:\n\\[\np_{j|i} = \\frac{\\exp\\left(-\\frac{D_{ij}^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{D_{ik}^2}{2\\sigma_i^2}\\right)}\n\\]\nWhere:\n\n\\(D_{ij}\\) is the distance between points \\(i\\) and \\(j\\),\n\\(\\sigma_i\\) is the standard deviation of distances for point \\(i\\),\n\\(p_{j|i}\\) is the probability of selecting point \\(j\\) given point \\(i\\).\n\nThe numerator \\(\\exp\\left(-\\frac{D_{ij}^2}{2\\sigma_i^2}\\right)\\) ensures that smaller distances (closer neighbors) are given higher probabilities, while larger distances contribute less. The denominator \\(\\sum_{k \\neq i} \\exp\\left(-\\frac{D_{ik}^2}{2\\sigma_i^2}\\right)\\) ensures that all probabilities for a given point (i) sum to 1, bringing all points onto a comparable scale.\nThe entire equation computes the probability \\(p_{j|i}\\), which represents the likelihood of selecting point \\(j\\), given point \\(i\\), based on how close point \\(j\\) is to point \\(i\\). This probability is higher for points that are close to \\(i\\) and lower for points that are far from \\(i\\).\n\n\nCode\n# Initialize an empty plotly object\n\n\nsigma_D &lt;- compute_sigma(D, 10)\n\n   # Compute P matrix based on sigma_D\n  P &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n        \n      }\n    }\n    P[i, ] &lt;- P[i, ] / sum(P[i, ])\n  }\n  \n  # Sort the first row of D and P for plotting\n  sorted_indices &lt;- order(D[1, ])\n  sorted_D &lt;- D[1, sorted_indices]\n  sorted_P &lt;- P[1, sorted_indices]\n  \n\n  \n# Customize the layout\nplot_ly(x = sorted_D[-1], y = sorted_P[-1], type = 'scatter', mode = 'lines')%&gt;%\n  layout(title = \"Line plot of D vs exp(-D^2/(2*sigma^2)) for different neighbors\",\n         xaxis = list(title = \"Original distance\"),\n         yaxis = list(title = \"exp(-D^2/(2*sigma^2))\"))\n\n\n\n\n\n\nYou can play around with the number of nearest neighbors to get a feeling of how the number of neighbors affect the distances.\n\n\nCode\ndata_2d&lt;-data[,1:2]\n\n\n\n# Select point 1\npoint_1 &lt;- data_2d[1, ]\n\n# Create a grid of points around point 1 for the Gaussian kernel\nx_seq &lt;- seq(min(data_2d$gene1) - 1, max(data_2d$gene1) + 1, length.out = 100)\ny_seq &lt;- seq(min(data_2d$gene2) - 1, max(data_2d$gene2) + 1, length.out = 100)\n\n# Function to compute sigma (standard deviation of distances for the first point)\ncompute_sigma &lt;- function(D, nn) {\n  sigma &lt;- numeric(1)\n  dist_to_point1 &lt;- D[1, ]\n  nearest_neighbors &lt;- sort(dist_to_point1, decreasing = FALSE)[2:(nn + 1)]\n  sigma[1] &lt;- sd(nearest_neighbors)\n  return(sigma)\n}\n\n# Function to create Gaussian kernel based on nn\ncreate_gaussian_kernel &lt;- function(nn) {\n\n  \n  # Get sigma for point 1\n  sigma &lt;- compute_sigma(D, nn)\n  \n  # Compute probabilities for all data points (Gaussian kernel)\n  gaussian_probabilities &lt;- numeric(nrow(data_2d))\n  for (i in 1:nrow(data_2d)) {\n    dist_sq &lt;- (data_2d$gene1[i] - point_1$gene1)^2 + (data_2d$gene2[i] - point_1$gene2)^2\n    gaussian_probabilities[i] &lt;- exp(-dist_sq / (2 * sigma^2))\n  }\n  \n  # Normalize the probabilities (sum to 1)\n  gaussian_probabilities &lt;- gaussian_probabilities / sum(gaussian_probabilities)\n  \n  # Create kernel values for contour plot\n  gaussian_kernel &lt;- matrix(0, nrow = length(x_seq), ncol = length(y_seq))\n  for (i in 1:length(x_seq)) {\n    for (j in 1:length(y_seq)) {\n      dist_sq &lt;- (x_seq[i] - point_1$gene1)^2 + (y_seq[j] - point_1$gene2)^2\n      gaussian_kernel[i, j] &lt;- exp(-dist_sq / (2 * sigma^2))\n    }\n  }\n  \n  return(list(kernel = as.vector(gaussian_kernel), probabilities = gaussian_probabilities))\n}\n\n# Create steps for slider (each step corresponds to a nn value)\nnn_values &lt;- seq(2, 50, by = 1)\nsteps &lt;- list()\nfor (i in 1:length(nn_values)) {\n  nn &lt;- nn_values[i]\n  result &lt;- create_gaussian_kernel(nn)\n  \n  steps[[i]] &lt;- list(\n    args = list(\n      list(z = list(result$kernel), \"marker.color\" = list(result$probabilities))\n    ),\n    label = paste0(\"nn: \", nn),\n    method = \"restyle\"\n  )\n}\n\n# Initial nn and kernel\ninitial_nn &lt;- 10\ninitial_result &lt;- create_gaussian_kernel(initial_nn)\n\n# Convert grid and kernel matrix into a data frame for plotting\nkernel_df &lt;- expand.grid(x = x_seq, y = y_seq)\nkernel_df$z &lt;- initial_result$kernel\n\n# Plot 2D data points and Gaussian kernel around point 1\nfig &lt;- plot_ly() %&gt;%\n  add_trace(data = data_2d, x = ~gene1, y = ~gene2, type = 'scatter', mode = 'markers',\n            marker = list(size = 10, color = initial_result$probabilities, showscale = TRUE),\n            name = 'Data Points') %&gt;%\n  add_trace(x = kernel_df$x, y = kernel_df$y, z = kernel_df$z, type = 'contour',\n            contours = list(showlabels = TRUE), name = 'Gaussian Kernel', showscale = F) %&gt;%\n  layout(title = \"Interactive Gaussian Kernel\",\n         sliders = list(\n           list(\n             active = 8,  # Set the active index (for nn = 10 in our steps)\n             currentvalue = list(prefix = \"\"),\n             pad = list(t = 60),\n             steps = steps\n           )\n         ))\n\n# Show the interactive plot with the slider\nfig\n\n\n\n\n\n\nAs you adjust the number of nearest neighbors (nn) using the slider, you‚Äôll notice a significant change in the probability assigned to the points, even though their positions remain fixed. This is because the Gaussian kernel adapts based on the local neighborhood of point_1, and its standard deviation (œÉ) increases with more neighbors included.\n\nSmaller Number of Neighbors (Low nn):\n\nWhen the number of nearest neighbors is small, the Gaussian kernel is tighter around point_1. This means the local neighborhood is defined narrowly, and only points that are very close to point_1 have high probabilities.\nAs a result, the probability for points near point_1 is high because these points are strongly influenced by the local structure around point_1. In the contour plot, this is reflected by the smaller, more concentrated yellow region, indicating that fewer points have high probabilities.\n\nLarger Number of Neighbors (High nn):\n\nAs you increase the number of nearest neighbors, the Gaussian kernel widens, taking more distant points into account. This effectively means the local neighborhood grows, and the probability is distributed across a larger area.\nFor the same point that previously had a high probability (when the neighborhood was small), its probability will now decrease because the influence of point_1 is spread out over a larger region. More points are included in the neighborhood, but the individual contribution from each point is reduced.\nThis is visually represented by the expansion of the yellow area in the contour plot, where more points now have non-negligible probabilities, but the magnitude of the probabilities for any specific point (including those close to point_1) is lower.\n\n\nThe probability assigned to an identical point is different under different numbers of nearest neighbors because the Gaussian kernel adapts based on the local neighborhood size. With fewer neighbors, the kernel is more focused, leading to higher probabilities for nearby points. As the number of neighbors increases, the kernel spreads out, and the probability for each individual point decreases, even though the point‚Äôs location remains unchanged.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#gaussian-kernel-for-low-dimentions",
    "href": "tsne.html#gaussian-kernel-for-low-dimentions",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.2 Gaussian kernel for low dimentions",
    "text": "5.2 Gaussian kernel for low dimentions\nNow we need to think about he low dimentional space. We can use the same formula:\n\\[\nq_{j|i} = \\frac{\\exp\\left(-Dz_{ij}^2\\right)}{\\sum_{k \\neq i} \\exp\\left(-Dz_{ik}^2\\right)}\n\\] Here \\(Dz\\) is the distances in the lower dimensional space (i will come to the point where we define the lower dimension). Here the normalization is over all data points.\nIt is important that once the data is projected into the low-dimensional space, the idea is to preserve local neighborhoods as faithfully as possible. In low-dimensional space, we are no longer working with highly varying densities like you have in high-dimensional space. Instead, points that are similar should already be close together in this space. Therfore, we don‚Äôt need dynamic standard deviation (SD) estimation. The aim here is to presernve the distance under this specific distribution.\n\n5.2.1 Making things symmetric\nSo far, we have measured how similar data points are in both high-dimensional and low-dimensional spaces. To do this, we computed conditional probabilities for each point, but these are inherently asymmetric. Basicaly, the conditional probability \\(p_{j|i}\\) is calculated based on the similarity between point \\(x_i\\) and point \\(x_j\\) in the high-dimensional space. This is done using a Gaussian kernel:\n\\[\np_{j|i} = \\frac{\\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)}\n\\]\nHowever, these conditional probabilities are not symmetric: \\(p_{j|i}\\) is not equal to \\(p_{i|j}\\) because they are computed separately for \\(x_i\\) and \\(x_j\\), with different reference points and sigma.\nFor example, if \\(x_i\\) is an outlier far from the rest of the data, \\(p_{j|i}\\) will be very small for all \\(j\\), but \\(p_{i|j}\\) might still be large if \\(x_j\\) is close to other points.\nTo deal with this asymmetry, we compute the joint probability \\(p_{ij}\\), which is symmetric:\n\\[\np_{ij} = \\frac{p_{i|j} + p_{j|i}}{2n}\n\\]\nThis ensures that \\(p_{ij} = p_{ji}\\), meaning the similarity between \\(x_i\\) and \\(x_j\\) is the same, regardless of the direction of comparison. Without symmetrization, outliers or points with unusual distances could have a disproportionately small or large influence on the our lower dimensions. This also ensure that sum of all probabilities is 1. We again need to calculate the gradient to update the lower dimension.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#similarity-between-higher-and-lower-dimentions",
    "href": "tsne.html#similarity-between-higher-and-lower-dimentions",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.3 Similarity between higher and lower dimentions",
    "text": "5.3 Similarity between higher and lower dimentions\nIn order to say how different our higher and lower dimentions are we are going to use KL (Kullback-Leibler) divergence. This is a measure of how one probability distribution is different from another. We have from before\n\n\\(P\\) represents similarities (or probabilities) in high-dimensional space.\n\\(Q\\) represents similarities in the lower-dimensional space (our reduced embedding).\n\nThe goal is to minimize the difference between these two distributions so that the lower-dimensional embedding represents the data as faithfully as possible.\nMathematically, the KL divergence between the high-dimensional and low-dimensional distributions is expressed as:\n\\[\nC = \\sum_i KL(P_i || Q_i) = \\sum_i \\sum_j p_{j|i} \\log \\frac{p_{j|i}}{q_{j|i}}\n\\]\n\n\\(p_{j|i}\\): Probability that point \\(j\\) is similar to point \\(i\\) in high-dimensional space.\n\\(q_{j|i}\\): Probability that point \\(j\\) is similar to point \\(i\\) in low-dimensional space.\n\nThis basically tells us how surprised we are by the difference between the two distributions.\nSimilar to before, we need to calculate the gradient to update the lower dimension. The gradient of the KL divergence with respect to \\(y_i\\) is:\n\\[\n   \\frac{\\delta C}{\\delta y_i} = 4 \\sum_j \\left( p_{j|i} - q_{j|i} \\right) (y_i - y_j)\n\\] Here \\(p_{j|i} - q_{j|i}\\) is showing the difference how much the high-dimensional similarity \\(p_{j|i}\\) differs from the low-dimensional similarity \\(q_{j|i}\\). If they differ significantly, the gradient will be larger. \\(y_i - y_j\\) is the distance between the points \\(y_i\\) and \\(y_j\\) in the low-dimensional space. It shows how far the two points are in the lower dimentions, and moving \\(y_i\\) toward \\(y_j\\) will help reduce the divergence.\n\n\n\n\n\n\nCost function\n\n\n\nI‚Äôm not going to include the cost function (and tolerance in the code) to speed things up. Maybe you want to do that and check for tolerance?\n\n\nAfter calculating the gradient, we can use it to update the position of \\(y_i\\) in the lower-dimensional space. The update rule looks like this:\n\\[\n   y_i \\leftarrow y_i - \\eta \\frac{\\delta C}{\\delta y_i}\n\\]\nWhere \\(\\eta\\) is the learning rate controlling the step size.\nLet‚Äôs try to implement this\n\n\nCode\n# calculate orignal distances\nD&lt;-as.matrix(dist(data[,1:3]))\n# Function to compute local standard deviation based on k-nearest neighbors\ncompute_sigma &lt;- function(dist_matrix, k_neighbors) {\n  n &lt;- nrow(dist_matrix)\n  sigma &lt;- numeric(n)\n  \n  for (i in 1:n) {\n    # Get distances from point i to all other points\n    dists &lt;- dist_matrix[i, ]\n    \n    # Sort the distances and select k nearest neighbors (excluding self)\n    nearest_neighbors &lt;- sort(dists, decreasing = FALSE)[2:(k_neighbors + 1)]\n    \n    # Compute standard deviation of distances to nearest neighbors\n    sigma[i] &lt;- sd(nearest_neighbors)\n    sigma[i] &lt;- max(sigma[i] , 1e-4)\n    # Avoid sigma being zero\n    # sigma &lt;- sigma+\n  }\n  \n  return(sigma)\n}\n\n# Compute sigma for original distances D (high-dimensional space)\nsigma_D &lt;- compute_sigma(D, 10)\n\n# Compute P matrix based on sigma_D\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n      P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      \n    }\n    \n  }\n   P[i, ] &lt;- P[i, ] / sum(P[i,])\n}\nP = (P + t(P))/(2*nrow(D))\n\n# Create random low dimention\nndim &lt;- 2\nset.seed(12345)\nY &lt;- matrix(rnorm(nrow(D) * ndim, sd = 1, mean = 0), nrow = nrow(D), ncol = ndim)\ncolnames(Y) &lt;- c(\"dim1\", \"dim2\")\n\n# define data for plot\n\nplot_frames_sne&lt;-c()\nfor (iter in 1:1000) {\n  Dy &lt;- as.matrix(dist(Y))\n  \n  Q &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        # Calculate conditional probability q_j|i using the same Gaussian kernel\n        Q[i, j] &lt;- exp(-(Dy[i, j])^2)\n        \n      }\n    }\n  }\n\nQ &lt;- Q / sum(Q)\n  grad &lt;- matrix(0, nrow = nrow(D), ncol = ndim)\n  # For each point i and j, compute the gradient of KL divergence\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n         diff_y &lt;- Y[i, ] - Y[j, ]\n        scaling_factor &lt;- (P[i, j] - Q[i, j])\n        grad[i, ] &lt;- grad[i, ] + scaling_factor * diff_y\n        \n        \n      }\n    }\n     grad[i, ]&lt;-4* grad[i, ]\n  }\n  Y &lt;- Y - 5 * (grad)\n\n  if (iter %% 50 == 0) {\n    plot_frames_sne&lt;-rbind(plot_frames_sne,data.frame(Y, cluster = data$cluster, subcluster = data$subcluster,iteration=iter))\n\n  }\n  \n}\n\n# \nplot_ly(\n  as.data.frame(plot_frames_sne), x = ~dim1, y = ~dim2,frame=~iteration,color=~cluster,symbol=~subcluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  mode = \"markers\",size = 5\n)  %&gt;%\n  layout(showlegend = FALSE,\n    title = \"SNE 2D Data with Clusters (animation)\"\n  )\n\n\n\n\n\n\nClick on Play to see how this algorithm maps the higher dimention to the lower one by starting from random points in the lower dimention and progressively change them so they get more and more similar with respect to thier distance to the higher dimension. In fact that process that we saw is symmetric SNE algortihm. The results are promising but we still have work to do to make it better!",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#perplexity-effective-number-of-neighbors",
    "href": "tsne.html#perplexity-effective-number-of-neighbors",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.4 Perplexity (Effective number of neighbors)",
    "text": "5.4 Perplexity (Effective number of neighbors)\nSo far we have been using standard deviation that has been directly calculated based on the data points‚Äôs nearest neighbors. This however has a potential problem. Using this kernel, we convert distances between points in high-dimensional space into probabilities that represent similarities between points. For two points \\(i\\) and \\(j\\), the probability that \\(j\\) is a neighbor of \\(i\\) is computed using a Gaussian distribution:\n\\[\np_{ij} = \\frac{\\exp \\left( - \\frac{d_{ij}^2}{2 \\sigma_i^2} \\right)}{\\sum_{k \\neq i} \\exp \\left( - \\frac{d_{ik}^2}{2 \\sigma_i^2} \\right)}\n\\]\nAgain here, \\(\\sigma_i\\) is the standard deviation used for point \\(i\\), and \\(d_{ij}\\) is the distance between points \\(i\\) and \\(j\\). The choice of \\(\\sigma_i\\) determines how much weight we assign to close vs.¬†distant neighbors when computing these probabilities.\nSo far we have used the real standard deviation (based on actual distances to neighbors in high-dimensional space). The problem is in dense regions, where many points are packed closely together, the real standard deviation \\(\\sigma_i\\) will be small because the average distance to neighbors is small. A small \\(\\sigma_i\\) makes the Gaussian distribution very narrow. his means that only the very closest neighbors will have significant probabilities, while slightly more distant neighbors (even if they aren‚Äôt that far away) will have almost zero probability. This causes the probabilities $ p_{ij}$ to focus too much on the nearest neighbors, potentially ignoring important local structure beyond the immediate vicinity. With certian number neighbors selected, points in dense clusters might appear farther apart than they should in the lower dimensions. Local clusters that should be tight could become overly spread out.\nIn sparse regions, where points are more spread out, the real standard deviation \\(\\sigma_i\\) will be larger because the average distance to neighbors is larger. A large \\(\\sigma_i\\) makes the Gaussian distribution wider. This means that the probabilities will be more spread out across neighbors, and distant neighbors will get significant probabilities, even though they are far away. In this case, the algorithm might overestimate the importance of distant neighbors, leading to blurring of the local structure in these regions.\nLet‚Äôs have a look at an example:\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Number of points per subcluster\nn &lt;- c(8,6,4,2)\n\n# Manually define main cluster centers (global structure)\nmain_cluster_centers &lt;- data.frame(\n  x = c(0, 5, 10, 15),\n  y = c(0, 5, 10, 15),\n  z = c(0, 5, 10, 15),\n  cluster = factor(1:4)\n)\n\n# Manually define subcluster offsets relative to each main cluster\n# These small offsets will determine subcluster locations within each main cluster\nsubcluster_offsets &lt;- data.frame(\n  x_offset = c(-0.25, 0.25, -0.25, 0.25),\n  y_offset = c(-0.25, -0.25, 0.25, 0.25),\n  z_offset = c(0.25, -0.25, -0.25, 0.25),\n  subcluster = factor(1:4)\n)\n\n# Initialize an empty data frame to hold all data\ndata &lt;- data.frame()\n\n# Generate data for each main cluster with subclusters\n\nfor (i in 1:nrow(main_cluster_centers)) {\n  for (j in 1:nrow(subcluster_offsets)) {\n    # Calculate subcluster center by adding the offset to the main cluster center\n    subcluster_center &lt;- main_cluster_centers[i, 1:3] + subcluster_offsets[j, 1:3]\n    \n    # Generate points for each subcluster with a small spread (to form local clusters)\n    subcluster_data &lt;- data.frame(\n      gene1 = rnorm(n[i], mean = subcluster_center$x, sd = 2),  # Small spread within subclusters\n      gene2 = rnorm(n[i], mean = subcluster_center$y, sd = 2),\n      gene3 = rnorm(n[i], mean = subcluster_center$z, sd = 2),\n      cluster = main_cluster_centers$cluster[i],\n      subcluster = subcluster_offsets$subcluster[j]\n    )\n    \n    # Add generated subcluster data to the main data frame\n    data &lt;- rbind(data, subcluster_data)\n  }\n}\n\n\nplot_ly(\n  data, x = ~gene1, y = ~gene2, z = ~gene3, color = ~cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter3d\", mode = \"markers\",size = 5\n)  %&gt;%\n  layout(\n    title = \"Original 3D Data with Clusters and Subclusters\",\n    scene = list(\n      camera = list(\n        eye = list(x = 0.3, y =2.5, z = 1.2)  # Change x, y, z to adjust the starting angle\n      )\n    )\n  )\n\n\n\n\n\n\nDifferent clusters have the same standard deviation but i have sampled different number of points. Cluster one is the densest cluster and the cluster four is the sparse one.\nLet‚Äôs see what our algorithm does on this:\n\n\nCode\nD&lt;-as.matrix(dist(data[,1:3]))\n# Function to compute local standard deviation based on k-nearest neighbors\ncompute_sigma &lt;- function(dist_matrix, k_neighbors) {\n  n &lt;- nrow(dist_matrix)\n  sigma &lt;- numeric(n)\n  \n  for (i in 1:n) {\n    # Get distances from point i to all other points\n    dists &lt;- dist_matrix[i, ]\n    \n    # Sort the distances and select k nearest neighbors (excluding self)\n    nearest_neighbors &lt;- sort(dists, decreasing = FALSE)[2:(k_neighbors + 1)]\n    \n    # Compute standard deviation of distances to nearest neighbors\n    sigma[i] &lt;- sd(nearest_neighbors)\n    sigma[i] &lt;- max(sigma[i] , 1e-4)\n    # Avoid sigma being zero\n    # sigma &lt;- sigma+\n  }\n  \n  return(sigma)\n}\n\n#compute\nsigma_D  &lt;-compute_sigma(D,10)\n\n# Compute P matrix based on sigma_D\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n      P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      \n    }\n    \n  }\n  P[i, ] &lt;- P[i, ] / sum(P[i,])\n}\nP = (P + t(P))/(2*nrow(D))\n\nndim &lt;- 2\nset.seed(12345)\nY &lt;- matrix(rnorm(nrow(D) * ndim, sd = 1, mean = 0), nrow = nrow(D), ncol = ndim)\ncolnames(Y) &lt;- c(\"dim1\", \"dim2\")\n\n\nfor (iter in 1:2000) {\n  Dy &lt;- as.matrix(dist(Y))\n  \n  Q &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        # Calculate conditional probability q_j|i using the same Gaussian kernel\n        Q[i, j] &lt;- exp(-(Dy[i, j])^2)\n        \n      }\n    }\n  }\n  \n  Q &lt;- Q / sum(Q)\n  grad &lt;- matrix(0, nrow = nrow(D), ncol = ndim)\n  # For each point i and j, compute the gradient of KL divergence\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        diff_y &lt;- Y[i, ] - Y[j, ]\n        scaling_factor &lt;- (P[i, j] - Q[i, j])\n        grad[i, ] &lt;- grad[i, ] + scaling_factor * diff_y\n        \n        \n      }\n    }\n    grad[i, ]&lt;-4* grad[i, ]\n  }\n  Y &lt;- Y - 10 * (grad)\n\n  \n}\nplot_ly(\n  data.frame(Y), x = ~dim1, y = ~dim2,color = ~data$cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n) \n\n\n\n\n\n\nDespite that for the sparse cluster things look OK but for the denser regions we spread the points unnecessarily. They should infact form tight clusters. We need a better measure of nearest neighbors that is more or less adaptive to the density of each region in the map. In this specific case, we probably have to consider a bit more neighbors in denser part of the data.\nIn an ideal situation we for a specific point, we want to have neighbors that more or less have same influence on the current point. So basically, we need a method that ensures each point considers a consistent number of neighbors, significantly influencing it across the entire dataset, regardless of local density variations.\nOne intiuative way to do that is to rather than setting \\(\\sigma_i\\) solely based on nearest neighbor distances, we can aim to adjust \\(\\sigma_i\\) so that each point considers the same effective number of neighbors.\nNow consider this, if all neighbors contributed equally, the probabilities \\(p_{ij}\\) would be uniformly distributed. For instance, if there were \\(k\\) neighbors, and each contributed equally, then:\n\\[\np_{ij} = \\frac{1}{k}, \\quad \\text{for each } j\n\\]\nHowever, in reality, the distances between the points vary, which means the probabilities \\(p_{ij}\\) are not uniform. Our task is to find a way to measure how far the actual distribution \\(p_{ij}\\) is from being uniform. To measure this, we can introduce a simple criterion: if we want to have \\(k\\) neighbors contributing equally, the sum of the probabilities for those neighbors should behave as if they are uniformly distributed across the \\(k\\) neighbors.\nTo express this mathematically, we can look at the average contribution of each neighbor and compare it with how far off each neighbor‚Äôs actual contribution is. So, we want a measure that tells us ff all probabilities \\(p_{ij}\\) are equal, this measure should indicate that all neighbors are contributing equally. And also If one or a few \\(p_{ij}\\)‚Äôs dominate, this measure should indicate that some neighbors are contributing more than others.\nLet‚Äôs consider the concept of information content associated with each probability \\(p_{ij}\\). The smaller \\(p_{ij}\\), the more surprising (or informative) it is that neighbor \\(j\\) has a significant influence. The information content for each neighbor \\(j\\) can be defined as:\n\\[\n\\text{Information content of } p_{ij} = -\\log(p_{ij})\n\\]\nThis makes sense because if \\(p_{ij}\\) is large (close neighbors), the information content is small, indicating that these neighbors are not surprising. But if \\(p_{ij}\\) is small (distant neighbors), the information content is large, indicating that these neighbors are more surprising.\nNow, to get a measure of the average information content of all the neighbors, we can take the weighted sum of these individual information contents, weighted by the probabilities ( p_{ij} ) themselves:\n\\[\n\\text{Average information content} = - \\sum_j p_{ij} \\log_2(p_{ij})\n\\]\nThis expression tells us the average uncertainty or spread in the probability distribution \\(P_i\\), based on the Gaussian distances. This is exactly the formula for Shannon entropy (\\(H\\)).\nNow let‚Äôs consider a super good scenario where all the neighbors are contributing euqally to the current point.\n\\[\nH(P_i) = -\\sum_{i=1}^{n} \\frac{1}{n} \\log_2 \\frac{1}{n} = \\log_2 n\n\\]\nThis is straightforward: the entropy is just the logarithm of the number of outcomes, which makes sense because more outcomes increase uncertainty. How do we transfomr this \\(log_2(n)\\) to the number of neighbors? We simply take \\(2^{H(P_i)}\\). In this case this is going to give us exactly \\(n\\). In cases where the distances are not equal, \\(H(P_i)\\) is no longer a simple logarithm of the number of outcomes. Instead, the value of \\(H(P_i)\\) reflects the uncertainty considering the actual probability distribution. However, we can still think about how many neighbors with equal contribution would generate the same amount of uncertainty. So we can still use \\(2^{H(P_i)}\\) which is going to give us the effective number of neighbors for a particular point. In our case, this parameter is called perplexity. The perplexity tells us the effective number of equally contributing neighbors and we want to adjust \\(\\sigma_i\\) so perplexity matches a desired number. That number is our number of neighbors (A smooth one!).\nHow do we find all \\(\\sigma_i\\)? Well, we have to search really. Either grid search or binary search etc can help us to figure out all the standard deviation for every single data point such that perplexity matches the desired value.\nHere i have changed our function to do that.\n\n\nCode\n# Function to compute perplexity given distances D and sigma\ncompute_perplexity &lt;- function(D, sigma) {\n  # Compute the conditional probabilities P_{j|i}\n  P &lt;- exp(-D^2 / (2 * sigma^2))\n  # Avoid division by zero\n  sumP &lt;- sum(P) + 1e-10\n  P &lt;- P / sumP\n  # Compute Shannon entropy H(P_i)\n  H &lt;- -sum(P * log2(P + 1e-10))\n  # Compute perplexity\n  perplexity &lt;- 2^H\n  return(perplexity)\n}\n\n# Function to find sigma given distances D and target perplexity\nfind_sigma &lt;- function(D, target_perplexity, tol = 1e-5, max_iter = 100, sigma_min = 1e-20, sigma_max = 1000) {\n  # Initialize sigma bounds\n  sigma_lower &lt;- sigma_min\n  sigma_upper &lt;- sigma_max\n  # Initial sigma guess\n  sigma &lt;- (sigma_lower + sigma_upper) / 2\n  for (i in 1:max_iter) {\n    # Compute perplexity with current sigma\n    perp &lt;- compute_perplexity(D, sigma)\n    # Check if perplexity is within tolerance\n    if (abs(perp - target_perplexity) &lt; tol) {\n      break\n    }\n    # Adjust sigma bounds based on perplexity\n    if (perp &gt; target_perplexity) {\n      # Perplexity too big, decrease sigma\n      sigma_upper &lt;- sigma\n    } else {\n      # Perplexity too small, increase sigma\n      sigma_lower &lt;- sigma\n    }\n    # Update sigma\n    sigma &lt;- (sigma_lower + sigma_upper) / 2\n  }\n  return(sigma)\n}\ncompute_sigma&lt;-function(D,perplexity)\n{\n  sigmas&lt;-c()\n  for(i in 1:nrow(D)){\n  sigmas&lt;-c(sigmas,find_sigma(D[i,-i], perplexity,max_iter = 1000))\n}\nsigmas\n}\n\n#compute\nD&lt;-as.matrix(dist(data[,1:3]))\nsigma_D  &lt;-compute_sigma(D,10)\n\n# Compute P matrix based on sigma_D\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n      P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      \n    }\n    \n  }\n  P[i, ] &lt;- P[i, ] / sum(P[i,])\n}\nP = (P + t(P))/(2*nrow(D))\n\nndim &lt;- 2\nset.seed(12345)\nY &lt;- matrix(rnorm(nrow(D) * ndim, sd = 1, mean = 0), nrow = nrow(D), ncol = ndim)\ncolnames(Y) &lt;- c(\"dim1\", \"dim2\")\n\n# define data for plot\n\nplot_frames_sne&lt;-c()\nfor (iter in 1:2000) {\n  Dy &lt;- as.matrix(dist(Y))\n  \n  Q &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        # Calculate conditional probability q_j|i using the same Gaussian kernel\n        Q[i, j] &lt;- exp(-(Dy[i, j])^2)\n        \n      }\n    }\n  }\n  \n  Q &lt;- Q / sum(Q)\n  grad &lt;- matrix(0, nrow = nrow(D), ncol = ndim)\n  # For each point i and j, compute the gradient of KL divergence\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        diff_y &lt;- Y[i, ] - Y[j, ]\n        scaling_factor &lt;- (P[i, j] - Q[i, j])\n        grad[i, ] &lt;- grad[i, ] + scaling_factor * diff_y\n        \n        \n      }\n    }\n    grad[i, ]&lt;-4* grad[i, ]\n  }\n  Y &lt;- Y - 5 * (grad)\n}\nplot_ly(\n  data.frame(Y), x = ~dim1, y = ~dim2,color = ~data$cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n) \n\n\n\n\n\n\nWe now have got much nicer clustering of the data. The gradients are kept and most things seem to be in place. We are very close in wrapping things up but there is still one thing left we need to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#crowding-problem",
    "href": "tsne.html#crowding-problem",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.5 Crowding problem",
    "text": "5.5 Crowding problem\nThe crowding problem arises because high-dimensional data points, when reduced to a lower-dimensional space, often become ‚Äúcrowded‚Äù in a way that fails to preserve the relative distances properly. This happens because it is difficult to accurately represent the larger pairwise distances in the lower-dimensional map, causing too many points to cluster closely together. So far our idea of dimensionality reduction has been to preserve the relative distances between points by converting distances into probabilities in both the high-dimensional and low-dimensional spaces. The relationship between probability and distance is important. Basicaly, the closer points in the high-dimensional spaceshould have higher probabilities of being close in the lower-dimensional map, while distant points should have lower probabilities of being near each other in the lower-dimensional map.\nWe used a Gaussian kernel in the lower-dimensional space, in this kernel the probabilities for distant points decrease rapidly because the Gaussian distribution assigns very small probabilities to pairs of points that are even moderately far apart. This means that even points that are moderately distant in the high-dimensional space will be assigned a very low probability of being far apart in the lower-dimensional map.\nNow, since our algorithm tries to match the probabilities (reduce the pariwise difference) between the high-dimensional and low-dimensional spaces, if the Gaussian assigns very low probabilities for moderately distant points in the low-dimensional map, the gradient will try to pull those points closer together to match this. The problem is that these points are not meant to be so close (since they are distant in high-dimensional space), but the fast decay of the Gaussian distribution forces them closer than they should be to match the low probability.\nWe bassicaly need another kernel that is decay more slowly in the lower dimension. The t-distribution is the one! It assigns a bit larger probabilities to distant points compared to a Gaussian. This allows moderately distant points to remain appropriately separated in the low-dimensional map. The slower decay helps to prevent crowding because it doesn‚Äôt force moderately distant points to come together as much as the Gaussian does.\nSo we are going to use t-distribution with one degree of freedom (Student‚Äôs t-distribution) in the low-dimensional space instead of a Gaussian. T-distribution has heavier tails, which allow for larger distances between points. Mathematically, the probability of a pair of points \\(i\\) and \\(j\\) in the low-dimensional map is computed as:\n\\[\nq_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l}(1 + ||y_k - y_l||^2)^{-1}}\n\\] Our gradient is also going to change to\n\\[\n   \\frac{\\delta C}{\\delta y_i} = 4 \\sum_j \\left( p_{j|i} - q_{j|i} \\right) (y_i - y_j)(1+||y_i-y_j||^2)^-1\n\\] All the notations are the same as we present before. We can go ahead and implement this in our algorithm:\n\n\nCode\n#compute\nD&lt;-as.matrix(dist(data[,1:3]))\nsigma_D  &lt;-compute_sigma(D,10)\n\n# Compute P matrix based on sigma_D\nP &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\nfor (i in 1:nrow(D)) {\n  for (j in 1:nrow(D)) {\n    if (i != j) {\n      P[i, j] &lt;- exp(-(D[i, j]^2) / (2*sigma_D[i]^2))\n      \n    }\n    \n  }\n  P[i, ] &lt;- P[i, ] / sum(P[i,])\n}\nP = (P + t(P))/(2*nrow(D))\n\nndim &lt;- 2\nset.seed(12345)\nY &lt;- matrix(rnorm(nrow(D) * ndim, sd = 1, mean = 0), nrow = nrow(D), ncol = ndim)\ncolnames(Y) &lt;- c(\"dim1\", \"dim2\")\n\nfor (iter in 1:2000) {\n  Dy &lt;- as.matrix(dist(Y))\n  \n  Q &lt;- matrix(0, nrow = nrow(D), ncol = nrow(D))\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        # Calculate conditional probability q_j|i using the same t-dist kernel\n        Q[i, j] &lt;- (1+(Dy[i, j])^2)^-1\n        \n      }\n    }\n  }\n  \n  Q &lt;- Q / sum(Q)\n  grad &lt;- matrix(0, nrow = nrow(D), ncol = ndim)\n  # For each point i and j, compute the gradient of KL divergence\n  for (i in 1:nrow(D)) {\n    for (j in 1:nrow(D)) {\n      if (i != j) {\n        diff_y &lt;- Y[i, ] - Y[j, ]\n        scaling_factor &lt;- (P[i, j] - Q[i, j])\n        grad[i, ] &lt;- grad[i, ] + scaling_factor * diff_y *((1+(Dy[i, j])^2)^-1)\n        \n        \n      }\n    }\n    grad[i, ]&lt;-4* grad[i, ]\n  }\n  Y &lt;- Y - 5 * (grad)\n  \n}\nplot_ly(\n  data.frame(Y), x = ~dim1, y = ~dim2,color = ~data$cluster,\n  colors = c(\"red\", \"green\", \"blue\", \"purple\"),\n  type = \"scatter\", mode = \"markers\",size = 5\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  },
  {
    "objectID": "tsne.html#t-sne-summary",
    "href": "tsne.html#t-sne-summary",
    "title": "1¬† t-distributed stochastic neighbor embedding (t-SNE)",
    "section": "5.6 t-SNE (summary)",
    "text": "5.6 t-SNE (summary)\nThe algorithm that we derived together is t-SNE. Obviously we have missed/omitted a lot of implemntational details but the general way that this algorithm works is more or less the same as we went through. t-SNE works by find finding standard deviations for each data point based on the perplexity parameter. It then uses the gaussian kernel to convert the distances between the data points to probability. It does the same thing for the lower dimention and tries to match the higher and lower dimensional probabilities. This is done by minimizing the Kullback-Leibler (KL) divergence between the two distributions. By iteratively adjusting the points in the lower-dimensional space, t-SNE captures both local and global structures in the data, with an emphasis on preserving local neighborhoods. The result is a visually intuitive low-dimensional embedding that reflects the high-dimensional relationships between points, allowing us to observe clusters and patterns that may not have been apparent in the original space.\nIn order to use t-SNE effectively, it is important to carefully choose the perplexity parameter, as it controls the balance between local and global structure preservation. A higher perplexity value emphasizes larger neighborhoods, capturing more global relationships, while a lower value focuses on smaller, local clusters. Additionally, t-SNE can be sensitive to initialization and learning rate, so experimenting with these parameters can help avoid poor embeddings. Preprocessing the data, such as normalizing or reducing dimensions beforehand (e.g., with PCA), can also improve performance and stability of the algorithm.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>t-distributed stochastic neighbor embedding (t-SNE)</span>"
    ]
  }
]